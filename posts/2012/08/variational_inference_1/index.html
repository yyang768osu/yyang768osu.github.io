

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>A step-by-step guide to variational inference (1): variational lower bound - Yang Yang</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Yang Yang">
<meta property="og:title" content="A step-by-step guide to variational inference (1): variational lower bound">


  <link rel="canonical" href="https://yyang768osu.github.io/posts/2012/08/variational_inference_1/">
  <meta property="og:url" content="https://yyang768osu.github.io/posts/2012/08/variational_inference_1/">



  <meta property="og:description" content="In machine learning, graphic models are often used to describe the factorization of probability distributions. The detailed form of the graphic model encodes one’s belief/hypothesis regarding the underlying hidden structure of the data. In this article, we confine the discussion to a general form of generative graphic model that we illustrate below.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2018-08-05T00:00:00-07:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yang Yang",
      "url" : "https://yyang768osu.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://yyang768osu.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Yang Yang Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://yyang768osu.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://yyang768osu.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://yyang768osu.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://yyang768osu.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://yyang768osu.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://yyang768osu.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://yyang768osu.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://yyang768osu.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://yyang768osu.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://yyang768osu.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://yyang768osu.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://yyang768osu.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://yyang768osu.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://yyang768osu.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://yyang768osu.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://yyang768osu.github.io/">Yang Yang</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/year-archive/">Blog Posts</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://yyang768osu.github.io/images/profile.png" class="author__avatar" alt="Yang Yang">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Yang Yang</h3>
    <p class="author__bio">Qualcomm AI Research</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> San Diego, California</li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/yyang768osu"><i class="fa fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=mcncpioAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-fw"></i> Google Scholar</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="A step-by-step guide to variational inference (1): variational lower bound">
    <meta itemprop="description" content="In machine learning, graphic models are often used to describe the factorization of probability distributions. The detailed form of the graphic model encodes one’s belief/hypothesis regarding the underlying hidden structure of the data. In this article, we confine the discussion to a general form of generative graphic model that we illustrate below.">
    <meta itemprop="datePublished" content="August 05, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">A step-by-step guide to variational inference (1): variational lower bound
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  7 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-08-05T00:00:00-07:00">August 05, 2018</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p>In machine learning, graphic models are often used to describe the factorization of probability distributions. The detailed form of the graphic model encodes one’s belief/hypothesis regarding the underlying hidden structure of the data. In this article, we confine the discussion to a general form of generative graphic model that we illustrate below.</p>

<p style="text-align: center;"><img src="/images/dgm.png" alt="generative model" /></p>

<p>Let $X=\{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\}$ denotes the dataset of interest, be it a set of images, a set of sound clips, or a set of document, depending on the problem at hand. The model describes a way for which the data is generated (and hence the name): we first sample a hidden/latent variable $z$ from the distribution $P(Z;\theta)$, and then sampled a data point $x$ from the distribution $P(X|Z;\theta)$ given the latent variable. The two probabilities are defined as we like and are given as part of the graphic models. Here we assume that the two probabilities are parameterized by a set of variables $\theta$, although in general we could merge it as part of latent variable $Z$ as well (as a global latent variable, the value of which is shared among all data samples), if there is a prior distribution for it. It is worth noting that the $P(Z;\theta)$ and $P(X|Z;\theta)$ could be further factorized, whichever way we design them to be, however, in this article we will just focus on the general setting.</p>

<p>There are two intertwined problems associated with this form of generative models: density estimation, and Bayesian inference. For the problem of density estimation, we want to estimate the probability that the model assigns to all the data $X$ we are given in training, or more precisely $p(X; \theta)$. The value of $p(X; \theta)$ explains how likely it is for the model to generate the given training data. Thus, the larger the $p(X; \theta)$, the better our model explains the existing data. For models that are parameterized by $\theta$, fitting the model to best match the training data amounts to finding a value of $\theta$ that maximize the density $p(X; \theta)$.</p>

<p>For the problem of Bayesian inference, we want to infer the posterior probability of the unobserved hidden/latent variable given the observed data, or more precisely $p(Z|X;\theta)$. It is easy to see that these two problems are naturally intertwined from Bayes rule: $p(Z|X) = \frac{p(X|Z)p(Z)}{P(X)}$: since $p(X|Z)$ and $p(Z)$ are already given as part of the model assumption, if we solve one of the two problems, then the other one can be solved as well. Conceptually, the solution to the problems can be viewed as trying to find a reverse graph in the generative model shown above.</p>

<p>Let’s now take a step back and ask the question: why do we even bother with the introduction of latent/hidden variable? Can we just propose models that capture $p(X;\theta)$ directly, and save the trouble of Bayesian inference for the latent variable all together?  Anyway, even with the direct characterization of  $p(X;\theta)$, the discussion above should still hold: the larger the  $p(X;\theta)$, the better our model explains the given data, and with a good model we can apply sampling to generate artificial data.</p>

<p>The significance of the hidden/latent variables are two-fold:</p>
<ol>
  <li>The adoption of hidden/latent variables allows one to construct complex marginal data distributions $p(X)$ from simple and easy to evaluate distribution functions. For example, with $p(Z;\theta)$ being the multinomial distribution and $p(X|Z;\theta)$ being the normal distribution, we arrive at the Gaussian mixture model $p(X;\theta)=\int P(X|Z;\theta)P(Z;\theta)dZ$, which can characterize a wide range of complex distribution and has significantly more expressiveness power compared with just Gaussian or multinomial distribution alone. It is evident that a model that characterizes more complex distributions can better fit the data, especially with the high-dimensional complicated data we are usually focusing on.</li>
  <li>The hidden/latent variables can be viewed as general features extracted from the data, which can be utilized for any downstream tasks. The hidden/latent variables normally has much lower dimension compared with the data itself, and they represent low-dimensional message that conveys condensed information regarding the corresponding data. If a model can fit the data well, meaning that the likelihood is high for the model to generate the training data by sampling $X$ conditioned on a sampling of $Z$, then one can argue that $Z$ should capture the essence of the data. It is interesting to note how the above two points echo the previous discussion regarding the inter-connection between density estimation and Bayesian inference.</li>
</ol>

<p>In the Bayesian inference problem, as stated before, the task is to find the posterior distribution of the unobserved variable $Z$ given then observed variable $X$. Instead of tackling this problem head on by deriving $p(Z|X;\theta)=\frac{p(X|Z;\theta)p(Z;\theta)}{\int p(X|Z;\theta)p(Z;\theta)dZ}$, which is often intractable, let us introduce another distribution $q(Z)$ with the goal of mimicking $p(Z|X;\theta)$, and look at what the KL divergence between the two could decompose into:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\text{KL}{\big(}q||p(Z|X;\theta){\big)}\\
=&\int q(Z) \ln \frac{q(Z)}{p(Z|X;\theta)}dZ\\
=&\int q(Z) \ln \frac{q(Z)}{P(Z|X;\theta)}\frac{p(X;\theta)}{p(X;\theta)}dZ\\
=&\int q(Z) \ln \frac{q(Z)p(X;\theta)}{P(Z,X;\theta)}dZ\\
=&\int q(Z) \ln p(X;\theta)dZ-\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ\\
=&\ln p(X;\theta)-\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ.
\end{align*} %]]></script>

<p>Making the short-hand notation of</p>

<script type="math/tex; mode=display">\begin{align*}
\mathcal{L}(q,\theta)=\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ,
\end{align*}</script>

<p>we can simplify the above equation as</p>

<script type="math/tex; mode=display">\begin{align*}
\ln p(X;\theta) = \text{KL}{\big(}q||p(Z|X;\theta){\big)} + \mathcal{L}(q,\theta).
\end{align*}</script>

<p>It turns out, the above equation is the cornerstone for a broad range of variational methods. Let’s stare at it for a while, observe it from different angles, and learn to appreciate its elegancy.</p>

<p>We should first observe that the three terms have fixed polarity: KL divergence is always nonnegative, whereas the log-likelihood term on the LHS of the equation, as well as the expression $\mathcal{L}(q,\theta)$ is always non-positive. At first glance into the definition of $\mathcal{L}$, it may look like it can be written in the form of negative KL divergence. However, one should note that $P(Z,X;\theta)$ is not a proper probability on $Z$ as $\int p(X,Z;\theta)dZ = p(X;\theta)&lt;1$.</p>

<p>Given that the KL divergence term is always nonnegative, $\mathcal{L}(q,\theta)$ yield a lower bound on the log-likelihood of the data. In precise term, we have $\ln p(X;\theta) \geq \mathcal{L}(q,\theta)$.</p>

<p>The term $\mathcal{L}(q,\theta)$ can be viewed as a functional that maps a probability distribution function into a value. 
Since the analysis and optimization of functional falls into the realm of calculus of variations, the distribution function $q$ itself is often called the variational distribution, and the lower bound $\mathcal{L}(q,\theta)$ is referred to as the variational lower-bound.</p>

<p>It is important to realize that the above equation is another manifestation of the inter-connection between the data likelihood $p(X;\theta)$ and the posterior distribution of latent variable $p(Z|X;\theta)$, this time linked through the variational distribution function $q$. For a fixed parameter $\theta$, if we increase the variational lower bound $\mathcal{L}(q,\theta)$ by adjusting $q$, then the updated lower-bound is closer to the log-likelihood of the data. At the same time, since an increment in $\mathcal{L}(q,\theta)$ would infer a decrement of $\text{KL}(q||p(Z|X;\theta))$, we know that the updated variational distribution $q$ is closer to the true posterior distribution measured in KL divergence. To precisely capture these observations, we arrive at the following two equations</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\ln p(X;\theta) &= \max_{q}\mathcal{L}(q,\theta)\\
p(Z|X;\theta) &= \arg\max_{q}\mathcal{L}(q,\theta).
\end{align*} %]]></script>

<p>Now we reached the core of variational inference: with an introduction of a variational distribution $q$, we can turn both the log-likelihood calculation (i.e., density estimation) problem and the Bayesian inference problem into an optimization problem, and attack it with different optimization algorithms. This inference-optimization duality provides a very powerful tool. It is the backbone of many of the variational inference related methods such as expectation-maximization, mean-field approximation, and variational auto-encoder, which we will discuss in details in the subsequent posts/subsections.</p>

<p>As an appendix, below we list two alternative proofs for the variational lower-bound.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\ln p(X;\theta)\\
=&\ln\frac{p(X,Z;\theta)}{p(Z|X;\theta)}\\
=&\int q(Z)\ln\frac{p(X,Z;\theta)}{p(Z|X;\theta)}dZ\\
=&\int q(Z)\ln\frac{p(X,Z;\theta) q(Z)}{p(Z|X;\theta) q(Z)}dZ\\
=&\int q(Z)\ln\frac{p(X,Z;\theta)}{q(Z)}dZ+\int q(Z)\ln\frac{q(Z)}{p(Z|X;\theta)}dZ.\\
=&\mathcal{L}(q,\theta) + \text{KL}{\big(}q||p(Z|X;\theta){\big)}.
\end{align*} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\ln p(X;\theta)\\
=&\ln \int p(X,Z;\theta)dZ\\
=&\ln \int q(Z)\frac{p(X,Z;\theta)}{q(Z)}dZ \\
\geq & \int q(Z)\ln\frac{p(X,Z;\theta)}{q(Z)}dZ.
\end{align*} %]]></script>

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://yyang768osu.github.io/tags/#variational-inference" class="page__taxonomy-item" rel="tag">variational inference</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://yyang768osu.github.io/posts/2012/08/variational_inference_1/" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://yyang768osu.github.io/posts/2012/08/variational_inference_1/" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=https://yyang768osu.github.io/posts/2012/08/variational_inference_1/" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://yyang768osu.github.io/posts/2012/08/variational_inference_1/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>
      
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://yyang768osu-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



      


  <nav class="pagination">
    
      <a href="#" class="pagination--pager disabled">Previous</a>
    
    
      <a href="https://yyang768osu.github.io/posts/2012/08/varitional_inference_2/" class="pagination--pager" title="A step-by-step guide to variational inference (2): expectation maximization
">Next</a>
    
  </nav>

    </div>

    
  </article>



  
  
</div>


    </script>

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/yyang768osu"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://yyang768osu.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Yang Yang. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://yyang768osu.github.io/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

