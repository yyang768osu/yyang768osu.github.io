

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>An introduction to Kalman filter and particle filter - Yang Yang</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Yang Yang">
<meta property="og:title" content="An introduction to Kalman filter and particle filter">


  <link rel="canonical" href="https://yyang768osu.github.io/posts/2012/08/kalman_filter_particle_filter/">
  <meta property="og:url" content="https://yyang768osu.github.io/posts/2012/08/kalman_filter_particle_filter/">



  <meta property="og:description" content="Kalman filter and particle filter are concepts that are intimidating for new learners due to its involved mathmatical discription, and are straightforward once you grasp the main idea and get used to Gaussian distributions. The goal of this post is to take a journey to Kalman filter by dissecting its idea and operation into pieces that are easy to absorb, and then assemble them together to give the whole picture. As a last step, we will see that particle filter achieves the same goal for non-Gaussian system resorting to Monte Carlo sampling.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2018-08-20T00:00:00-07:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yang Yang",
      "url" : "https://yyang768osu.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://yyang768osu.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Yang Yang Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://yyang768osu.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://yyang768osu.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://yyang768osu.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://yyang768osu.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://yyang768osu.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://yyang768osu.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://yyang768osu.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://yyang768osu.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://yyang768osu.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://yyang768osu.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://yyang768osu.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://yyang768osu.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://yyang768osu.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://yyang768osu.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://yyang768osu.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://yyang768osu.github.io/">Yang Yang</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/year-archive/">Blog Posts</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://yyang768osu.github.io/images/profile.png" class="author__avatar" alt="Yang Yang">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Yang Yang</h3>
    <p class="author__bio">Qualcomm AI Research</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> San Diego, California</li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/yyang768osu"><i class="fa fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=mcncpioAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-fw"></i> Google Scholar</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="An introduction to Kalman filter and particle filter">
    <meta itemprop="description" content="Kalman filter and particle filter are concepts that are intimidating for new learners due to its involved mathmatical discription, and are straightforward once you grasp the main idea and get used to Gaussian distributions. The goal of this post is to take a journey to Kalman filter by dissecting its idea and operation into pieces that are easy to absorb, and then assemble them together to give the whole picture. As a last step, we will see that particle filter achieves the same goal for non-Gaussian system resorting to Monte Carlo sampling.">
    <meta itemprop="datePublished" content="August 20, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">An introduction to Kalman filter and particle filter
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  9 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-08-20T00:00:00-07:00">August 20, 2018</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p>Kalman filter and particle filter are concepts that are intimidating for new learners due to its involved mathmatical discription, and are straightforward once you grasp the main idea and get used to Gaussian distributions. The goal of this post is to take a journey to Kalman filter by dissecting its idea and operation into pieces that are easy to absorb, and then assemble them together to give the whole picture. As a last step, we will see that particle filter achieves the same goal for non-Gaussian system resorting to Monte Carlo sampling.</p>

<p>Below let’s walk through three simple problems and their solutions stemming from Gaussian distributions, and then stitching them together to form the problem that Kalman filter tries to solve and present its solution.</p>

<h2 id="a-conditional-gaussian-distribution">A. Conditional Gaussian distribution</h2>

<p>Here I assume you have a basic knowledge regarding multi-variant Gaussian distribution. A multi-variant Gaussian distribution is captured by its mean vector and covariance matrix, often denoted as $\mu$ and $\Sigma$. Below let us consider the bi-variant Gaussian vector $[z,x]^T$, with the following general notation:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\left[
\begin{array}{c}
z\\
x
\end{array}
\right]
\sim
\mathcal{N}
\left(
\left[
\begin{array}{c}
\mu_z\\
\mu_x
\end{array}
\right],
\left[\begin{array}{cc}
\Sigma_z & \Sigma_{zx} \\
\Sigma_{xz} & \Sigma_{x}
\end{array}\right]
\right)\notag
\end{align*} %]]></script>

<p>The covariance matrix is formally defined as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\left[\begin{array}{cc}
\Sigma_z & \Sigma_{zx} \\
\Sigma_{xz} & \Sigma_{x}
\end{array}\right]
=&
\mathbb{E}\left[
\left[
\begin{array}{c}
z-\mu_z\\
x-\mu_x
\end{array}
\right]
\left[
\begin{array}{c}
z-\mu_z\\
x-\mu_x
\end{array}
\right]^T
\right]\notag\\
=&
\left[
\begin{array}{cc}
\mathbb{E}[(z-\mu_z)(z-\mu_z)^T] & \mathbb{E}[(z-\mu_z)(x-\mu_x)^T]\\
\mathbb{E}[(x-\mu_x)(z-\mu_z)^T] & \mathbb{E}[(x-\mu_x)(x-\mu_x)^T]
\end{array}
\right]\notag.
\end{align*} %]]></script>

<p>The off-diagonal term represents the cross covariance between the two random variables $x$ and $z$, which, by checking the definition above, satisfies $\Sigma_{xz}=\Sigma^T_{zx}$. The larger the cross covariance, the more correlated the two random variables are. For correlated random variables, knowing the value of one would help us in guessing the value of the other. Let us take a look at the figure below for a concrete example.</p>

<p style="text-align: center;"><img src="/images/bivariant_normal.png" alt="Bivariant Gaussian" /></p>

<p>The figure above illustrates the joint distribution of a bivariant Gaussian distribution with $\mu_z=\mu_x=0$, $\Sigma_z=\Sigma_x=1$, and $\Sigma_{zx}=\Sigma_{xz}=0.8$. The marginal distributions of both $x$ and $z$ are $\mathcal{N}(0,1)$. The contour of the distribution forms a thin ellipse, reflecting the strong covariance $\Sigma_{zx}=\Sigma_{xz}=0.8$ between the two random variables $x$ and $z$. To testify the claim that knowing one variable would help us estimating the other,  let us take a look at the the conditional distribution of $z$ given $x=1$, and compare it with the marginal distribution of $z$. As can be seen from the figure above, the distribution of $z|x=1$ has much narrow span than $z$ with a shift in the mean. The reduction of variance of $z$ after observing $x=1$ is an evident that the observation of $x$ narrows down the potential values of $z$.</p>

<p>An important fact here is that the conditional distribution of a joint-Gaussian distribution is also Gaussian</p>

<script type="math/tex; mode=display">\begin{align*}
p(z|x) = p(x,z)/p(x)\sim\mathcal{N}\left(\mu_{z|x}, \Sigma_{z|x}\right)\notag.
\end{align*}</script>

<p>Below are two identities on the general expressions of the conditional distribution, here let us accept them as they are without bothering with any proof.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mu_{z|x} &= \mu_z + \Sigma_{zx}\Sigma_{xx}^{-1}(x-\mu_x)\notag\\ 
\Sigma_{z|x} &= \Sigma_{z} - \Sigma_{zx}\Sigma_x^{-1}\Sigma_{xz} \text{ } \left(\preccurlyeq \Sigma_{z}\right)  \notag
\end{align*} %]]></script>

<p>The above two equations are very important, and lies in the core of many concepts such as MMSE estimator, Wiener filter and of course, Kalman filter. To see that knowing $x$ would reduce the uncertainty in $z$, here let’s just point out that the entropy (measure of uncertainty) of a Gaussian random vector is an increasing function of the determinant of the covariance-matrix, and that $\Sigma_{z|x}$ always <a href="https://math.stackexchange.com/questions/466158/on-the-difference-of-two-positive-semi-definite-matrices">has a smaller determinant</a> than $\Sigma_z$ for any non-zero cross covariance $\Sigma_{zx}$, followed from the second equation with the fact that $\Sigma_{z}-\Sigma_{z|x}$ $=\Sigma_{zx}\Sigma_x^{-1}\Sigma_{xz}\succcurlyeq 0$ is a semi-positive-definite matrix.</p>

<p>These two equations will become useful when we visit part C, and we will come back to them.</p>

<h2 id="b-gaussian-distribution-with-linear-transformation">B. Gaussian distribution with linear transformation</h2>

<p>In the first section we looked at the case of obtaining the conditional distribution from a joint Gaussian distribution, here let’s look at the distribution of a Gaussian vector going through a linear transform. More precisely, let us define a random variable $z_n$ as obtained from the following transform</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
z_{n-1}&\sim\mathbb{N}(\mu_{n-1}, V_{n-1})\notag\\
z_n |z_{n-1}&\sim A z_{n-1}+a+\mathcal{N}(0,\Gamma)= \mathbb{N}(Az_{n-1}+a, \Gamma)\notag
\end{align*} %]]></script>

<p>One typical example for the above problem setup is the following: consider the problem of tracking the location and velocity of an object traveling in a strict line. Let us label $z_n=[\text{loc}_n, \text{vel}_n]^T$ as the location-velocity state of the object in time-step $n$. To model the estimation inaccuracy, assume that $z_n$ is a random variable with mean $\mu_{n-1}$ and variance $V_{n-1}$. Here the mean value reflect the estimated value and the variance can be viewed as capturing the amount of and the structure of the uncertainty in the estimate. The location-velocity estimate in the time-step $n$ can be modeled by</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\left[
\begin{array}{c}
\text{loc}_{n}\\
\text{vel}_{n}
\end{array}
\right]=
\underbrace{
\left[
\begin{array}{cc}
1 & 1\notag\\
0 & 1\notag
\end{array}
\right]}_{
\text{loc}_{n} = \text{loc}_{n-1}+\text{vel}_{n-1}
}
\times
\left[
\begin{array}{c}
\text{loc}_{n-1}\\
\text{vel}_{n-1}
\end{array}
\right]
+
\underbrace{
\left[
\begin{array}{c}
a_\text{loc}\notag\\
a_\text{vel}\notag
\end{array}
\right]}_{\substack{
\text{external known}\\\text{change}
}
}
+
\underbrace{
\left[
\begin{array}{c}
\text{noise}_\text{loc}\notag\\
\text{noise}_\text{vel}\notag
\end{array}
\right]}_{\substack{
\text{additional noise}\\\text{in the system}
}
}
\end{align*} %]]></script>

<p>with a one-to-one correspondence to the conditional probability $z_n|z_{n-1}$ restated above.</p>

<p>The problem of interest here is to characterize the distribution of $z_n$, given that it is obtained from a linear transformation of a previous estimate with additional Gaussian noise. Precisely, what we want to solve is</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
p(z_n) =& \int p(z_{n}|z_{n-1})p(z_{n-1})dz_{n-1}\notag.
\end{align*} %]]></script>

<p>Here is another important property of Gaussian distribution: any linear transformation of Gaussian variable is still Gaussian. With this property given, we can calculate the mean and variance of the updated state $z_n$, as shown below, which fully captures its distribution.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mu_{z_n} =&\mathbb{E}[Az_{n-1}+a]= A\mu_{n-1}+a\notag\\
\Sigma_{z_n} =&\mathbb{E}[(Az_{n-1}-A\mu_{n-1})(Az_{n-1}-A\mu_{n-1})^T]+\Gamma\notag\\
=& AV_{n-1}A^T + \Gamma\notag,
\end{align*} %]]></script>

<p>which leads to the following solution</p>

<script type="math/tex; mode=display">\begin{align*}
z_n \sim \mathcal{N}\left(A\mu_{n-1}+a, AV_{n-1}A^T + \Gamma\right).
\end{align*}</script>

<h2 id="c-bayes-theorem-for-gaussian">C. Bayes’ theorem for Gaussian</h2>

<p>In part A, we provide the equation for calculating the conditional distribution from a joint Gaussian distribution, i.e., for a given joint-Gaussian probability $p(x,z)$, the conditional distribution of $p(z|x)$ is also Gaussian and it can be expressed in closed-form.</p>

<p>In this part, we consider a slightly more complex problem, whereby we are given Gaussian distributions $p(x|z)$ and $p(z)$,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
z&\sim \mathcal{N}(\nu, P),\notag\\
x|z&\sim \mathcal{N}(Cz+c,\Sigma) = Cz+c+\mathcal{N}(0, \Pi),\notag
\end{align*} %]]></script>

<p>and the problem is find the posterior distribution $p(z|x)$, which can be expressed using $p(x|z)$ and $p(z)$ by Bayes’ rule:</p>

<script type="math/tex; mode=display">\begin{align*}
p(z|x) = \frac{p(z)p(x|z)}{p(x)}  \propto p(z)p(x|z).\notag
\end{align*}</script>

<p>Here’s a typical application for this problem: let us consider the task of estimating the temperature and humidity of a room (denoted as vector $z$). We are given two sources of information: (1) prior knowledge on the distribution from history data and, e.g., $p(z)$ (2) the reading from a thermometer with some known accuracy $p(x|z)$. Intuitively, a good estimate should be obtained by fusing these two informations. Indeed, this is evident from the Bayes’ rule, where the posterior probability of $z$ given $x$ is proportional to the product of the two distributions $p(z)p(x|z)$.</p>

<p>Since part A taught us how to obtain a conditional distribution from a joint distribution. We can solve this problem by obtaining the joint distribution $p(z,x)=p(z)p(x|z)$ first and then plugin the solution presented in part A.</p>

<p>For Gaussian, the multivariant joint distribution is fully captured by the marginalized mean/variance together with the cross-variance among all factors, which, in our case, can be expressed as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mu_{x} &= \mathbb{E}[Cz+c+\mathcal{N}(0, \Sigma)] = C\nu+c\notag\\
\Sigma_{x} &= \mathbb{E}[xx^T] = \mathbb{E}[(Cz-C\nu)(Cz-C\nu)^T]+\Sigma=C P C^T + \Pi\notag\\
\Sigma_{zx} &= \mathbb{E}[z(Cz-C\mu)^T]=P C^T  \notag
\end{align*} %]]></script>

<p>Accordingly, the joint distribution of $z$ and $x$ can be written as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\left[
\begin{array}{c}
z\\
x
\end{array}
\right]
\sim
\mathcal{N}
\left(
\left[
\begin{array}{c}
\nu\\
C\nu+c
\end{array}
\right],
\left[\begin{array}{cc}
P &P C^T \\
CP & C P C^T + \Pi
\end{array}\right]
\right)\notag.
\end{align*} %]]></script>

<p>Now, by plugging in the solution in part A, we can obtain below the expression of the mean and variance of the posterior probability $p(z|x)$.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mu_{z|x} &= \nu + PC^T (CPC^T+\Pi)^{-1}(x-C\nu-c)\notag\\
\Sigma_{z|x} &= P - PC^T (CPC^T+\Pi)^{-1}CP\notag 
\end{align*} %]]></script>

<p>To simplify the expression as well as to gain some insights into the expression, it is necessary to group some of the terms in $K$ below and substitute the corresponding terms.</p>

<script type="math/tex; mode=display">\begin{align*}
K\triangleq PC^T(CPC^T+\Pi)^{-1},\notag
\end{align*}</script>

<p>resulting in the rewritten form below:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mu_{z|x} &= {\color{red}\nu} + K(x-C\nu-c)\notag\\
\Sigma_{z|x} &= {\color{red}P}-KCP =(I-KC)P.\notag
\end{align*} %]]></script>

<p>It is interesting to observe that the highlighted term in the expression above is the mean and the variance of the prior distribution $p(z)$ without taking $p(x|z)$ into account. The effect of the $p(x|z)$ can be thought of as a correction to the prior distribution: the mean is shifted by $K(x-Cv-c)$ and the covariance matrix is reduced by $KCP$ (or shrunk by $(I-KC)$), leading to a refined posterior distribution $p(z|x)$. Here $K$ can be considered as a <em>gain</em> factor, as it shifts the mean towards that dictated by $x$ and it shrinks the covariance matrix, leading to a more concentrated distributed with less amount of uncertainty.</p>

<p>Next we will see that Kalman filter is just a repeated (or sequential) application of this Bayes’ rule on Gaussian distribution.</p>

<h2 id="kalman-filter">Kalman filter</h2>

<p>It’s time assemble what we learnt from the previous parts. Let’s consider following an evolving system, where the system state $z_n$ follows linear evolving over time, whose true value is hidden from us. Every time instance, we obtain a noisy observation $x_n$ of the system state. The noisy observation $x_n$ may not be directly the state itself, but is in general an linear function of the state of interest, with added Gaussian noise. The task is to keep updating the belief on the system state, based on all the noisy observations, and the knowledge on the system evolution itself. In the degenerated case where the system does not evolve, then the problem amount to the sequential application of Bayes’ rule on the same hidden variable to fuse all the instances of noisy observations.</p>

<p style="text-align: center;"><img src="/images/linear_dynamic_system.png" alt="Bivariant Gaussian" /></p>

<p>To devise a sequence update rule on the brief of the system state based on all observations $p(z_n| x_1^n)$, let us look at the atomic case when $p(z_{n-1}|x_1^{n-1})$ — the prior belief of the previous state, and $p(x_n|z_n)$ — the noisy observation based on the current state, are given, and the task is to find $p(z_n|x_1^{n})$ — the posterior belief of the current state.</p>

<p>Here we assume that the noisy observations $x$ are independent given the underlying system state $z$ (this assumption is actually encoded in the graphic model above), then $p(z_{n-1}|x_1^{n-1})$  captures all the information regarding $x_1^{n-1}$, and we can simply drop them from the expression.</p>

<p>Then the problem simplifies to the following:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
z_{n-1}&\sim\mathcal{N}(\mu_{n-1}, V_{n-1})\notag\\
z_n|z_{n-1} &\sim \mathcal{N}(Az_{n-1}+a, \Gamma)= \mathcal{N}(A\mu_{n-1}+a,AV_{n-1}A^T + \Gamma)\notag\\
z_n&\sim \mathcal{N}(\nu_{n-1}, P_{n-1})\notag
\end{align*} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\nu_{n-1}\triangleq&  A\mu_{n-1}+a\notag\\
P_{n-1}\triangleq& AV_{n-1}A^T+\Gamma\notag
\end{align*} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\mu_{z_n|x_n} =& \nu_{n-1} + K_n(x_n-C\nu_{n-1}-c)\notag\\
\Sigma_{z_n|x_n} =& (I-K_nC)P_{n-1}\notag
\end{align*} %]]></script>

<script type="math/tex; mode=display">\begin{align*}
K_n\triangleq P_{n-1}C^T(CP_{n-1}C^T+\Pi)^{-1}\notag
\end{align*}</script>

<h2 id="particle-filter">Particle filter</h2>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
p(z_n| x_1^n) \propto&\text{ }  p(x_n | z_n) p(z_n | x_1^{n-1})\notag\\
p(z_{n+1}|x_1^n) =& \int p(z_n| x_1^n) p(z_{n+1}|z_n) dz_n\notag
\end{align*} %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
p(z_{n+1}|x_1^n) =& \int p(z_n| x_1^n) p(z_{n+1}|z_n) dz_n\notag\\
=&\int \notag p(z_n| x_1^{n-1}) \frac{p(z_n| x_1^n)}{p(z_n| x_1^{n-1})}  p(z_{n+1}|z_n) dz_n\notag\\
=&\int \notag p(z_n| x_1^{n-1}) \frac{p(z_n| x_1^n)}{p(z_n| x_1^{n-1})}  p(z_{n+1}|z_n) dz_n\notag\\
=&\sum_{\substack{s=1\\ z_n^{(s)}\sim p(z_n|x_{1}^{n-1})}}^S \frac{p(x_n|z_n^{(s)})}{\sum_{l=1}^S p(x_n|z_n^{(l)})}p(z_{n+1}|z_n^{(s)})\notag
\end{align*} %]]></script>


        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://yyang768osu.github.io/tags/#kalman-filter" class="page__taxonomy-item" rel="tag">kalman filter</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://yyang768osu.github.io/posts/2012/08/kalman_filter_particle_filter/" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://yyang768osu.github.io/posts/2012/08/kalman_filter_particle_filter/" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=https://yyang768osu.github.io/posts/2012/08/kalman_filter_particle_filter/" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://yyang768osu.github.io/posts/2012/08/kalman_filter_particle_filter/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>
      
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://yyang768osu-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



      


  <nav class="pagination">
    
      <a href="https://yyang768osu.github.io/posts/2012/08/griffin-lim/" class="pagination--pager" title="Griffin-Lim algorithm for waveform reconstruction
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>



  
  
</div>


    </script>

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/yyang768osu"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://yyang768osu.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Yang Yang. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://yyang768osu.github.io/assets/js/main.min.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123722738-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123722738-1');
</script>







  </body>
</html>

