

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Enforcing Lipschitz Constant in Neural Network - Yang Yang</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Yang Yang">
<meta property="og:title" content="Enforcing Lipschitz Constant in Neural Network">


  <link rel="canonical" href="https://yyang768osu.github.io/posts/2021/04/lipschitz-constant/">
  <meta property="og:url" content="https://yyang768osu.github.io/posts/2021/04/lipschitz-constant/">



  <meta property="og:description" content="A function $g(x)$ is Lipschitz continous if there exists a constant $L$ such that $||g(x_1) - g(x_2)|| &lt; L ||x_1 - x_2||$ for any $x_1$ and $x_2$ in its domain. $L$ is referred to as a Lipschitz constant of $g$. In deep learning, the need to enforce a certain Lipschitz constant for neural networks arises in many different sub-areas listed below. In this post, we derive a common technique used in many existing literatures.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2021-04-03T00:00:00-07:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yang Yang",
      "url" : "https://yyang768osu.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://yyang768osu.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Yang Yang Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://yyang768osu.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://yyang768osu.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://yyang768osu.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://yyang768osu.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://yyang768osu.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://yyang768osu.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://yyang768osu.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://yyang768osu.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://yyang768osu.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://yyang768osu.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://yyang768osu.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://yyang768osu.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://yyang768osu.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://yyang768osu.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://yyang768osu.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://yyang768osu.github.io/">Yang Yang</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/blog-posts/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/reading-notes/">Reading Notes</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/reading-list/">Reading List</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://yyang768osu.github.io/images/yy.jpg" class="author__avatar" alt="Yang Yang">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Yang Yang</h3>
    <p class="author__bio"></p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> San Diego, California</li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/yyang768osu"><i class="fa fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=mcncpioAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-fw"></i> Google Scholar</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Enforcing Lipschitz Constant in Neural Network">
    <meta itemprop="description" content="A function $g(x)$ is Lipschitz continous if there exists a constant $L$ such that $||g(x_1) - g(x_2)|| &lt; L ||x_1 - x_2||$ for any $x_1$ and $x_2$ in its domain. $L$ is referred to as a Lipschitz constant of $g$. In deep learning, the need to enforce a certain Lipschitz constant for neural networks arises in many different sub-areas listed below. In this post, we derive a common technique used in many existing literatures.">
    <meta itemprop="datePublished" content="April 03, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Enforcing Lipschitz Constant in Neural Network
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  4 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2021-04-03T00:00:00-07:00">April 03, 2021</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p>A function $g(x)$ is Lipschitz continous if there exists a constant $L$ such that $||g(x_1) - g(x_2)|| &lt; L ||x_1 - x_2||$ for any $x_1$ and $x_2$ in its domain. $L$ is referred to as a Lipschitz constant of $g$. In deep learning, the need to enforce a certain Lipschitz constant for neural networks arises in many different sub-areas listed below. In this post, we derive a common technique used in many existing literatures.</p>

<ul>
  <li>Guarantee invertibility in normalizing flows build with residual blocks
    <ul>
      <li><a href="https://arxiv.org/abs/1811.00995">iResNet(ICML2019)</a></li>
    </ul>
  </li>
  <li>Discriminator regularization in GAN training
    <ul>
      <li><a href="https://arxiv.org/abs/1701.07875">Wasserstein-GAN(ICML2017)</a></li>
      <li><a href="https://arxiv.org/abs/1802.05957">SpectralNormalization(ICLR2018)</a></li>
    </ul>
  </li>
  <li>Improve network robustness against adversarial perturbations
    <ul>
      <li><a href="https://arxiv.org/abs/1802.04034">Lipschitz-margin-training(NIPS2018)</a></li>
    </ul>
  </li>
</ul>

<p>A small note before we move on: Lipschitz continous/constant is defined with respect to a choice of the norm $||\cdot||$. In this post, we constrain to 2-norm.</p>

<h2 id="lipschitz-constant-vs-spectral-norm-of-matrices">Lipschitz constant vs spectral norm of matrices</h2>

<p>Deep neural networks are typically build with interleaved linear layers (such as Conv, TConv, Pooling) together with nonlinear activations (such as ReLU, sigmoid). The Lipschitz constant of most activation function are either constant or easy to control, so we will only focus on linear operationss. Linear operations in genenral can be expressed as in the form of a matrix-vector product $y = g(x) = Wx$ where $W$ denotes a matrix. The smallest Lipschitz constant of $g$ can be expressed as</p>

<script type="math/tex; mode=display">\begin{align}
\min_{x_1, x_2, x_1\not=x_2} \frac{
||g(x_1)-g(x_2)||
}{
||x_1 - x_2||
}
=
\min_{||v||\not=0}\frac{
||Wv||
}{
||v||
}
=
\min_{||v||=1}
||Wv||,
\end{align}</script>

<p>which is the definition of the <em>spectral norm</em> of matrix $W$. Let us express $W$ as its singular-value-decomposition $U\Sigma V^T$, then it is not hard to see that the spectral norm is of $W$ its maximum singular value, hereafter denoted as $\sigma_1$, which is the same as the eigenvalues of $M\triangleq W^TW$ given that eigenvalues of $W^TW$ is square of singular values of $W$: $M=V\Sigma U^TU\Sigma V^T=V\Sigma^2V^T=V\Lambda V^T$.</p>

<p>Now we know that obtaining the best Lipschitz constant of a linear operations amounts to finding the dominant singular value of its matrix representation $W$ ($\sigma_1$), or dominant eigenvalue of $M\triangleq W^TW$ ($\lambda_1$), next let us introduce an algorithm that finds it.</p>

<h2 id="power-method-aka-von-mises-iteration">Power method (aka Von Mises iteration)</h2>

<p>Power method finds the maximum eigenvalue of a matrix $M$ using the following iterations:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\text{start with a random vector }v^{(0)} \\
&\text{for }k=1, 2, \ldots, \\
&v^{(k)} = \frac{
M v^{(k-1)}
}{
||M v^{(k-1)}||
}
\end{align*} %]]></script>

<p>Claim: $||M v^{(k)}||$ converges to the maximum eigen value of $M$ as $k$ approaches infinity.</p>

<p>To show it, let us write the initial vector $v^{(0)}$ as a linear combinations of eigen vectors of $M$: $v^{(0)}=\sum_{i}\alpha_i v_i$, and expand the iterative formula as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&v^{(k)} = \frac{
M v^{(k-1)}
}{
||M v^{(k-1)}||
}
= \frac{
M^2 v^{(k-2)}
}{
||M^2v^{(k-2)}||
}=\ldots
= \frac{
M^k v^{(0)}
}{
||M^kv^{(0)}||
}
\\
&M^k v^{(0)} = M^k \sum_{i} \alpha_i v_i = \sum_{i} \alpha_i M^k v_i \sum_{i}\alpha_i \lambda_i^k v_i = \alpha_1\lambda_1^k
\left(
v_1 + \sum_{i>1}\underbrace{\frac{\alpha_i}{\alpha_1}\left(\frac{\lambda_i}{\lambda_1}\right)^k}_{\to 0 \text{ as } k\to\infty} v_i
\right).
\end{align*} %]]></script>

<p>From the last Equation we know that $v^{(k)}$ converges to dominant eigen vector $v_1$ of $M$ up to a sign difference, and similarly $Mv^{(k)}$ converges to the maximum eigen value $\sigma_1$ of $M$.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
v^{(k)}\to\left\{\begin{array}{ll}
v_1 & \text{if }\alpha_1>0\\
-v_1 & \text{if }\alpha_1<0
\end{array}\right., \text{ as }k\to\infty
\end{align*} %]]></script>

<h2 id="compute-power-iteration-through-auto-differentiation">Compute power iteration through auto-differentiation</h2>

<p>From last section we know that the maximum singular value can be computed if we carry out the following iteration procedure:</p>

<script type="math/tex; mode=display">\begin{align*}
 v^{(k-1)} \Longrightarrow \underbrace{\tilde{v}^{(k)}=W^TWv^{(k-1)}}_{\text{step 1}} \Longrightarrow \underbrace{v^{(k)}=\tilde{v}^{(k)}/||\tilde{v}^{(k)}||}_{\text{step 2}} \Longrightarrow\ldots
\end{align*}</script>

<p>While it is easy to compute a vector norm as done in step 2, it is not immediately clear how to easily compute $W^TWv^{(k-1)}$ step 1, especially for linear operations where expressing matrix $W$ is involved. For example, for normal 2D convolution operation, expressing it in the matrix-vector product form requires unpacking the convolution kernel into a doubly Toeplitz matrix. We know that $Wv^{(k-1)}$ is just the output of the linear operator $g$ when $v^{(k-1)}$ is used as input, but seemingly there is no easy way to multiply by $W^T$ without knowning $W$ explicitly.</p>

<p>Luckily, the following equation points us a way to compute $W^TWx$ without expressing $W$ explicitly, and can be readily computed with auto-differentiation.</p>

<script type="math/tex; mode=display">\begin{align*}
W^TW x = \frac{1}{2}\frac{\partial x^TW^TWx}{\partial x} = \frac{1}{2}\frac{\partial ||Wx||^2}{\partial x} =\frac{\partial \frac{1}{2}||g(x)||^2}{\partial x}
\end{align*}</script>

<p>We can then modify the iteration procedure as</p>

<script type="math/tex; mode=display">\begin{align*}
 v^{(k-1)} \Longrightarrow \underbrace{
\tilde{v}^{(k)} = \frac{
\partial\frac{1}{2}||g(v^{v^{(k-1)}})||^2
}{
\partial v^{(k-1)}
}
 }_{\text{step 1}} \Longrightarrow \underbrace{v^{(k)}=\tilde{v}^{(k)}/||\tilde{v}^{(k)}||}_{\text{step 2}} \Longrightarrow\ldots
\end{align*}</script>

<p>Based on last section, $\sqrt{||\tilde{v}^{(k)}||}$ yields an estimate of the dominant singular value of $M$, which is the Lipschitz constant of the linear operator $g$.
With Pytorch, step 1 can be calculated using <a href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad">torch.atuograd.grad</a>.</p>

<p>Upon further thinking, I realize that it is not surprising at all that the above iteration procedure converges to maximum singular value of $W$ – it is simply the gradient ascent with Equation (1) as the optimization objective.</p>

<h2 id="enforce-lipschitz-constant-c-during-training">Enforce Lipschitz constant $c$ during training</h2>

<p>It is easy to see that the Lipshitz constant of $a\times g(\cdot)$ is $a$ times the Lipschitz constant of $g(\cdot)$. Stating in another way $\text{Lip}(ag) = a\text{Lip}(g)$. Hence, to enforce the Lipschitz constant of an operator to be some target value $c$, we just need to normalize the output the operator by $c/\text{Lip}(g)$.</p>

<p>The power iteration procedure itself can be amortized and blended into the optimization step of the network training, given that network weight changes slowly, especially when the training is close to convergence, resulting in the following general training loop:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\text{for step }k=1, \ldots:\\
&v = v/||v||\\
&v = \frac{
\partial\frac{1}{2}||g(v)||^2
}{
\partial v 
}\\
&\sigma = \sqrt(||v||)\\
&\text{set the normalization scale of output of }g\text{ as }\frac{c}{\sigma}\\
&\text{network training step}.
\end{align*} %]]></script>


        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://yyang768osu.github.io/tags/#model" class="page__taxonomy-item" rel="tag">Model</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://yyang768osu.github.io/posts/2021/04/lipschitz-constant/" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://yyang768osu.github.io/posts/2021/04/lipschitz-constant/" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=https://yyang768osu.github.io/posts/2021/04/lipschitz-constant/" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://yyang768osu.github.io/posts/2021/04/lipschitz-constant/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>
      
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://yyang768osu-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



      


  <nav class="pagination">
    
      <a href="https://yyang768osu.github.io/posts/2020/09/langevin-dynamics/" class="pagination--pager" title="An Introduction of Lagevin Dynamics for Bayesian Inference
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>



  
  
</div>


    </script>

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/yyang768osu"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://yyang768osu.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Yang Yang. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://yyang768osu.github.io/assets/js/main.min.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123722738-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123722738-1');
</script>







  </body>
</html>

