

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Understanding conventional HMM-based ASR training - Yang Yang</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Yang Yang">
<meta property="og:title" content="Understanding conventional HMM-based ASR training">


  <link rel="canonical" href="https://yyang768osu.github.io/posts/2018/11/conventional-hmm-asr-training/">
  <meta property="og:url" content="https://yyang768osu.github.io/posts/2018/11/conventional-hmm-asr-training/">



  <meta property="og:description" content="Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here $\theta$ denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2018-11-17T00:00:00-08:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yang Yang",
      "url" : "https://yyang768osu.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://yyang768osu.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Yang Yang Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://yyang768osu.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://yyang768osu.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://yyang768osu.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://yyang768osu.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://yyang768osu.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://yyang768osu.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://yyang768osu.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://yyang768osu.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://yyang768osu.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://yyang768osu.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://yyang768osu.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://yyang768osu.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://yyang768osu.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://yyang768osu.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://yyang768osu.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://yyang768osu.github.io/">Yang Yang</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/blog-posts/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/reading-notes/">Reading Notes</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/reading-list/">Reading List</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://yyang768osu.github.io/images/profile.png" class="author__avatar" alt="Yang Yang">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Yang Yang</h3>
    <p class="author__bio"></p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> San Diego, California</li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/yyang768osu"><i class="fa fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=mcncpioAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-fw"></i> Google Scholar</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Understanding conventional HMM-based ASR training">
    <meta itemprop="description" content="Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here $\theta$ denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.">
    <meta itemprop="datePublished" content="November 17, 2018">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Understanding conventional HMM-based ASR training
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  6 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2018-11-17T00:00:00-08:00">November 17, 2018</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p>Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here $\theta$ denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.</p>

<p style="text-align: center;"><img src="/images/HMM.png" alt="conventional HMM based ASR probabilistic assumption" /></p>

<h2 id="maximum-likelihood-training">Maximum likelihood training</h2>

<p>In maximum likelihood estimation (MLE), as stated in the equation below, the objective is to maximize the likelihood of the data being generated by the generative model. In other words, we want to find the value of the parameters $\theta$ so that the above model best explains the acoustic features (e.g., spectrogram) that we observe.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\arg\max_\theta \prod_{n=1}^N \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right)\\
=&\arg\max_\theta \sum_{n=1}^N \log \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right).
\end{align*} %]]></script>

<p>For ease of notation, for the rest of this section, letâ€™s ignore the conditioning on ${\bf y}^{(n)}$ (or ${\bf p}^{(n)}$). The difficulty in evaluating the above log-likelihood lies in the need to marginalize over all potential values of ${\bf z}^{(n)}$. This formulation falls right into the discussion of the previous two posts:  <a href="/posts/2018/08/variational_inference_1/">variational lower bound</a> and <a href="/posts/2018/08/variational_inference_1/">expectation maximization</a>, which provide an iterative algorithm to approach the solution</p>

<script type="math/tex; mode=display">\begin{align}
\theta^{[i+1]} = \arg\max_{\theta} \sum_{n=1}^N \int \color{red}{\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)}\log \mathbb{P}\left({\bf x}^{(n)}, {\bf z}^{(n)};\theta\right)d z^{(n)}.
\end{align}</script>

<p>Most of the computation complexity in the above equation lies in finding the posterior probability of the latent state given the observed $\color{red}{\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta\right)}$. To elaborate on how the posterior probability is computed, letâ€™s expand the acoustic model part in the previous figure as below, which is essentially a hidden-Markov chain.</p>

<p style="text-align: center;"><img src="/images/HMM2.png" alt="conventional HMM based acoustic model" /></p>

<p>The inference problem (finding the posterior of the latent given the observed) in a hidden Markov chain can be solved by a forward-backward algorithm. The algorithm manifests itself as BCJR algorithm in convolutional code bit-level MAP decoding and <a href="/posts/2018/08/kalman_filter_particle_filter/">Kalman filtering</a> in linear dynamic system.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\text{Forward path: }&\text{calculate }\mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\text{ from }\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right) \notag \\
&\mathbb{P}\left(z_t, {\bf x}_{1\to t};\theta\right) = \sum_{z_{t-1}} \mathbb{P}(z_{t}|z_{t-1};\theta)\mathbb{P}(x_{t}|z_{t};\theta)\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right)  \notag \\
\text{Backward path: }&\text{calculate }\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right)\text{ from }\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
&\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) = \sum_{z_{t+1}} \mathbb{P}(z_{t+1}|z_{t};\theta)\mathbb{P}(x_{t+1}|z_{t+1};\theta)\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
\text{Combined: }&\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) \notag \\
&\mathbb{P}\left(z_t| {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) / \sum_{z_t}\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right)
\end{align} %]]></script>

<h2 id="circular-dependency-between-segmentation-and-recognition">Circular dependency between segmentation and recognition</h2>
<p>The expectation-maximization formulation for likelihood maximization reveals a fundamental circular dependency between segmentation and recognition.</p>

<p>Here <strong>segmentation</strong> refers to the alignment of sub-phoneme states of ${\bf y}$ and the acoustic feature observations ${\bf x}$, encoded in the hidden state sequence ${\bf z}$, and <strong>recognition</strong> refers to the classification of sub-phoneme hidden state sequence ${\bf z}$ for the corresponds acoustic feature observations ${\bf x}$.</p>

<p>The two equations below make the circular dependency precise:
<script type="math/tex">% <![CDATA[
\begin{align*}
\theta^{[i]} &
\underset{\text{ }}{\xrightarrow{\substack{\text{update soft-alignment}\\\text{based on recognition}}}} \mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)\text{ using Equation (2)}\\
\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right) &
\underset{\text{ }}{\xrightarrow{\substack{\text{update recognition}\\\text{based on soft-alignment}}}}
\theta^{[i+1]}\text{ using Equation (1)}
\end{align*} %]]></script></p>

<p>It is easy to argue that to have an accurate alignment, we need accurate recognition, and to train an accurate recognition, we have to rely on accurate alignment/segmentation.</p>

<p>In a convention ASR system, to bootstrap the training procedure, we have to start with a dataset that has human curated phoneme boundary/segmentation. Once the system is capacitated with reasonable recognition/inference, it is no longer confined with human aligned dataset and a much larger dataset can be used with just waveform and the corresponding phoneme transcription. Eventually, after the system is able to deliver robust segmentation, we can make hard decision on the alignment, and only focus on improving the recognition performance with potentially a different system that has a much larger capacity, e.g., a DNN replacing the GMM model.</p>

<h2 id="decoding">Decoding</h2>
<p>In the decoding stage, we try to find the word/sentence with the maximum a posterior (MAP) probability given the observed data</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{=}&\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) \\
=&\arg\max_{\bf y} \underbrace{\mathbb{P}({\bf x}|{\bf p};\theta)}_{\text{acoustic model}}\times\underbrace{\mathbb{P}({\bf p}|{\bf y})}_{\text{lexion}}\times\underbrace{\mathbb{P}({\bf y})}_{\text{language model}}
\end{align*} %]]></script>

<p>The lexicon and language model together construct a state transition diagram, which we unrolled in time to form a decoding trellis. For each transcription hypothesis, a proper MAP decoder would sum across all the paths in the trellis that corresponds to the transcription, which is computationally prohibitive.</p>

<p>One simplification one can make is to find the most probable path by running the Viterbi algorithm. However, even for Viterbi algorithm, the complexity is still too high for practical deployment due to the large state space and potentially large number of time steps.</p>

<p>To further reduce the computation complexity, the conventional system resorts to the beam-search algorithm â€“ basically a breath-first-search algorithm on the trellis that maintain only a limited number of candidates. The beam-search algorithm is often run on a weighted finite state transducer that captures the concatenation of language model and lexicon.</p>

<h2 id="discrepancy-between-mle-training-and-map-decoding">Discrepancy between MLE training and MAP decoding</h2>

<p>At first glance into the MAP decoding equation, it may appear that the MLE based training is well-aligned with the decoding process: maximizing the posterior probability of the transcription ${\bf y}$ given the acoustic feature ${\bf x}$ is equivalent to maximizing the likelihood of the observation ${\bf x}$. The argument being that the probability of the observation $\mathbb{P}(x;\theta)$ is anyway a constant dictated by the natural of peopleâ€™s speech, not something we can control. But is it true?</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\text{inference time:}\\
&\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&\arg\max_{\bf y} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*} %]]></script>

<p>It turns out there is a subtle difference between inference time (MAP decoding) and training time (MLE parameter update) that render the above statement wrong.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\text{training time:}\\
&\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{\not=}&\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*} %]]></script>

<p>As is evident by comparing the above two equations, when we try to update parameter $\theta$ to maximize directly the posterior probability of the transcription ${\bf y}$ given the acoustic feature ${\bf x}$, we can no longer ignore the term $\mathbb{P}({\bf x};\theta)$. The key is to realize that we model the speech as a generative model, where <strong>the probability of observing a certain acoustic features ${\bf x}$ is not dictated by the nature, but rather the generative model that we assume</strong>. By updating the parameter $\theta$ that best increase the likelihood, we inevitably change $\mathbb{P}({\bf x};\theta)$ too, and thus there is no guarantee that the posterior probability is increased. $\mathbb{P}({\bf x};\theta)$ is calculated by marginalizing over all potential transcriptions: $\mathbb{P}({\bf x};\theta)=\sum_{\bf y}\mathbb{P}({\bf x}|{\bf y};\theta)$.</p>

<p>To elaborate, in MLE, we try to maximize $\color{red}{\mathbb{P}({\bf y}|{\bf x};\theta)}$ with respect to $\theta$, we may very well also increased the likelihood for competing transcription sequences $\color{blue}{\mathbb{P}({\bf x}|{\bf \tilde{y}};\theta)}$, potentially resulting in decreased posterior probability $\mathbb{P}({\bf y}|{\bf x};\theta)$.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&\mathbb{P}({\bf y}|{\bf x};\theta) \\
=& \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=& \frac{\color{red}{\mathbb{P}({\bf x}|{\bf y};\theta)}\mathbb{P}({\bf y})}{\sum_{\bf \tilde{y}}\color{blue}{\mathbb{P}({\bf x}|{\bf \tilde{y}};\theta)}\mathbb{P}({\bf \tilde{y}})}
\end{align*} %]]></script>

<p>Fundamentally, <strong>the misalignment is rooted from the fact that we are using a generative model for discriminative tasks</strong>. In the next section, we discuss several ways to train the generative model for better discrimination.</p>

<h2 id="sequence-discriminative-training">Sequence discriminative training</h2>
<p>To bridge the aforementioned discrepancy, several other training targets are proposed.</p>

<p>to be continuedâ€¦</p>


        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://yyang768osu.github.io/tags/#asr" class="page__taxonomy-item" rel="tag">ASR</a><span class="sep">, </span>
    
      
      
      <a href="https://yyang768osu.github.io/tags/#hmm" class="page__taxonomy-item" rel="tag">HMM</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://yyang768osu.github.io/posts/2018/11/conventional-hmm-asr-training/" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://yyang768osu.github.io/posts/2018/11/conventional-hmm-asr-training/" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=https://yyang768osu.github.io/posts/2018/11/conventional-hmm-asr-training/" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://yyang768osu.github.io/posts/2018/11/conventional-hmm-asr-training/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>
      
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://yyang768osu-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



      


  <nav class="pagination">
    
      <a href="https://yyang768osu.github.io/posts/2018/11/end_to_end_asr_models/" class="pagination--pager" title="Comparison of end-to-end ASR models
">Previous</a>
    
    
      <a href="https://yyang768osu.github.io/posts/2019/03/normalizing-flow-1/" class="pagination--pager" title="Normalizing Flow I: understanding the change of variable equation
">Next</a>
    
  </nav>

    </div>

    
  </article>



  
  
</div>


    </script>

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/yyang768osu"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://yyang768osu.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2020 Yang Yang. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://yyang768osu.github.io/assets/js/main.min.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123722738-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123722738-1');
</script>







  </body>
</html>

