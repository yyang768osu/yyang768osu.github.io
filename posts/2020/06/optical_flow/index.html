

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Optical Flow – An Overview - Yang Yang</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Yang Yang">
<meta property="og:title" content="Optical Flow – An Overview">


  <link rel="canonical" href="https://yyang768osu.github.io/posts/2020/06/optical_flow/">
  <meta property="og:url" content="https://yyang768osu.github.io/posts/2020/06/optical_flow/">



  <meta property="og:description" content="">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2020-06-16T00:00:00-07:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Yang Yang",
      "url" : "https://yyang768osu.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://yyang768osu.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Yang Yang Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://yyang768osu.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://yyang768osu.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://yyang768osu.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://yyang768osu.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://yyang768osu.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://yyang768osu.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://yyang768osu.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://yyang768osu.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://yyang768osu.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://yyang768osu.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://yyang768osu.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://yyang768osu.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://yyang768osu.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://yyang768osu.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://yyang768osu.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://yyang768osu.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$', '$$'] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://yyang768osu.github.io/">Yang Yang</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/blog-posts/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/reading-notes/">Reading Notes</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://yyang768osu.github.io/reading-list/">Reading List</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://yyang768osu.github.io/images/yy.jpg" class="author__avatar" alt="Yang Yang">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Yang Yang</h3>
    <p class="author__bio"></p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> San Diego, California</li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/yyang768osu"><i class="fa fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=mcncpioAAAAJ&hl=en"><i class="ai ai-google-scholar-square ai-fw"></i> Google Scholar</a></li>
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Optical Flow – An Overview">
    <meta itemprop="description" content="">
    <meta itemprop="datePublished" content="June 16, 2020">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Optical Flow – An Overview
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  11 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2020-06-16T00:00:00-07:00">June 16, 2020</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <ul id="markdown-toc">
  <li><a href="#definition-of-optical-flow" id="markdown-toc-definition-of-optical-flow">Definition of Optical Flow</a></li>
  <li><a href="#useful-resources" id="markdown-toc-useful-resources">Useful Resources</a></li>
  <li><a href="#traditional-approach" id="markdown-toc-traditional-approach">Traditional Approach</a>    <ul>
      <li><a href="#brightness-constancy-assumption" id="markdown-toc-brightness-constancy-assumption">Brightness Constancy Assumption</a></li>
      <li><a href="#small-motion-assumption" id="markdown-toc-small-motion-assumption">Small Motion Assumption</a></li>
      <li><a href="#brightness-constancy-equation" id="markdown-toc-brightness-constancy-equation">Brightness Constancy Equation</a></li>
      <li><a href="#how-to-solve-brightness-constancy-equation" id="markdown-toc-how-to-solve-brightness-constancy-equation">How to solve Brightness Constancy Equation?</a></li>
      <li><a href="#formulation-of-horn-shunck-optical-flow" id="markdown-toc-formulation-of-horn-shunck-optical-flow">Formulation of Horn-Shunck Optical flow</a></li>
      <li><a href="#discrete-optical-flow-estimation" id="markdown-toc-discrete-optical-flow-estimation">Discrete Optical Flow Estimation</a></li>
    </ul>
  </li>
  <li><a href="#dataset" id="markdown-toc-dataset">Dataset</a>    <ul>
      <li><a href="#middlebury-link-paper" id="markdown-toc-middlebury-link-paper">Middlebury (link, paper)</a></li>
      <li><a href="#mpi-sintel-link-paper" id="markdown-toc-mpi-sintel-link-paper">MPI Sintel (link, paper)</a></li>
      <li><a href="#kitti-link-paper" id="markdown-toc-kitti-link-paper">KITTI (link, paper)</a></li>
      <li><a href="#flying-chairs-link-paper" id="markdown-toc-flying-chairs-link-paper">Flying Chairs (link, paper)</a></li>
      <li><a href="#flying-things-3d-link-paper" id="markdown-toc-flying-things-3d-link-paper">Flying Things 3D (link, paper)</a></li>
    </ul>
  </li>
  <li><a href="#evaluation-metric" id="markdown-toc-evaluation-metric">Evaluation Metric</a>    <ul>
      <li><a href="#angular-error-ae" id="markdown-toc-angular-error-ae">Angular Error (AE)</a></li>
      <li><a href="#end-point-error-epe" id="markdown-toc-end-point-error-epe">End Point Error (EPE)</a></li>
    </ul>
  </li>
  <li><a href="#end-to-end-regression-based-optical-flow-estimation" id="markdown-toc-end-to-end-regression-based-optical-flow-estimation">End-to-end regression based optical flow estimation</a>    <ul>
      <li><a href="#some-useful-concepts" id="markdown-toc-some-useful-concepts">Some useful concepts</a></li>
      <li><a href="#overview-of-different-models" id="markdown-toc-overview-of-different-models">Overview of different models</a></li>
      <li><a href="#flownet-iccv-2015-paper" id="markdown-toc-flownet-iccv-2015-paper">FlowNet (ICCV 2015) paper</a></li>
      <li><a href="#flownet-20-cvpr-2017-paper" id="markdown-toc-flownet-20-cvpr-2017-paper">FlowNet 2.0 (CVPR 2017) paper</a></li>
      <li><a href="#spynet-cvpr-2017-paper-code" id="markdown-toc-spynet-cvpr-2017-paper-code">SPyNet (CVPR 2017) paper code</a></li>
      <li><a href="#pwcnet-cvpr-2018-paper-code-video" id="markdown-toc-pwcnet-cvpr-2018-paper-code-video">PWCNet (CVPR 2018) paper code video</a></li>
      <li><a href="#irr-pwcnet-cvpr-2019-paper" id="markdown-toc-irr-pwcnet-cvpr-2019-paper">IRR-PWCNet (CVPR 2019) paper</a></li>
      <li><a href="#pwcnet-fusion-wacv-2019-paper" id="markdown-toc-pwcnet-fusion-wacv-2019-paper">PWCNet Fusion (WACV 2019) paper</a></li>
      <li><a href="#scopeflow-cvpr-2020-paper-code" id="markdown-toc-scopeflow-cvpr-2020-paper-code">ScopeFlow (CVPR 2020) paper code</a></li>
      <li><a href="#maskflownet-cvpr-2020-paper-code" id="markdown-toc-maskflownet-cvpr-2020-paper-code">MaskFlownet (CVPR 2020) paper code</a></li>
      <li><a href="#raft-recurrent-all-pairs-field-transforms-for-optical-flow-arxiv-2020-paper-code" id="markdown-toc-raft-recurrent-all-pairs-field-transforms-for-optical-flow-arxiv-2020-paper-code">RAFT: REcurrent All-Pairs Field Transforms for Optical Flow (Arxiv 2020) paper code</a></li>
    </ul>
  </li>
</ul>

<h2 id="definition-of-optical-flow">Definition of Optical Flow</h2>

<p>Distribution of apparent velocities of movement of brightness pattern in an image.</p>

<ul>
  <li>Where do we need it?
    <ul>
      <li>Action recognition</li>
      <li>Motion segmentation</li>
      <li>Video compression</li>
    </ul>
  </li>
</ul>

<h2 id="useful-resources">Useful Resources</h2>

<ul>
  <li>CMU Computer Vision 16-385
    <ul>
      <li><a href="http://www.cs.cmu.edu/~16385/s17/Slides/14.1_Brightness_Constancy.pdf">Brightness Constancy</a></li>
      <li><a href="http://www.cs.cmu.edu/~16385/s17/Slides/14.2_OF__ConstantFlow.pdf">Optical Flow : Constant Flow</a></li>
      <li><a href="http://www.cs.cmu.edu/~16385/s15/lectures/Lecture21.pdf">Optical Flow : Lucas-Kanade</a></li>
      <li><a href="http://www.cs.cmu.edu/~16385/s17/Slides/14.3_OF__HornSchunck.pdf">Optical Flow : Horn-Shunck</a></li>
    </ul>
  </li>
  <li>CMU Computer Vision 16-720
    <ul>
      <li><a href="http://16720.courses.cs.cmu.edu/lec/motion_lec12.pdf">Motion and Flow</a></li>
      <li><a href="http://16720.courses.cs.cmu.edu/lec/flow.pdf">Estimating Optical Flow 1</a></li>
      <li><a href="http://16720.courses.cs.cmu.edu/lec/flow_lec13.pdf">Estimating Optical Flow 2</a></li>
    </ul>
  </li>
  <li>Papers
    <ul>
      <li><a href="https://arxiv.org/abs/1504.06852">FlowNet: Learning Optical Flow with Convolutional Networks (ICCV 2015)</a></li>
      <li><a href="https://arxiv.org/abs/1612.01925">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks (CVPR 2017)</a></li>
      <li><a href="https://arxiv.org/abs/1611.00850">Optical Flow Estimation using a Spatial Pyramid Network (CVPR 2017)</a></li>
      <li><a href="https://arxiv.org/abs/2004.02853">Optical Flow Estimation in the Deep Learning Age (2020/04/06)</a></li>
      <li><a href="https://arxiv.org/abs/1709.02371">PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume (CVPR 2018)</a></li>
      <li><a href="https://arxiv.org/abs/1904.05290">Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation (CVPR 2019)</a></li>
      <li><a href="https://arxiv.org/abs/1810.10066">A fusion approach for multi-frame optical flow estimation (WACV 2019)</a></li>
      <li><a href="https://arxiv.org/abs/2002.10770">ScopeFlow: Dynamic Scene Scoping for Optical Flow (CVPR 2020)</a></li>
      <li><a href="https://arxiv.org/abs/2003.10955">MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask (CVPR 2020)</a></li>
      <li><a href="https://arxiv.org/abs/2003.12039">RAFT: REcurrent All-Pairs Field Transforms for Optical Flow (Arxiv 2020)</a></li>
    </ul>
  </li>
</ul>

<h2 id="traditional-approach">Traditional Approach</h2>

<h3 id="brightness-constancy-assumption">Brightness Constancy Assumption</h3>

<script type="math/tex; mode=display">\begin{align*}
I(x(t), y(t), t) = C
\end{align*}</script>

<h3 id="small-motion-assumption">Small Motion Assumption</h3>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&I(x(t+\delta t), y(t+\delta t), t+ \delta t) = I(x(t), y(t), t) + \frac{dI}{dt}\delta t, \text{(higher order term ignored)}\\
&\text{where }\frac{dI}{dt} = \frac{\partial I}{\partial x}\frac{d x}{d t} + \frac{\partial I}{\partial y}\frac{d y}{d t} + \frac{\partial I}{\partial t}\triangleq I_x u + I_y v + I_t \triangleq \nabla^T I [u, v]^T + I_t
\end{align*} %]]></script>

<p>$\nabla I = [I_x, I_y]^T$ : spatial derivative</p>

<p>$I_t$ : temporal derivative</p>

<p>$[u, v]$ : optical flow velocities</p>

<h3 id="brightness-constancy-equation">Brightness Constancy Equation</h3>
<p>Combining the above two assumptions, we obtain</p>

<script type="math/tex; mode=display">\begin{align*}
\nabla I [u, v]^T + I_t = 0.
\end{align*}</script>

<h3 id="how-to-solve-brightness-constancy-equation">How to solve Brightness Constancy Equation?</h3>
<p>Temporal derivative $I_t$ can be estimated by frame difference; spatial derivative $\nabla I$ can be estimated using spatial filters. Since there are two unknowns ($u$ and $v$), the system is underdetermined.</p>

<p>Two ways to enforce additional constraints:</p>

<ul>
  <li>Lucas-Kanade Optical Flow (1981) : assuming local patch has constant flow
    <ul>
      <li>LS can be applied to solve this overdetermined set of equations</li>
      <li>If there is lack of sptial gradient in a local path, then the set of equations could still be underdetermined. This is referred to as the <code class="highlighter-rouge">aperture</code> problem</li>
      <li>If applied to only tractable patches, these are called sparse flow</li>
    </ul>
  </li>
  <li>Horn-Schunck Optical Flow (1981) : assuming a smooth flow field</li>
</ul>

<h3 id="formulation-of-horn-shunck-optical-flow">Formulation of Horn-Shunck Optical flow</h3>

<p>Brightness constancy constraint/loss :</p>

<script type="math/tex; mode=display">\begin{align*}
E_d(i, j) = \left[I_x(i,j) u(i,j)+I_y(i,j)v(i,j) + I_t(i,j)\right]^2
\end{align*}</script>

<p>Smoothness constraint/loss :</p>

<script type="math/tex; mode=display">\begin{align*}
E_s(i, j) = \frac{1}{4}\left[(u(i,j)-u(i+1,j))^2, (u(i,j)-u(i,j+1))^2, (v(i,j)-v(i+1,j)^2, (v(i,j)-v(i,j+1))^2\right]
\end{align*}</script>

<p>Solving for optical flow :</p>

<script type="math/tex; mode=display">\begin{align*}
\text{min}_{\bf{u}, \bf{v}} \sum_{i,j} E_d(i,j) + \lambda E_s(i,j)
\end{align*}</script>

<p>Gradient descent can be used to solve the above optimization problem.</p>

<h3 id="discrete-optical-flow-estimation">Discrete Optical Flow Estimation</h3>

<p>Brightness Constancy Equation assumes small motion, which is in general not the case. If the movement is beyond 1 pixel, then higher order terms in the Taylor expansion of $I(x(t), y(t), t)$ could dominate. There are two solutions</p>
<ol>
  <li>To reduce the resolution using coarse-to-fine architecture</li>
  <li>Resort to discrete optical flow estimation</li>
</ol>

<p>For case-2, we obtain optical flow estimate by minimizing the following objective</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&E({\bf{z}}) = \sum_{i\in\mathcal{I}}D(z_i) + \sum_{(i,j)\in \mathcal{N}}S(z_i, z_j)\\
&\text{where } z_i\triangleq (u_i, v_j), \mathcal{I}\triangleq\text{set of all pixels }, \mathcal{N}\triangleq\text{set of all neighboring pixels}
\end{align*} %]]></script>

<p>The above can be viewed as energy minimization in a Markov random field.</p>

<h2 id="dataset">Dataset</h2>

<p>Table 1 from <a href="https://arxiv.org/pdf/1504.06852.pdf">FlowNet</a></p>

<table>
  <thead>
    <tr>
      <th>Entry</th>
      <th>Frame Pairs</th>
      <th>Frames with ground truth</th>
      <th>Ground-truth density per frame</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Middlebury</td>
      <td>72</td>
      <td>72</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>KITTI2012</td>
      <td>194</td>
      <td>194</td>
      <td>50%</td>
    </tr>
    <tr>
      <td>MPI Sintel</td>
      <td>1041</td>
      <td>1041</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>Flying Chairs</td>
      <td>22872</td>
      <td>22972</td>
      <td>100%</td>
    </tr>
    <tr>
      <td>Flying Things 3D</td>
      <td>22872</td>
      <td>-</td>
      <td>100%</td>
    </tr>
  </tbody>
</table>

<h3 id="middlebury-link-paper">Middlebury (<a href="http://vision.middlebury.edu/flow/">link</a>, <a href="http://vision.middlebury.edu/flow/floweval-ijcv2011.pdf">paper</a>)</h3>
<p>Contains only 8 image pairs for training, with ground truth flows generated using four different techniques. Displacements are very small, typically below 10 pixels. (Section 4.1 in <a href="https://arxiv.org/pdf/1504.06852.pdf">FlowNet</a>)</p>

<h3 id="mpi-sintel-link-paper">MPI Sintel (<a href="http://sintel.is.tue.mpg.de">link</a>, <a href="http://files.is.tue.mpg.de/black/papers/ButlerECCV2012-corrected.pdf">paper</a>)</h3>

<p>Computer-animated action movie. There are three render passes with varying degree of realism</p>
<ul>
  <li>Albedo render pass</li>
  <li>Clean pass (adds natural shading, cast shadows, specular reflections, and more complex lighting effects)</li>
  <li>Final pass (adds motion blur,  focus blur, and atmospherical effect)</li>
</ul>

<p>Contains 1064 training / 564 withheld test flow fields</p>

<h3 id="kitti-link-paper">KITTI (<a href="http://www.cvlibs.net/datasets/kitti/">link</a>, <a href="http://ww.cvlibs.net/publications/Geiger2013IJRR.pdf">paper</a>)</h3>

<p>Contains 194 training image pairs and includes large displacements, but contains only a very special motion type. The ground truth is obtained from real world scenes by simultaneously recording the scenes with a camera and a 3D laser scanner. This assumes that the scene is rigid and that the motion stems from a moving observer. Moreover, motion of distant objects, such as the sky, cannot be captured, resulting in sparse optical flow ground truth.</p>

<h3 id="flying-chairs-link-paper">Flying Chairs (<a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html">link</a>, <a href="https://arxiv.org/abs/1504.06852">paper</a>)</h3>

<p>Contains about 22k image pairs of chairs superimposed on random background images from Flickr. Random affine transformations are applied to chairs and background to obtain the second image and ground truth flow fields. The dataset contains only planar motions. (Section 3 in <a href="https://arxiv.org/abs/1612.01925">FlowNet 2.0</a>)</p>

<h3 id="flying-things-3d-link-paper">Flying Things 3D (<a href="https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html">link</a>, <a href="https://arxiv.org/pdf/1512.02134.pdf">paper</a>)</h3>

<p>A natural extension of the FlyingChairs dataset, having 22,872 larger 3D scenes with more complex motion patterns.</p>

<h2 id="evaluation-metric">Evaluation Metric</h2>

<h3 id="angular-error-ae">Angular Error (AE)</h3>
<p>AE between $(u_0, v_0)$ and $(u_1, v_1)$ is the angle in 3D space between $(u_0, v_0, 1.0)$ and $(u_1, v_1, 1.0)$. Error in large flow is penalized less than errors in small flow. (Section 4.1 in <a href="http://vision.middlebury.edu/flow/flowEval-iccv07.pdf">link</a>)</p>

<h3 id="end-point-error-epe">End Point Error (EPE)</h3>
<p>EPE between $(u_0, v_0)$ and $(u_1, v_1)$ is $\sqrt{(u_0-u_1)^2 + (v_0-v_1)^2}$ (Euclidean distance).</p>

<p>For Sintel MPI, papers also often reports detailed breakdown of EPE for pixels with different distance to motion boundaries ($d_{0-10}$, $d_{10-60}$, $d_{60-140}$) and different velocities ($s_{0-10}$, $s_{10-40}$, $s_{40+}$).</p>

<h2 id="end-to-end-regression-based-optical-flow-estimation">End-to-end regression based optical flow estimation</h2>

<h3 id="some-useful-concepts">Some useful concepts</h3>

<ul>
  <li>Backward warping</li>
</ul>

<p>$I_1(\cdot, \cdot)\triangleq I(\cdot, \cdot, t=1), I_2(\cdot, \cdot)\triangleq I(\cdot, \cdot, t=2)$. Optical flow field $u, v$ satisfies $I_1(x, y) = I_2(x+u, y+v)$. In other words, $u,v$ tells us where each pixel in $I_1$ is coming from, compared with $I_2$, and given $u, v$, we know how to move around (warp) the pixels in $I_2$ to obtain $I_1$. Here $I_1$ is often referred to as the source image and $I_2$ the target image – flow vector is defined per source image. Specifically, we can define a <code class="highlighter-rouge">warp</code> operation as below</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
&I_{\text{source}} = \texttt{warp}(I_\text{target}, f) \text{ where}\\
&I_{\text{source}}(x, y) = I_\text{target}(x+u, y+v)
\end{align*} %]]></script>

<ul>
  <li>Compositivity of backward warping</li>
</ul>

<script type="math/tex; mode=display">\begin{align*}
\texttt{warp}(I_\text{target}, f_a+f_b) = \texttt{warp}\left(\texttt{warp}(I_\text{target}, f_a), f_b\right)
\end{align*}</script>

<h3 id="overview-of-different-models">Overview of different models</h3>

<table>
  <thead>
    <tr>
      <th>Model Name</th>
      <th>Num of parameters</th>
      <th>inference speed</th>
      <th>Training time</th>
      <th>MPI Sintel final test EPE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>FlowNetS</td>
      <td>32M</td>
      <td>87.72fps</td>
      <td>4days</td>
      <td>7.218</td>
    </tr>
    <tr>
      <td>FlowNetC</td>
      <td>32M</td>
      <td>46.10fps</td>
      <td>6days</td>
      <td>7.883</td>
    </tr>
    <tr>
      <td>FlowNet2.0</td>
      <td>162M</td>
      <td>11.79fps</td>
      <td>14days</td>
      <td>6.016</td>
    </tr>
    <tr>
      <td>SPyNet</td>
      <td>1.2M</td>
      <td>-</td>
      <td>-</td>
      <td>8.360</td>
    </tr>
    <tr>
      <td>PWCNet</td>
      <td>8.7M</td>
      <td>35.01fps</td>
      <td>4.8days</td>
      <td>5.042</td>
    </tr>
  </tbody>
</table>

<p class="notice">The EPE column is taken from Table 2 of <a href="https://arxiv.org/abs/2004.02853">an overview paper</a>. The inference speed (on Pascal Titan X) and training time column is taken from Table 7 of <a href="https://arxiv.org/abs/1709.02371">PWCNet paper</a>.</p>

<h3 id="flownet-iccv-2015-paper">FlowNet (ICCV 2015) <a href="https://arxiv.org/abs/1504.06852">paper</a></h3>

<p style="text-align: center;"><img src="/images/optical_flow/FlowNet_encoder.png" alt="FlowNet encoder" /></p>
<p style="text-align: center;"><img src="/images/optical_flow/FlowNet_decoder.png" alt="FlowNet decoder" /></p>

<p>The first end-to-end CNN architecture for estimating optical flow. Two variants:</p>
<ul>
  <li>FlowNetS
    <ul>
      <li>A pair of input images is simply concatenated and then input to the U-shaped network that directly outputs optical flow.</li>
    </ul>
  </li>
  <li>FlowNetC
    <ul>
      <li>FlowNetC has a shared encoder for both images, which extracts a feature map for each input image, and a cost volume is constructed by measuring patch-level similarity between the two feature maps with a correlation operation. The result is fed into the subsequent network layers.</li>
    </ul>
  </li>
</ul>

<p>Multi-scale training loss is applied. Both models still underperform energy-based approaches.</p>

<h3 id="flownet-20-cvpr-2017-paper">FlowNet 2.0 (CVPR 2017) <a href="https://arxiv.org/abs/1612.01925">paper</a></h3>

<p style="text-align: center;"><img src="/images/optical_flow/FlowNet_2.png" alt="FlowNet 2.0" /></p>

<p>Key ideas:</p>
<ol>
  <li>By stacking multiple FlowNet style networks, one can sequentially refine the output from previous network modules.</li>
  <li>It is helpful to pre-train networks on a less challenging synthetic dataset first and then further train on a more challenging synthetic dataset with 3D motion and photometric effects</li>
</ol>

<p>End-to-end based approach starts to outperform energy-based ones.</p>

<h3 id="spynet-cvpr-2017-paper-code">SPyNet (CVPR 2017) <a href="https://arxiv.org/abs/1611.00850">paper</a> <a href="https://github.com/anuragranj/spynet">code</a></h3>

<p style="text-align: center;"><img src="/images/optical_flow/SPyNet.png" alt="SPyNet" /></p>

<p>Key idea:</p>
<ul>
  <li>Incorporate classic <code class="highlighter-rouge">coarse-to-fine</code> concepts into CNN network and update residual flow over mulitple pyramid levels (5 image pyramid levels are used). Networks at different levels have separate parameters.</li>
</ul>

<p>Achieves comparable performance to FlowNet with 96% less number of parameters.</p>

<h3 id="pwcnet-cvpr-2018-paper-code-video">PWCNet (CVPR 2018) <a href="https://arxiv.org/abs/1709.02371">paper</a> <a href="https://github.com/NVlabs/PWC-Net">code</a> <a href="https://www.youtube.com/watch?v=vVU8XV0Ac_0">video</a></h3>

<p style="text-align: center;"><img src="/images/optical_flow/PWCNet.png" alt="PWCNet" /></p>

<p>Key ideas:</p>
<ol>
  <li>Learned feature pyramid instead of image pyramid</li>
  <li>Warping of feature maps</li>
  <li>Computing a cost volume of learned feature maps (correlation)</li>
</ol>

<p>Computation steps:</p>
<ol>
  <li>Feature pyramid extractor: conv-net with down-sampling</li>
  <li>Target feature map is warped by upsampled previous flow estimation</li>
  <li>Cost volume is computed based on source feature map and warped target feature map</li>
  <li>Optical flow estimator: a DenseNet type of network that takes (1) source feature map (2) cost volume (3) upsampled previous optical flow estimate</li>
  <li>Context network: a dilated convolution network to post process the estimated optical flow</li>
</ol>

<p>Remarks:</p>
<ul>
  <li>Multi-scale training loss</li>
  <li>Network at each scale estimates the optical flow for that scale, not the residual optical flow (the addition happens implictly inside the optical flow estimator).</li>
</ul>

<h3 id="irr-pwcnet-cvpr-2019-paper">IRR-PWCNet (CVPR 2019) <a href="https://arxiv.org/abs/1904.05290">paper</a></h3>

<p style="text-align: center;"><img src="/images/optical_flow/IRR_PWCNet.png" alt="IRR PWCNet" /></p>

<p>Key ideas</p>
<ul>
  <li>Take the output from a previous pass through the network as input and iteratively refine it by only using a single network block with shared weights, which allows the network to residually refine the previous estimate.</li>
  <li>For PWCNet, the decoder module at different pyramid level is achieved using a 1x1 convolution before feeding the source feature map to the optical flow estimator/decoder.</li>
  <li>Joint occlusion and bidirectional optical flow estimation leads to further performance enhancement.</li>
</ul>

<h3 id="pwcnet-fusion-wacv-2019-paper">PWCNet Fusion (WACV 2019) <a href="https://arxiv.org/abs/1810.10066">paper</a></h3>

<p style="text-align: center;"><img src="/images/optical_flow/PWCNet_Fusion.png" alt="PWCNet Fusion" /></p>

<p>The paper focuses on three-frame optical flow estimation problem: given $I_{t-1}$, $I_{t}$, and $I_{t+1}$, estimate $f_{t\to t+1}$.</p>

<p>Key ideas:</p>
<ul>
  <li>If we are given $f_{t-1\to t}$ and $f_{t\to t-1}$, and assume constant velocity of movement, then an estimate of $f_{t\to t+1}$ can be formed by backward warping $f_{t-1\to t}$ with $f_{t\to t-1}$.
<script type="math/tex">% <![CDATA[
\begin{align*}
&\widehat{f}_{t\to t+1} \triangleq \texttt{warp}(f_{t-1 \to t}, f_{t\to t-1}), \\
&\widehat{f}_{t\to t+1}(x, y) \triangleq f_{t-1 \to t}\left(x+f_{t\to t-1}(x,y)(x), y+f_{t\to t-1}(x,y)(y)\right)
\end{align*} %]]></script></li>
  <li>With three frames available, we can plug-in any two-frame optical flow estimation solution (PWCNet in this case) to obtain $f_{t-1 \to t}$, $f_{t\to t+1}$ and $f_{t \to t-1}$.</li>
  <li>A fusion network (similar to the one used in the last stage of FlowNet 2.0) can be used to fuse together <script type="math/tex">\widehat{f}_{t \to t-1}\triangleq\texttt{warp}(f_{t-1 \to t}, f_{t\to t-1})</script> and <script type="math/tex">f_{t \to t+1}</script>.
    <ul>
      <li>Note that <script type="math/tex">\widehat{f}_{t\to t-1}</script> would be identical to <script type="math/tex">f_{t\to t+1}</script> if (a) velocity is constant (b) three optical flow estimations are correct, and (c) there are no occlusions. Brightness constancy errors of the two flow maps together with the source frame $I_t$ are fed into the fusion network to provide additional info.</li>
    </ul>
  </li>
</ul>

<p>Why multi-frame may perform better than 2-frame solutions:</p>
<ul>
  <li>temporal smoothness leads to additional regularization.</li>
  <li>longer time sequences may help in ambiguous situations such as occluded regions.</li>
</ul>

<h3 id="scopeflow-cvpr-2020-paper-code">ScopeFlow (CVPR 2020) <a href="https://arxiv.org/abs/2002.10770">paper</a> <a href="https://github.com/avirambh/ScopeFlow">code</a></h3>

<p>ScopeFlow revisits the following two parts in the conventional end-to-end training pipeline/protocol</p>
<ul>
  <li>Data augmentation:
    <ol>
      <li>photometric transformations: input image perturbation, such as color and gamma corrections.</li>
      <li>geometric augmentations: global or relative affine transformation, followed by random horizontal and vertical flipping.</li>
      <li>cropping</li>
    </ol>
  </li>
  <li>Regularization
    <ul>
      <li>weighted decay</li>
      <li>adding random Gaussian noises</li>
    </ul>
  </li>
</ul>

<p>and advocates</p>
<ul>
  <li>use larger scopes (crops and zoom-out) when possible.</li>
  <li>gradually reduce regularization</li>
</ul>

<h3 id="maskflownet-cvpr-2020-paper-code">MaskFlownet (CVPR 2020) <a href="https://arxiv.org/abs/2003.10955">paper</a> <a href="https://github.com/microsoft/MaskFlownet">code</a></h3>

<p style="text-align: center;"><img src="/images/optical_flow/AsymOFMM.png" alt="Asymmetric Occlusion-aware Featuring Matching Module" /></p>

<p style="text-align: center;"><img src="/images/optical_flow/MaskFlowNetS.png" alt="MaskFlowNetS" /></p>

<p>Key idea:</p>
<ul>
  <li>Incorporates a learnable occlusion mask that filters occluded areas immediately after feature warping without any explicit supervision.</li>
</ul>

<h3 id="raft-recurrent-all-pairs-field-transforms-for-optical-flow-arxiv-2020-paper-code">RAFT: REcurrent All-Pairs Field Transforms for Optical Flow (Arxiv 2020) <a href="https://arxiv.org/abs/2003.12039">paper</a> <a href="https://github.com/princeton-vl/RAFT">code</a></h3>

<p>To be added</p>

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://yyang768osu.github.io/tags/#computer-vision" class="page__taxonomy-item" rel="tag">computer vision</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://yyang768osu.github.io/posts/2020/06/optical_flow/" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://yyang768osu.github.io/posts/2020/06/optical_flow/" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=https://yyang768osu.github.io/posts/2020/06/optical_flow/" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://yyang768osu.github.io/posts/2020/06/optical_flow/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>
      
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://yyang768osu-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>



      


  <nav class="pagination">
    
      <a href="https://yyang768osu.github.io/posts/2019/07/mcmc/" class="pagination--pager" title="Markov Chain Monte Carlo: Gibbs, Metropolis-Hasting, and Hamiltonian
">Previous</a>
    
    
      <a href="https://yyang768osu.github.io/posts/2020/06/arithmetic-coding/" class="pagination--pager" title="Understanding and Implementing Arithmetic Coding (AC)
">Next</a>
    
  </nav>

    </div>

    
  </article>



  
  
</div>


    </script>

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/yyang768osu"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://yyang768osu.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Yang Yang. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://yyang768osu.github.io/assets/js/main.min.js"></script>




  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-123722738-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-123722738-1');
</script>







  </body>
</html>

