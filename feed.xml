<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://yyang768osu.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yyang768osu.github.io/" rel="alternate" type="text/html" /><updated>2020-05-16T22:39:23-07:00</updated><id>https://yyang768osu.github.io/</id><title type="html">Yang Yang</title><subtitle>Engineer at Qualcomm</subtitle><author><name>Yang Yang</name></author><entry><title type="html">Notes on Abstract Algebra</title><link href="https://yyang768osu.github.io/posts/2019/08/abstract_algebra/" rel="alternate" type="text/html" title="Notes on Abstract Algebra" /><published>2019-08-01T00:00:00-07:00</published><updated>2019-08-01T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2019/08/notes-on-abstract-algebra</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2019/08/abstract_algebra/">&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=VdLhQs_y_E8&quot;&gt;lecture 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Group $(G, \bullet)$ is a set $G$ equipped with a binary operation (often called a product operation) $\bullet$ that satisfies four conditions: closure, associativity, identity and invertibility, together called group axiom. The one that further satisfies commutativity is called Abelian group. One immediate property of group is that every element’s inverse is unique.&lt;/p&gt;

&lt;p&gt;One example is generalized linear group $G=GL_n(\mathbb{R})\subset M_n(\mathbb{R})$, with $\bullet$ being matrix multiplication. Note that there is no addition defined in $GL_n(\mathbb{R})$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
GL_n(\mathbb{R}) = \{A_{n\times n}: \det(A)\not=0\} = \{A_{n\times n}:\text{ there is an inverse matrix }A^{-1}\}
\end{align*}&lt;/script&gt;

&lt;p&gt;To prove that it is closed under matrix multiplication, we need to show that $A\bullet B$ is invertible if $A$ and $B$ are invertible. Two proofs: (1) $A\bullet B\bullet (B^{-1}\bullet A^{-1}) = I$ (2) $\det(A\bullet B)=\det{A}\det{B}$. Since matrix multiplication is not commutative, $GL_n(\mathbb{R})$ is non-Abelian.&lt;/p&gt;

&lt;p&gt;Another example is integer $\mathbb{Z}$ with $\bullet$ being integer addition. It is an Abelian group.&lt;/p&gt;

&lt;p&gt;Another example is vector space $V$ with vector addition. It is also an Abelian group.&lt;/p&gt;

&lt;p&gt;For any set $T$, the set of all bijections (one-to-one and onto, aka injection and surjection) also forms a group with $\bullet$ being composition of maps. This is THE most general group. ALL groups arise by putting extra conditioning on this invertible bijection.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
G=\left\{\text{all bijections } g:T\to T\right\} = \text{Sym}\{T\}
\end{align*}&lt;/script&gt;

&lt;p&gt;In particular, as a famous group, for a finite set $T={1,2,\ldots, n}$, we note the symmetry group of $T$, $\text{Sym}{T}$ as symmetry group $S_n$. This is a finite group of order $n!$ that contains all permutations of a set with $n$ elements, and is non-Abelian once $n\geq 3$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=e-a8auViFM0&amp;amp;list=PLelIK3uylPMGzHBuR3hLMHrYfMqWWsmx5&amp;amp;index=2&quot;&gt;lecture 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Auto-morphism is same as bijection. $\text{Sym}(T)$ is also noted $\text{AA}(T)$, with $AA$ being All Auto-morphism.&lt;/p&gt;

&lt;p&gt;A subgroup $H\subset G$ is a subset that is closed under $\bullet$, contains the identity, and contains the inverses. Two trivial subgroups $G$ and ${e}$.&lt;/p&gt;

&lt;p&gt;$GL_n(\mathbb{R}) \subset \text{Sym}(\mathbb{R}^n)$ are bijections in $\mathbb{R}^n$ that preserve linear structure.&lt;/p&gt;

&lt;p&gt;$S_n=\text{Sym}\{1,2,\ldots, n\}$ is called permutation group of size $n$. $|S_n| = n!$.&lt;/p&gt;

&lt;p&gt;$S_3 = \{e, \tau, \tau’, \tau’’, \sigma, \sigma’\}$. $\tau$ is called transposition and $\sigma$ is rotation. Since $S_3$ is non-Abelian, $S_n$ for $n&amp;gt;3$ is definitely non-Abelian given $S_3$ is a subgroup of $S_n$.&lt;/p&gt;

&lt;p&gt;Proposition: The subgroup of $(\mathbb{Z}, +)$ are precisely given by $(b\mathbb{Z}, +)$ where $b$ is a fixed integer.&lt;/p&gt;

&lt;p&gt;For any group $G$, and any element $g\in G$, there is a natural subgroup $H=\langle g\rangle$, called cyclic subgroup, generated by $g$, consists of all product of $g$ with itself. It is the smallest subgroup containing the element $g$. $\langle g\rangle=\{e, g, g^{-1}, g^2, g^{-2}, \ldots\}$, it is certainly closed under product and has inverse. Do not necessary think that those powers are distinct!&lt;/p&gt;

&lt;p&gt;If $g^m=e$ and $m$ is the smallest such power, we say $m$ is the order of $g\in G$. If no power $g^m=e$, we say that $g$ has infinite order.&lt;/p&gt;

&lt;p&gt;Big theorem: In a finite group, every element has finite order, and the order divides the order of the group. In infinite group, element can either has finite or infinite order.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=mwcNETa0KFI&amp;amp;list=PLelIK3uylPMGzHBuR3hLMHrYfMqWWsmx5&amp;amp;index=3&quot;&gt;lecture 3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Examples of groups: $GL_n(\mathbb{R})$, symmetric group on $n$ letters $S_n$, group of integer under addition.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;All groups arise as a structure-preserving bijection of something.&lt;/li&gt;
  &lt;li&gt;Making new groups out of old&lt;/li&gt;
  &lt;li&gt;Subgraph structure of a group&lt;/li&gt;
  &lt;li&gt;Cyclic subgraph&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Isomorphism&lt;/p&gt;

&lt;p&gt;Consider the following groups:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$G_1=\{1, -1, i, -i\}\subset \mathbb{C}^*\triangleq(\mathbb{C}\backslash \{0\}, \times)$.&lt;/li&gt;
  &lt;li&gt;$G_2$: cyclic subgroup of $S_4$ (special group on four letters) generated by $\rho$, which cycles $(1,2,3,4)$ to $(2,3,4,1)$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$G_1$ and $G_2$ have the same multiplication table, but with different labeling.&lt;/p&gt;

&lt;p&gt;Formalization&lt;/p&gt;

&lt;p&gt;Isomorphism is a map $f$ between two groups which is bijective $G_1\to G_2$ such that it preserves the multiplication operations $f(x\bullet_{G_1}y) = f(x)\bullet_{G_2}f(y)$. $f$ is telling us how to relabel things.&lt;/p&gt;

&lt;p&gt;Fact&lt;/p&gt;

&lt;p&gt;Every two cyclic groups of order $n$ are isomorphic.&lt;/p&gt;

&lt;p&gt;Definition: a cyclic group is a group generated by some element. $G_1$ and $G_2$ are isomorphic if there is an isomorphism $f: G_1\to G_2$ between them.&lt;/p&gt;

&lt;p&gt;$(\mathbb{R}, +)$ and $(\mathbb{R}_{&amp;gt;0}, \times)$ are isomorphic with the isomorphic $f(x)=exp(x)$. They are non-finite and non-cyclic.&lt;/p&gt;

&lt;p&gt;Example for finite groups that are not cyclic. Klein-4 group. Two ways to write it.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$G_1:\{e, \tau_1=((1,2)(3,4)), \tau_2=((1,3),(2,4)), \tau_1\tau_2=((1,4),(2,3))\}$.&lt;/li&gt;
  &lt;li&gt;$G_2: \{I, (-1,0; 0, 1), (1, 0; 0, -1), (-1,0;0;-1)\}$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Is Klein-4 group isomorphic to $\{1, -1, i, -i\}$. No, the former does not have elements of order 4 whereas $i$ and $-i$ are with order $4$.&lt;/p&gt;

&lt;p&gt;Some properties to test whether groups are isomorphic:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$|G_1|=|G_2|$&lt;/li&gt;
  &lt;li&gt;$G_1$ abelian $\Leftrightarrow$ $G_2$ abelian&lt;/li&gt;
  &lt;li&gt;$G_1$ and $G_2$ have the same number of elements of every order.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Yang Yang</name></author><category term="algebra" /><summary type="html">lecture 1</summary></entry><entry><title type="html">Markov Chain Monte Carlo: Gibbs, Metropolis-Hasting, and Hamiltonian</title><link href="https://yyang768osu.github.io/posts/2019/07/mcmc/" rel="alternate" type="text/html" title="Markov Chain Monte Carlo: Gibbs, Metropolis-Hasting, and Hamiltonian" /><published>2019-07-06T00:00:00-07:00</published><updated>2019-07-06T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2019/07/markov-chain-monte-carlo</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2019/07/mcmc/">&lt;p&gt;A fundamental problem in statistical learning is to compute the expectation with respect to some target probability distribution $\pi$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbb{E}_\pi\left[f\right] \triangleq \int \pi(x) f(x) dx.
\end{align*}&lt;/script&gt;

&lt;p&gt;There are two difficulties in the evaluation of the above (1) often $\pi(\cdot)$ is available to us only as a form of unnormalized probability, i.e., it can be evaluated only up to a normalizing constant (2) even if $\pi(\cdot)$ can be evaluated exactly, it is often hard to directly generate samples from it (e.g., for high-dimensional space).&lt;/p&gt;

&lt;p&gt;One example application is Bayesian inference, where the posterior probability of the latent $\pi(x|D)$ is available only in the form of prior $\pi(x)$ times likelihood $\pi(D|x)$ up to the unknown normalizing constant of $\pi(D)$, and we would like to either sample or obtain the expectation with respect to the posterior probability.&lt;/p&gt;

&lt;p&gt;The idea of Markov Chain Monte Carlo (MCMC) is to construct a Markov chain whose stationary distribution is exactly the target distribution with easy-to-sample transition kernels. One could then start with a random initial state, and yield samples by simply running the transitions and use the generated samples after the chain reaches steady state for the Monte Carlo evaluation of the expectation.&lt;/p&gt;

&lt;p&gt;For the design of such Markov chain, all methods that I encountered utilize the following theorem&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;An irreducible and aperiodic Markov chain with transition probability $P$ has stationary distribution of $\pi$ if it satisfies \begin{align} 
\pi(x)P(x’|x) = \pi(x’)P(x|x’) \notag
\end{align}&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The game, then, is to design $P$ for which the above equality holds. In this article, we will go through three MCMC methods with different ways in the design of $P$, namely &lt;strong&gt;Gibbs sampling&lt;/strong&gt;, &lt;strong&gt;Metropolis-Hastings&lt;/strong&gt;, and &lt;strong&gt;Hamiltonian Monte Carlo&lt;/strong&gt; (HMC).&lt;/p&gt;

&lt;p&gt;As a side note, it is worth pointing out that the above equation, referred to as &lt;em&gt;detailed balance equation&lt;/em&gt;, is a sufficient but not necessary condition for a Markov chain to have stationary distribution $\pi$. It defines a special case of Markov chain called reversible Markov chain. The detailed balance equation should be contrasted with &lt;em&gt;global balance equation&lt;/em&gt; below, which all Markov chains with stationary distribution $\pi$ satisfy. Then it shouldn’t be surprising that global balance equation can be easily derived from detailed balance equation (by summing over $x’$ on both sides of Equation (1)) but not the other way around.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\pi(x) = \sum_{x'} \pi(x')P(x'|x).
\end{align*}&lt;/script&gt;

&lt;h2 id=&quot;gibbs-sampling&quot;&gt;Gibbs sampling&lt;/h2&gt;

&lt;p&gt;In Gibbs sampling, the transition probability $P$ is defined as the following&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
P\left(x'|x\right)=\left\{
\begin{array}{ll}
\frac{1}{d}\pi\left(x'_j|x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_d\right) &amp; \text{if there exits }j\text{ such that }x_i'=x_i\text{ for }i\not=j.\\
0&amp;\text{otherwise.}
\end{array}
\right.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The state $x$ is a vector of dimension $d$, and the transition probability from state $x$ to state $x’$ is non-zero when they differ by only one dimension, say dimension $j$, and the transition probability is designed to be the conditional probability of $x’_j$, given all the other dimensions fixed, scaled by $1/d$. This corresponds to a transition scheme where we uniformly pick a dimension $j$, and then randomly sample a value in dimension $j$ following the conditional distribution. Detailed balance equation holds with such design&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\pi(x)P(x'|x)\\
=&amp;\frac{1}{d}\pi(x)\pi(x'_j|x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_d)\\
=&amp; \frac{1}{d}\pi(x)\pi(x')/\sum_{z}\pi(x_1, \ldots, x_{j-1}, z, x_{j+1}, \ldots, x_d)\\
=&amp;\frac{1}{d}\pi(x)\pi(x')/\sum_{z}\pi(x'_1, \ldots, x'_{j-1}, z, x'_{j+1}, \ldots, x'_d)\\
=&amp;\frac{1}{d}\pi(x')\pi(x_j|x'_1, \ldots, x'_{j-1}, x'_{j+1}, \ldots, x'_d)\\
=&amp;\pi(x')P(x|x').
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The premise of Gibbs sampling is that the conditional distribution of one dimension given the rest is much easier to normalize and sample from. It is quite limited though, in the sense that the transition can never go very far in each step – only one dimension can be changed at a time. As a consequence, the transition matrix is quite sparse and the Markov chain may suffer from very large mixing time (time to stationary distribution) and it may not scale well with large dimensional space.&lt;/p&gt;

&lt;h2 id=&quot;metropolis-hastings&quot;&gt;Metropolis Hastings&lt;/h2&gt;

&lt;p&gt;Metropolis Hastings algorithm is a much more general version of Gibbs; in fact it encompasses both Gibbs sampling and Hamiltonian MC as special realizations. The basic idea is to construct the transition distribution from a flexible form of proposal distribution $g(x’|x)$, corrected by a &lt;em&gt;acceptance ratio&lt;/em&gt; term $A(x’,x)$  to guarantee reversibility in time. Specifically, the acceptance ratio is chosen to enforce the detailed balance equation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\pi(x) g(x'|x) A(x', x) = \pi(x') g(x|x') A(x, x').
\end{align*}&lt;/script&gt;

&lt;p&gt;The actual transition probability is then $P(x’|x) \triangleq g(x’|x) A(x’, x)$, corresponding to a sampling scheme where we first sample from $g(x’|x)$ to have a candidate next state $x’$, and then accept this candidate with probability $A(x’, x)$. If the candidate state is rejected, the next state will remain the same as the current state. For an arbitrary proposal distribution $g$, from the above equation, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\frac{A(x', x)}{A(x, x')} = \frac{\pi(x')g(x|x')}{\pi(x)g(x'|x)}.
\end{align*}&lt;/script&gt;

&lt;p&gt;To reduce the mixing time of the Markov chain, it is desirable to maximize the acceptance ratio $A$. This means that we want to set either $A(x’,x)$ or $A(x, x’)$ to be $1$ for any pair of $x$ and $x’$, resulting in the expression below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
A(x', x) = \min\left\{1, \frac{\pi(x')g(x|x')}{\pi(x)g(x'|x)}\right\}.
\end{align}&lt;/script&gt;

&lt;p&gt;In the above equation, since $\pi$ appear in both numerator and denominator, we can easily work with unnormalized probability distribution, as long as it can be evaluated efficiently for each data point.&lt;/p&gt;

&lt;p&gt;Metropolis-Hasting algorithm itself is just a MCMC framework; it still relies on a good choice of proposal distribution to perform well. The design of $g$ can be problem specific and is the &lt;em&gt;art&lt;/em&gt;. The clear optimal choice of is $g(x’|x)=\pi(x)$, which degenerates to the direct sampling of $\pi$ with acceptance ratio of $1$.&lt;/p&gt;

&lt;h2 id=&quot;hamiltonian-monte-carlo&quot;&gt;Hamiltonian Monte Carlo&lt;/h2&gt;

&lt;p&gt;Let’s now image a high dimensional surface for which the potential energy at each point $x$ is defined as $V(x)\triangleq -\log\pi(x)$. Here we introduce an auxiliary variable $p$ with the same dimension as $x$, and interpret the pair of variable $(x, p)$ as describing the position and momentum of an object on the high dimensional space.&lt;/p&gt;

&lt;p&gt;The kinetic energy of the object with mass $m$ and momentum $p$ is known as $K(p)=\frac{p^2}{2m}$ (e.g., $\frac{1}{2}mv^2 = (mv)^2/2m$). We now construct a joint probability distribution of $(x,p)$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\pi(x, p) = \frac{1}{Z}e^{-V(x)-K(p)} = \frac{1}{Z} e^{\log\pi(x)}e^{p^2/2m} = \frac{1}{Z}\pi(x)\mathcal{N}\left(p|0, \sqrt{m}\right).
\end{align*}&lt;/script&gt;

&lt;p&gt;Two remarks here: (1) The joint probability defined above is a function of the total energy $V(x) + K(p)$ (potential energy plus kinetic energy) of the imaginary object. (2) Since the marginal distribution of $\pi(x, p)$ with respect to $x$ is $\pi(x)$, if we can construct an effective MCMC algorithm for $(x, p)$, we then obtain an MCMC algorithm for $x$ by discarding $p$.&lt;/p&gt;

&lt;p&gt;The key in Hamiltonian MC is to use Hamiltonian mechanism as a way to obtain new candidate state (corresponding to proposal $g$ in Metropolis-Hastings).  Hamiltonian mechanics is an alternative reformation of the classic Newtonian mechanics describing Newton’s second law of motion. It characterizes the time evolution of the system in terms of location $x$ and momentum $p$, with the conservation of the sum of potential energy $V(x)$ and Kinetic energy of $K(p)$, a.k.a. Hamiltonian $\mathcal{H}(x, p) \triangleq V(x) + K(p)$, through the following differential equations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\frac{d p}{dt} =&amp; -\frac{\partial \mathcal{H}}{\partial x} &amp;\text{force equals to negative gradient of potential energy}\\
\frac{d x}{dt} =&amp; \frac{\partial \mathcal{H}}{\partial p} &amp;\text{velocity equals to derivative of kinetic energy w.r.t. momentum}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;By solving the path of $(x, p)$ according to Hamiltonian mechanics, we are essentially traversing along the contour for which $\pi (x, p)$ is fixed. This provide a very nice way of coming up with a proposal function $g(x’, p’| x, p)$ without having to reject any candidate. In other words, if we start with the point $(x, p)$ and derive the system state $(x_\tau, p_\tau)$ after a period of time $\tau$ , we then know that $\pi(x, p) = \pi(x_\tau, p_\tau)$. If we further apply a negation in the momentum, then the proposal function is reversible.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x, p \xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau \xrightarrow[]{\substack{\text{negate}\\\text{momentum}}} x_\tau, -p_\tau  = x', p'\\
x', p' \xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x'_\tau, p'_\tau \xrightarrow[]{\substack{\text{negate}\\\text{momentum}}}x'_\tau, -p'_\tau  = x, p
\end{align*}&lt;/script&gt;

&lt;p&gt;If we have perfect solver for the differential equation, then according to Equation (2) there is no need to reject any transition proposal. However, in reality the differential equation can only be solved in approximation with error, and thus $\pi(x, p)\not=\pi(x’, p’)$, meaning that the acceptance ratio is not strictly $1$ and certain fraction of the transition proposal would be rejected. It is worth noting that the method for computing the solution to the differential equation should still be reversible to respect the detailed balance equation. One hidden condition for such transition to be feasible is that the potential energy $V(\cdot)$ has to be differentiable, implying that the target distribution $\pi(\cdot)$ should be differentiable.&lt;/p&gt;

&lt;p&gt;So now we have defined a proposal function according to Hamiltonian mechanics, which leads to large acceptance ratio. Are we done here? Not yet. If we stop here, then the Markov chain we defined is reducible, i.e., not every state is accessible from an initial state. In fact, we only have pairwise transition in the Markov chain. To ensure the sampling of the entire space, another proposal distribution $g_2$ is introduced, taking advantage of the fact that $\pi(x, p)$ has factorized form for which $p$ follows a zero-mean normal distribution – the proposal distribution $g_2$ simply samples the momentum value $p$ from the corresponding marginal distribution. For such proposal, the corresponding acceptance ratio is $1$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
A((x, p'), (x, p)) = \min\left\{1, \frac{\pi(x, p')g_2(p|p')}{\pi(x, p)g_2(p'|p)}\right\} = \min\left\{1, \frac{\pi(x)}{\pi(x)}\right\}=1 .
\end{align*}&lt;/script&gt;

&lt;p&gt;Now we concatenate the above two proposals to have the final form of Hamiltonian MC sampling&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x, p_0 \xrightarrow[]{\substack{\text{resample}\\\text{momentum}}} x, p 
\xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau 
\xrightarrow[]{\substack{\text{negate}\\\text{momentum}}} x_\tau, -p_\tau  = x', p'.
\end{align*}&lt;/script&gt;

&lt;p&gt;Since every time after applying the Hamiltonian mechanics the momentum is resampled, we can ignore the momentum negation operation, leading to the following&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x \xrightarrow[]{\substack{\text{resample}\\\text{momentum}}} x, p 
\xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau 
\xrightarrow[]{\substack{\text{discard}\\\text{momentum}}} x_\tau = x',
\end{align*}&lt;/script&gt;

&lt;p&gt;and the corresponding acceptance ratio is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
A((x_\tau, p_\tau), (x, p)) = \min\left\{1, \frac{\pi(x_\tau, p_\tau)}{\pi(x, p)}\right\} =  \min\left\{1, e^{\mathcal{H}(x, p) - \mathcal{H}(x_\tau, p_\tau)}\right\}.
\end{align*}&lt;/script&gt;</content><author><name>Yang Yang</name></author><category term="Monte Carlo" /><summary type="html">A fundamental problem in statistical learning is to compute the expectation with respect to some target probability distribution $\pi$</summary></entry><entry><title type="html">Normalizing Flow I: understanding the change of variable equation</title><link href="https://yyang768osu.github.io/posts/2019/03/normalizing-flow-1/" rel="alternate" type="text/html" title="Normalizing Flow I: understanding the change of variable equation" /><published>2019-03-15T00:00:00-07:00</published><updated>2019-03-15T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2019/03/normalizing-flow-I-change-of-variable-equation</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2019/03/normalizing-flow-1/">&lt;p&gt;Normalizing flow is a technique for constructing complex probability distributions through invertible transformations of a simple distribution. It has been studied and applied in generative models under two contexts: (1) characterizing the approximation posterior distribution of latent variables in the case of variational inference (2) directly approximating the data distribution. When used in the second context, it has demonstrated its capability in generating high-fidelity audio, image, and video data.&lt;/p&gt;

&lt;p&gt;The study of generative models is all about learning a distribution &lt;script type=&quot;math/tex&quot;&gt;\mathbb{P}_{\mathcal{X}}&lt;/script&gt; that fits the data $\mathcal{X}$ well. With such distribution $\mathbb{P}(\mathcal{X})$ we can, among other things, generate, by sampling from  $\mathbb{P}_{\mathcal{X}}$, artificial data point that resembles $\mathcal{X}$. Since the true data distribution lies in high-dimensional space and is potentially very complex, it is essential to have a parameterized distribution family that is flexible and expressive enough to approximate the true data distribution well.&lt;/p&gt;

&lt;p&gt;The idea of flow-based methods is to &lt;em&gt;explicitly&lt;/em&gt; construct a parameterized family of distributions by transforming a known distribution &lt;script type=&quot;math/tex&quot;&gt;\mathbb{P}_{\mathcal{Z}}&lt;/script&gt;, e.g., a standard multi-variant Gaussian, through a concatenation of function mappings. Let’s consider the elementary case of a single function mapping $g$. For each sampled value $z$ from $\mathbb{P}_{\mathcal{Z}}$, we map it to a new value $x=g(z)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
z \xrightarrow{g(.)}  x
\end{align*}&lt;/script&gt;

&lt;p&gt;Up until this point, we have not introduced anything new. This way of transforming a known distribution using a function mapping $g$ is also adopted by generative adversarial networks (GAN). The question that flow-based method asks is: can we get a tractable probability density function (pdf) of $x=g(z)$? If so, we can &lt;em&gt;directly&lt;/em&gt; optimize the probability density of the dataset, i.e., the log likelihood of the data, rather than resorting to the duality approach adopted by GAN, or the lower-bound approach adopted by VAE.&lt;/p&gt;

&lt;p&gt;Unfortunately, for a general function $g$ that maps $z$ to $x$, the pdf of the new random variable $x=g(z)$ is quite complicated and usually intractable due to the need to calculate a multi-dimensional integral. However, if we restrict $g$ to be a bijective (one-to-one correspondence) and differentiable function, then the general change-of-variable technique reduces to the following tractable form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbb{P}_\mathcal{X}(x) =  \mathbb{P}_{\mathcal{Z}}(z)\left|\det \frac{d g(z)}{d z}\right|^{-1}, x=g(z)
\end{align*}&lt;/script&gt;

&lt;p&gt;An important consequence with the bijective assumption is that $z$ and $x$ must have the same dimension: if $z$ is a $d-$dimensional vector $z=[z_1, z_2, \ldots, z_d]$, the corresponds $x$ must also be a $d-$dimensional vector $x=[x_1, x_2, \ldots, x_d]$. It is worth emphasizing that the bijective assumption is essential to the tractability of the change-of-variable operation, and the resulting dimension invariance is a key restriction in flow-based methods.&lt;/p&gt;

&lt;p&gt;The above equation, albeit tractable, looks by no means familiar or friendly — what is with the absolute value? the determinant? the Jacobian? the inverse? The whole equation screams for an intuitive explanation. So here we go — let’s gain some insights into the meaning of the formula.&lt;/p&gt;

&lt;p&gt;First off, since $g$ is bijective and thus invertible, we can denote the inverse of $g$ as $f=g^{-1}$, which allows us to rewrite the equation as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbb{P}_\mathcal{X}(x) =  \mathbb{P}_{\mathcal{Z}}(f(x))\left|\det \frac{d x}{d f(x)}\right|^{-1} =  \mathbb{P}_{\mathcal{Z}}(f(x))\left|\det \frac{d f(x)}{d x}\right|
\end{align*}&lt;/script&gt;

&lt;p&gt;In the last equation, we get ride of the inverse by resorting to the identity that the determinant of an inverse is the inverse of the determinant, the intuition of which will become clear later.&lt;/p&gt;

&lt;p&gt;To understand the above equation, we start with a fundamental invariance in the change of probability random variables: &lt;strong&gt;the probability mass of the random variable $z$ in any subset of $\mathcal{Z}$ must be the same as the probability mass of $x$ in the corresponding subset of $\mathcal{X}$ induced by transformation from $z$ to $x$&lt;/strong&gt;, and vice versa.&lt;/p&gt;

&lt;p&gt;Let us exemplify the above statement with an example. Consider the case when $x$ and $z$ are 2 dimensional, and focus on a small rectangular in $\mathcal{X}$ defined by two corner points $(a, b)$ and $(a+\Delta x_1, b + \Delta x_2)$. If $\Delta x_1$ and $\Delta x_2$ are small enough, we can approximate the probability mass on the rectangular as the density $\mathbb{P}_\mathcal{X}$ evaluated at point $(a,b)$ times the area of the rectangular. More precisely,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;P {\big(}  (x_1, x_2) \in [a, a+\Delta x_1]\times[b, b+\Delta x_2] {\big)}\\
\approx&amp; \mathbb{P}_\mathcal{X} ((a, b)) \times \text{area of }[a, a+\Delta x_1]\times[b, b+\Delta x_2]\\
=&amp;\mathbb{P}_\mathcal{X} ((a, b)) \Delta x_1 \Delta x_2
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This approximation is basically assuming that the probabilistic density on the rectangular stays constant and equal to $\mathbb{P}_\mathcal{X} ((a, b))$, which holds asymptotically true as we shrink the width $\Delta x_1$ and height $\Delta x_2$ of the rectangular. The left figure below provides an illustration.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/flow.png&quot; alt=&quot;change of variable equation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now resorting to the aforementioned invariance, the probability mass on the $\Delta x_1 \times \Delta x_2$ rectangular must remain unchanged after the transformation. So what does the rectangular look like after the transformation of $f$? Let us focus on the corner point $(a+\Delta x_1, b)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
f((a+\Delta x_1, b))=&amp;(f_1(a+\Delta x_1,b), f_2(a+\Delta x_1,b))  \\
=&amp; (f_1(a,b), f_2(a,b))  \\
+&amp; \left(\frac{\partial f_1}{\partial x_1}(a, b)\Delta x_1 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_1\right) \text{ }\text{ first order component} \\
+&amp; \left(o(\Delta x_1), o(\Delta x_1)\right) \text{ }\text{ second and higher order residual}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;With $\Delta x_1$ and $\Delta x_2$ small enough, we can just ignore the higher order term and keep the linearized term. As can be seen from the figure above, the rectangular area is morphed into a parallelogram defined by the two vectors&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\left(\frac{\partial f_1}{\partial x_1}(a, b)\Delta x_1 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_1\right)\\
&amp;\left(\frac{\partial f_1}{\partial x_2}(a, b)\Delta x_2 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_2\right)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We have &lt;a href=&quot;https://textbooks.math.gatech.edu/ila/determinants-volumes.html&quot;&gt;geometry&lt;/a&gt; to tell us that the area of a parallelogram is just the absolute determinant of the matrix composed of the edge vectors, which is expressed as below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
{\Bigg|} \det \underbrace{\left[
\begin{array}{ll}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_1}\\
\frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_2}{\partial x_2}
\end{array}
\right]_{(a,b)}}_{\substack{\text{Jacobian of $f$}\\\text{evaluated at $(a,b)$}}}
{\Bigg |} 
\Delta x_1 \Delta x_2
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;By plugging in the above into the invariance statement, we reached the following identity&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbb{P}_\mathcal{X} ((a, b)) \Delta x_1 \Delta x_2 = \mathbb{P}_\mathcal{Z} (f(a, b)) \left|\det {\bf J}_f(a,b)\right| \Delta x_1 \Delta x_2
\end{align*}&lt;/script&gt;

&lt;p&gt;With $\Delta x_1\Delta x_2$ canceled out, we reached our target equation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbb{P}_\mathcal{X} (x) = \mathbb{P}_\mathcal{Z} (f(x)) \left|\det {\bf J}_f(x)\right| = \mathbb{P}_\mathcal{Z} (f(x)) \left|\det \frac{\partial f(x)}{\partial x}\right|.
\end{align*}&lt;/script&gt;

&lt;p&gt;For data with dimension larger than two, the above equation still holds, with the distinctions that the parallelogram becomes a parallelepiped, and the concept of area becomes a more general notion of volume.&lt;/p&gt;

&lt;p&gt;It should be clear now what the physical interpretation is for the absolute-determinant-of-Jacobian — it represents the &lt;strong&gt;local, linearized rate of volume change&lt;/strong&gt; (quoted from &lt;a href=&quot;https://blog.evjang.com/2018/01/nf1.html&quot;&gt;this excellent blog&lt;/a&gt;) for the function transform. Why do we care about the rate of volume change? exactly because of the invariance of probability measure — in order to make sure each volume holds the same measure of probability before and after the transformation, we need to factor in the volume change induced by the transformation.&lt;/p&gt;

&lt;p&gt;With this interpretation that the absolute-determinant-of-Jacobian is just local linearized rate of volume change, it should not be surprising that the determinant of a Jacobian of an inverse function is the inverse of the determinant Jacobian of the original function. In other words, if function $f$ expands a volume around $x$ by rate of $r$, then the inverse function $g=f^{-1}$ must shrink a volume around $f(x)$ by the same rate of $r$.&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="generative model" /><summary type="html">Normalizing flow is a technique for constructing complex probability distributions through invertible transformations of a simple distribution. It has been studied and applied in generative models under two contexts: (1) characterizing the approximation posterior distribution of latent variables in the case of variational inference (2) directly approximating the data distribution. When used in the second context, it has demonstrated its capability in generating high-fidelity audio, image, and video data.</summary></entry><entry><title type="html">Understanding conventional HMM-based ASR training</title><link href="https://yyang768osu.github.io/posts/2018/11/conventional-hmm-asr-training/" rel="alternate" type="text/html" title="Understanding conventional HMM-based ASR training" /><published>2018-11-17T00:00:00-08:00</published><updated>2018-11-17T00:00:00-08:00</updated><id>https://yyang768osu.github.io/posts/2018/11/understanding-conventional-hmm-based-asr-training</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2018/11/conventional-hmm-asr-training/">&lt;p&gt;Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here $\theta$ denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/HMM.png&quot; alt=&quot;conventional HMM based ASR probabilistic assumption&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;maximum-likelihood-training&quot;&gt;Maximum likelihood training&lt;/h2&gt;

&lt;p&gt;In maximum likelihood estimation (MLE), as stated in the equation below, the objective is to maximize the likelihood of the data being generated by the generative model. In other words, we want to find the value of the parameters $\theta$ so that the above model best explains the acoustic features (e.g., spectrogram) that we observe.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\max_\theta \prod_{n=1}^N \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right)\\
=&amp;\arg\max_\theta \sum_{n=1}^N \log \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;For ease of notation, for the rest of this section, let’s ignore the conditioning on ${\bf y}^{(n)}$ (or ${\bf p}^{(n)}$). The difficulty in evaluating the above log-likelihood lies in the need to marginalize over all potential values of ${\bf z}^{(n)}$. This formulation falls right into the discussion of the previous two posts:  &lt;a href=&quot;/posts/2018/08/variational_inference_1/&quot;&gt;variational lower bound&lt;/a&gt; and &lt;a href=&quot;/posts/2018/08/variational_inference_1/&quot;&gt;expectation maximization&lt;/a&gt;, which provide an iterative algorithm to approach the solution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\theta^{[i+1]} = \arg\max_{\theta} \sum_{n=1}^N \int \color{red}{\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)}\log \mathbb{P}\left({\bf x}^{(n)}, {\bf z}^{(n)};\theta\right)d z^{(n)}.
\end{align}&lt;/script&gt;

&lt;p&gt;Most of the computation complexity in the above equation lies in finding the posterior probability of the latent state given the observed $\color{red}{\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta\right)}$. To elaborate on how the posterior probability is computed, let’s expand the acoustic model part in the previous figure as below, which is essentially a hidden-Markov chain.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/HMM2.png&quot; alt=&quot;conventional HMM based acoustic model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The inference problem (finding the posterior of the latent given the observed) in a hidden Markov chain can be solved by a forward-backward algorithm. The algorithm manifests itself as BCJR algorithm in convolutional code bit-level MAP decoding and &lt;a href=&quot;/posts/2018/08/kalman_filter_particle_filter/&quot;&gt;Kalman filtering&lt;/a&gt; in linear dynamic system.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\text{Forward path: }&amp;\text{calculate }\mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\text{ from }\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right) \notag \\
&amp;\mathbb{P}\left(z_t, {\bf x}_{1\to t};\theta\right) = \sum_{z_{t-1}} \mathbb{P}(z_{t}|z_{t-1};\theta)\mathbb{P}(x_{t}|z_{t};\theta)\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right)  \notag \\
\text{Backward path: }&amp;\text{calculate }\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right)\text{ from }\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
&amp;\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) = \sum_{z_{t+1}} \mathbb{P}(z_{t+1}|z_{t};\theta)\mathbb{P}(x_{t+1}|z_{t+1};\theta)\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
\text{Combined: }&amp;\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) \notag \\
&amp;\mathbb{P}\left(z_t| {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) / \sum_{z_t}\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;circular-dependency-between-segmentation-and-recognition&quot;&gt;Circular dependency between segmentation and recognition&lt;/h2&gt;
&lt;p&gt;The expectation-maximization formulation for likelihood maximization reveals a fundamental circular dependency between segmentation and recognition.&lt;/p&gt;

&lt;p&gt;Here &lt;strong&gt;segmentation&lt;/strong&gt; refers to the alignment of sub-phoneme states of ${\bf y}$ and the acoustic feature observations ${\bf x}$, encoded in the hidden state sequence ${\bf z}$, and &lt;strong&gt;recognition&lt;/strong&gt; refers to the classification of sub-phoneme hidden state sequence ${\bf z}$ for the corresponds acoustic feature observations ${\bf x}$.&lt;/p&gt;

&lt;p&gt;The two equations below make the circular dependency precise:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
\theta^{[i]} &amp;
\underset{\text{ }}{\xrightarrow{\substack{\text{update soft-alignment}\\\text{based on recognition}}}} \mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)\text{ using Equation (2)}\\
\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right) &amp;
\underset{\text{ }}{\xrightarrow{\substack{\text{update recognition}\\\text{based on soft-alignment}}}}
\theta^{[i+1]}\text{ using Equation (1)}
\end{align*} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;It is easy to argue that to have an accurate alignment, we need accurate recognition, and to train an accurate recognition, we have to rely on accurate alignment/segmentation.&lt;/p&gt;

&lt;p&gt;In a convention ASR system, to bootstrap the training procedure, we have to start with a dataset that has human curated phoneme boundary/segmentation. Once the system is capacitated with reasonable recognition/inference, it is no longer confined with human aligned dataset and a much larger dataset can be used with just waveform and the corresponding phoneme transcription. Eventually, after the system is able to deliver robust segmentation, we can make hard decision on the alignment, and only focus on improving the recognition performance with potentially a different system that has a much larger capacity, e.g., a DNN replacing the GMM model.&lt;/p&gt;

&lt;h2 id=&quot;decoding&quot;&gt;Decoding&lt;/h2&gt;
&lt;p&gt;In the decoding stage, we try to find the word/sentence with the maximum a posterior (MAP) probability given the observed data&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{=}&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) \\
=&amp;\arg\max_{\bf y} \underbrace{\mathbb{P}({\bf x}|{\bf p};\theta)}_{\text{acoustic model}}\times\underbrace{\mathbb{P}({\bf p}|{\bf y})}_{\text{lexion}}\times\underbrace{\mathbb{P}({\bf y})}_{\text{language model}}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The lexicon and language model together construct a state transition diagram, which we unrolled in time to form a decoding trellis. For each transcription hypothesis, a proper MAP decoder would sum across all the paths in the trellis that corresponds to the transcription, which is computationally prohibitive.&lt;/p&gt;

&lt;p&gt;One simplification one can make is to find the most probable path by running the Viterbi algorithm. However, even for Viterbi algorithm, the complexity is still too high for practical deployment due to the large state space and potentially large number of time steps.&lt;/p&gt;

&lt;p&gt;To further reduce the computation complexity, the conventional system resorts to the beam-search algorithm – basically a breath-first-search algorithm on the trellis that maintain only a limited number of candidates. The beam-search algorithm is often run on a weighted finite state transducer that captures the concatenation of language model and lexicon.&lt;/p&gt;

&lt;h2 id=&quot;discrepancy-between-mle-training-and-map-decoding&quot;&gt;Discrepancy between MLE training and MAP decoding&lt;/h2&gt;

&lt;p&gt;At first glance into the MAP decoding equation, it may appear that the MLE based training is well-aligned with the decoding process: maximizing the posterior probability of the transcription ${\bf y}$ given the acoustic feature ${\bf x}$ is equivalent to maximizing the likelihood of the observation ${\bf x}$. The argument being that the probability of the observation $\mathbb{P}(x;\theta)$ is anyway a constant dictated by the natural of people’s speech, not something we can control. But is it true?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\text{inference time:}\\
&amp;\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;It turns out there is a subtle difference between inference time (MAP decoding) and training time (MLE parameter update) that render the above statement wrong.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\text{training time:}\\
&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{\not=}&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;As is evident by comparing the above two equations, when we try to update parameter $\theta$ to maximize directly the posterior probability of the transcription ${\bf y}$ given the acoustic feature ${\bf x}$, we can no longer ignore the term $\mathbb{P}({\bf x};\theta)$. The key is to realize that we model the speech as a generative model, where &lt;strong&gt;the probability of observing a certain acoustic features ${\bf x}$ is not dictated by the nature, but rather the generative model that we assume&lt;/strong&gt;. By updating the parameter $\theta$ that best increase the likelihood, we inevitably change $\mathbb{P}({\bf x};\theta)$ too, and thus there is no guarantee that the posterior probability is increased. $\mathbb{P}({\bf x};\theta)$ is calculated by marginalizing over all potential transcriptions: $\mathbb{P}({\bf x};\theta)=\sum_{\bf y}\mathbb{P}({\bf x}|{\bf y};\theta)$.&lt;/p&gt;

&lt;p&gt;To elaborate, in MLE, we try to maximize $\color{red}{\mathbb{P}({\bf y}|{\bf x};\theta)}$ with respect to $\theta$, we may very well also increased the likelihood for competing transcription sequences $\color{blue}{\mathbb{P}({\bf x}|{\bf \tilde{y}};\theta)}$, potentially resulting in decreased posterior probability $\mathbb{P}({\bf y}|{\bf x};\theta)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp; \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&amp; \frac{\color{red}{\mathbb{P}({\bf x}|{\bf y};\theta)}\mathbb{P}({\bf y})}{\sum_{\bf \tilde{y}}\color{blue}{\mathbb{P}({\bf x}|{\bf \tilde{y}};\theta)}\mathbb{P}({\bf \tilde{y}})}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Fundamentally, &lt;strong&gt;the misalignment is rooted from the fact that we are using a generative model for discriminative tasks&lt;/strong&gt;. In the next section, we discuss several ways to train the generative model for better discrimination.&lt;/p&gt;

&lt;h2 id=&quot;sequence-discriminative-training&quot;&gt;Sequence discriminative training&lt;/h2&gt;
&lt;p&gt;To bridge the aforementioned discrepancy, several other training targets are proposed.&lt;/p&gt;

&lt;p&gt;to be continued…&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="ASR" /><category term="HMM" /><summary type="html">Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here $\theta$ denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.</summary></entry><entry><title type="html">Comparison of end-to-end ASR models</title><link href="https://yyang768osu.github.io/posts/2018/11/end_to_end_asr_models/" rel="alternate" type="text/html" title="Comparison of end-to-end ASR models" /><published>2018-11-13T00:00:00-08:00</published><updated>2018-11-13T00:00:00-08:00</updated><id>https://yyang768osu.github.io/posts/2018/11/comparison-of-end-2-end-asr-models</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2018/11/end_to_end_asr_models/">&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/end2end.png&quot; alt=&quot;End-to-end ASR model derivation&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/transcription_model.png&quot; alt=&quot;Sentence model for CTC, RNN-T, and Attention&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;language-model-integration&quot;&gt;Language model integration&lt;/h2&gt;</content><author><name>Yang Yang</name></author><category term="ASR" /><summary type="html"></summary></entry><entry><title type="html">Gumbel max and Gumbel softmax</title><link href="https://yyang768osu.github.io/posts/2018/11/gumbel_max_and_gumbel_softmax/" rel="alternate" type="text/html" title="Gumbel max and Gumbel softmax" /><published>2018-11-05T00:00:00-08:00</published><updated>2018-11-05T00:00:00-08:00</updated><id>https://yyang768osu.github.io/posts/2018/11/gumbel-max-and-gumbel-softmax</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2018/11/gumbel_max_and_gumbel_softmax/">&lt;p&gt;Before we talk about Gumbel distribution, let’s refresh our knowledge on exponential distribution. It is well-known that the exponential distribution is min-stable: the min of $n$ I.I.D. exponential random variables  $X_i\sim \text{Exp}(\lambda_i), i=1,2,\ldots, n$ is also exponentially distributed with decay rate $\sum_{1\leq i\leq n} \lambda_i$, as can be seen from the equation below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
P(\min_{1\leq i \leq n} X_i &gt; x) &amp;= \prod_{1\leq i\leq n} P(X_i&gt;x) = e^{-\sum_{1\leq i\leq n}\lambda_i x}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;A lesser known property is that the arg-min of exponential variables is a multinomial distribution with event probabilities $\left(\frac{\lambda_1}{\sum_{i}\lambda_i}, \ldots, \frac{\lambda_n}{\sum_{i}\lambda_i}\right)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;P(\arg\min_{1\leq i\leq n}X_i =k)\\
=&amp;\int_0^\infty P(\arg\min_{1\leq i\leq n} X_i = x | X_k=x)f(X_k = x)dx\\
=&amp;\int_0^\infty \prod_{i=1\to n, i\not=k} e^{-\lambda_i x} \lambda_k e^{-\lambda_k x} dx\\
=&amp;\frac{\lambda_k}{\sum_{i=1\to n} \lambda_i}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;As we will see shortly, this property directly leads to the Gumbel max trick.&lt;/p&gt;

&lt;h2 id=&quot;gumbel-max-trick&quot;&gt;Gumbel max trick&lt;/h2&gt;

&lt;p&gt;Assume that we are given a multinomial (a.k.a. categorical) distribution with unnormalized event probabilities $\lambda_i, i=1\to n$ (i.e., $\sum_{i=1}^n\lambda_i\not=1$), the above property provides us a way to sample from the distribution without the need for normalization:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;draw $n$ samples from an exponential distributions with decay rate of $1$&lt;/li&gt;
  &lt;li&gt;scale the value of these $n$ samples with $1/\lambda_i$ for $i=1\to n$.&lt;/li&gt;
  &lt;li&gt;take the index of the minimum of the scaled samples&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To be more precise, we are utilizing the fact that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\min_{1\leq i\leq n} \left(\frac{1}{\lambda_i}s_i\right) \sim \text{Multinomial}\left(\frac{\lambda_1}{\sum_{i}\lambda_i}, \ldots, \frac{\lambda_n}{\sum_{i}\lambda_i}\right)\\
&amp;\text{where } s_i\sim \text{Exp}(1), \forall i.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;For the case of soft-max operation, we have direct access to the log of the unnormalized probabilities $\alpha_i=\log \lambda_i$ (multinomial logit), instead of the unnormalized probabilities itself. In this case, we can modify the above equation as below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\min_{1\leq i\leq n} \left(\frac{1}{e^{\alpha_i}}s_i\right) \sim \text{Multinomial}\left(\frac{e^{\alpha_1}}{\sum_{i}e^{\alpha_i}}, \ldots, \frac{e^{\alpha_n}}{\sum_{i}e^{\alpha_i}}\right)\\
&amp;\text{where } s_i\sim \text{Exp}(1), \forall i.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;One observation is that the left hand side of the above equation is invariant to any linear transform. The Gumbel-max trick is obtained by taking $-\log(\cdot)$ operation to the right-hand-side, in which case $-\log(\text{Exp}(1))$ is a standard Gumbel distribution, leading to the equation below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\max_{1\leq i\leq n} \left(\alpha_i+g_i\right) \sim \text{Multinomial}\left(\frac{e^{\alpha_1}}{\sum_{i}e^{\alpha_i}}, \ldots, \frac{e^{\alpha_n}}{\sum_{i}e^{\alpha_i}}\right)\\
&amp;\text{where } g_i\sim \text{Gumbel}(\text{location}=0, \text{scale}=1), \forall i.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This provides us a way to obtain samples directly from the logits without going through the exponentiate-and-normalization step&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;draw $n$ samples from a standard Gumbel distributions with location of $0$ and scale of $1$.&lt;/li&gt;
  &lt;li&gt;add the values of the $n$ samples to the logits.&lt;/li&gt;
  &lt;li&gt;take the index of the minimum of the $n$ summations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Essentially, &lt;em&gt;the Gumbel max trick converts the sampling operation from a categorical/multinomial distribution into an argmax operation&lt;/em&gt;. The sampling process can be expedited if we pre-calculate and store a stream of Gumbel samples.&lt;/p&gt;

&lt;h2 id=&quot;gumbel-softmax&quot;&gt;Gumbel softmax&lt;/h2&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/gumbel.png&quot; alt=&quot;Gumbel softmax&quot; /&gt;&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="gumble" /><summary type="html">Before we talk about Gumbel distribution, let’s refresh our knowledge on exponential distribution. It is well-known that the exponential distribution is min-stable: the min of $n$ I.I.D. exponential random variables $X_i\sim \text{Exp}(\lambda_i), i=1,2,\ldots, n$ is also exponentially distributed with decay rate $\sum_{1\leq i\leq n} \lambda_i$, as can be seen from the equation below.</summary></entry><entry><title type="html">An introduction to Kalman filter and particle filter</title><link href="https://yyang768osu.github.io/posts/2018/08/kalman_filter_particle_filter/" rel="alternate" type="text/html" title="An introduction to Kalman filter and particle filter" /><published>2018-08-20T00:00:00-07:00</published><updated>2018-08-20T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2018/08/kalman-filter-and-particle-filter</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2018/08/kalman_filter_particle_filter/">&lt;p&gt;Kalman filter and particle filter are concepts that are intimidating for new learners due to its involved mathmatical discription, and are straightforward once you grasp the main idea and get used to Gaussian distributions. The goal of this post is to take a journey to Kalman filter by dissecting its idea and operation into pieces that are easy to absorb, and then assemble them together to give the whole picture. As a last step, we will see that particle filter achieves the same goal for non-Gaussian system resorting to Monte Carlo sampling.&lt;/p&gt;

&lt;p&gt;Below let’s walk through three simple problems and their solutions stemming from Gaussian distributions, and then stitching them together to form the problem that Kalman filter tries to solve and present its solution.&lt;/p&gt;

&lt;h2 id=&quot;a-conditional-gaussian-distribution&quot;&gt;A. Conditional Gaussian distribution&lt;/h2&gt;

&lt;p&gt;Here I assume you have a basic knowledge regarding multi-variant Gaussian distribution. A multi-variant Gaussian distribution is captured by its mean vector and covariance matrix, often denoted as $\mu$ and $\Sigma$. Below let us consider the bi-variant Gaussian vector $[z,x]^T$, with the following general notation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[
\begin{array}{c}
z\\
x
\end{array}
\right]
\sim
\mathcal{N}
\left(
\left[
\begin{array}{c}
\mu_z\\
\mu_x
\end{array}
\right],
\left[\begin{array}{cc}
\Sigma_z &amp; \Sigma_{zx} \\
\Sigma_{xz} &amp; \Sigma_{x}
\end{array}\right]
\right)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The covariance matrix is formally defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[\begin{array}{cc}
\Sigma_z &amp; \Sigma_{zx} \\
\Sigma_{xz} &amp; \Sigma_{x}
\end{array}\right]
=&amp;
\mathbb{E}\left[
\left[
\begin{array}{c}
z-\mu_z\\
x-\mu_x
\end{array}
\right]
\left[
\begin{array}{c}
z-\mu_z\\
x-\mu_x
\end{array}
\right]^T
\right]\notag\\
=&amp;
\left[
\begin{array}{cc}
\mathbb{E}[(z-\mu_z)(z-\mu_z)^T] &amp; \mathbb{E}[(z-\mu_z)(x-\mu_x)^T]\\
\mathbb{E}[(x-\mu_x)(z-\mu_z)^T] &amp; \mathbb{E}[(x-\mu_x)(x-\mu_x)^T]
\end{array}
\right]\notag.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The off-diagonal term represents the cross covariance between the two random variables $x$ and $z$, which, by checking the definition above, satisfies $\Sigma_{xz}=\Sigma^T_{zx}$. The larger the cross covariance, the more correlated the two random variables are. For correlated random variables, knowing the value of one would help us in guessing the value of the other. Let us take a look at the figure below for a concrete example.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/bivariant_normal.png&quot; alt=&quot;Bivariant Gaussian&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The figure above illustrates the joint distribution of a bivariant Gaussian distribution with $\mu_z=\mu_x=0$, $\Sigma_z=\Sigma_x=1$, and $\Sigma_{zx}=\Sigma_{xz}=0.8$. The marginal distributions of both $x$ and $z$ are $\mathcal{N}(0,1)$. The contour of the distribution forms a thin ellipse, reflecting the strong covariance $\Sigma_{zx}=\Sigma_{xz}=0.8$ between the two random variables $x$ and $z$. To testify the claim that knowing one variable would help us estimating the other,  let us take a look at the the conditional distribution of $z$ given $x=1$, and compare it with the marginal distribution of $z$. As can be seen from the figure above, the distribution of $z|x=1$ has much narrow span than $z$ with a shift in the mean. The reduction of variance of $z$ after observing $x=1$ is an evident that the observation of $x$ narrows down the potential values of $z$.&lt;/p&gt;

&lt;p&gt;An important fact here is that the conditional distribution of a joint-Gaussian distribution is also Gaussian&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
p(z|x) = p(x,z)/p(x)\sim\mathcal{N}\left(\mu_{z|x}, \Sigma_{z|x}\right)\notag.
\end{align*}&lt;/script&gt;

&lt;p&gt;Below are two identities on the general expressions of the conditional distribution, here let us accept them as they are without bothering with any proof.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z|x} &amp;= \mu_z + \Sigma_{zx}\Sigma_{xx}^{-1}(x-\mu_x)\notag\\ 
\Sigma_{z|x} &amp;= \Sigma_{z} - \Sigma_{zx}\Sigma_x^{-1}\Sigma_{xz} \text{ } \left(\preccurlyeq \Sigma_{z}\right)  \notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The above two equations are very important, and lies in the core of many concepts such as MMSE estimator, Wiener filter and of course, Kalman filter. To see that knowing $x$ would reduce the uncertainty in $z$, here let’s just point out that the entropy (measure of uncertainty) of a Gaussian random vector is an increasing function of the determinant of the covariance-matrix, and that $\Sigma_{z|x}$ always &lt;a href=&quot;https://math.stackexchange.com/questions/466158/on-the-difference-of-two-positive-semi-definite-matrices&quot;&gt;has a smaller determinant&lt;/a&gt; than $\Sigma_z$ for any non-zero cross covariance $\Sigma_{zx}$, followed from the second equation with the fact that $\Sigma_{z}-\Sigma_{z|x}$ $=\Sigma_{zx}\Sigma_x^{-1}\Sigma_{xz}\succcurlyeq 0$ is a semi-positive-definite matrix.&lt;/p&gt;

&lt;p&gt;These two equations will become useful when we visit part C, and we will come back to them.&lt;/p&gt;

&lt;h2 id=&quot;b-gaussian-distribution-with-linear-transformation&quot;&gt;B. Gaussian distribution with linear transformation&lt;/h2&gt;

&lt;p&gt;In the first section we looked at the case of obtaining the conditional distribution from a joint Gaussian distribution, here let’s look at the distribution of a Gaussian vector going through a linear transform. More precisely, let us define a random variable $z_n$ as obtained from the following transform&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z_{n-1}&amp;\sim\mathbb{N}(\mu_{n-1}, V_{n-1})\notag\\
z_n |z_{n-1}&amp;\sim A z_{n-1}+a+\mathcal{N}(0,\Gamma)= \mathbb{N}(Az_{n-1}+a, \Gamma)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;One typical example for the above problem setup is the following: consider the problem of tracking the location and velocity of an object traveling in a strict line. Let us label $z_n=[\text{loc}_n, \text{vel}_n]^T$ as the location-velocity state of the object in time-step $n$. To model the estimation inaccuracy, assume that $z_n$ is a random variable with mean $\mu_{n-1}$ and variance $V_{n-1}$. Here the mean value reflect the estimated value and the variance can be viewed as capturing the amount of and the structure of the uncertainty in the estimate. The location-velocity estimate in the time-step $n$ can be modeled by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[
\begin{array}{c}
\text{loc}_{n}\\
\text{vel}_{n}
\end{array}
\right]=
\underbrace{
\left[
\begin{array}{cc}
1 &amp; 1\notag\\
0 &amp; 1\notag
\end{array}
\right]}_{
\text{loc}_{n} = \text{loc}_{n-1}+\text{vel}_{n-1}
}
\times
\left[
\begin{array}{c}
\text{loc}_{n-1}\\
\text{vel}_{n-1}
\end{array}
\right]
+
\underbrace{
\left[
\begin{array}{c}
a_\text{loc}\notag\\
a_\text{vel}\notag
\end{array}
\right]}_{\substack{
\text{external known}\\\text{change}
}
}
+
\underbrace{
\left[
\begin{array}{c}
\text{noise}_\text{loc}\notag\\
\text{noise}_\text{vel}\notag
\end{array}
\right]}_{\substack{
\text{additional noise}\\\text{in the system}
}
}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;with a one-to-one correspondence to the conditional probability $z_n|z_{n-1}$ restated above.&lt;/p&gt;

&lt;p&gt;The problem of interest here is to characterize the distribution of $z_n$, given that it is obtained from a linear transformation of a previous estimate with additional Gaussian noise. Precisely, what we want to solve is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(z_n) =&amp; \int p(z_{n}|z_{n-1})p(z_{n-1})dz_{n-1}\notag.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here is another important property of Gaussian distribution: any linear transformation of Gaussian variable is still Gaussian. With this property given, we can calculate the mean and variance of the updated state $z_n$, as shown below, which fully captures its distribution.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z_n} =&amp;\mathbb{E}[Az_{n-1}+a]= A\mu_{n-1}+a\notag\\
\Sigma_{z_n} =&amp;\mathbb{E}[(Az_{n-1}-A\mu_{n-1})(Az_{n-1}-A\mu_{n-1})^T]+\Gamma\notag\\
=&amp; AV_{n-1}A^T + \Gamma\notag,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;which leads to the following solution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
z_n \sim \mathcal{N}\left(A\mu_{n-1}+a, AV_{n-1}A^T + \Gamma\right).
\end{align*}&lt;/script&gt;

&lt;h2 id=&quot;c-bayes-theorem-for-gaussian&quot;&gt;C. Bayes’ theorem for Gaussian&lt;/h2&gt;

&lt;p&gt;In part A, we provide the equation for calculating the conditional distribution from a joint Gaussian distribution, i.e., for a given joint-Gaussian probability $p(x,z)$, the conditional distribution of $p(z|x)$ is also Gaussian and it can be expressed in closed-form.&lt;/p&gt;

&lt;p&gt;In this part, we consider a slightly more complex problem, whereby we are given Gaussian distributions $p(x|z)$ and $p(z)$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z&amp;\sim \mathcal{N}(\nu, P),\notag\\
x|z&amp;\sim \mathcal{N}(Cz+c,\Pi) = Cz+c+\mathcal{N}(0, \Pi),\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the problem is find the posterior distribution $p(z|x)$, which can be expressed using $p(x|z)$ and $p(z)$ by Bayes’ rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
p(z|x) = \frac{p(z)p(x|z)}{p(x)}  \propto p(z)p(x|z).\notag
\end{align*}&lt;/script&gt;

&lt;p&gt;Here’s a typical application for this problem: let us consider the task of estimating the temperature and humidity of a room (denoted as vector $z$). We are given two sources of information: (1) prior knowledge on the distribution from history data and, e.g., $p(z)$ (2) the reading from a thermometer with some known accuracy $p(x|z)$. Intuitively, a good estimate should be obtained by fusing these two informations. Indeed, this is evident from the Bayes’ rule, where the posterior probability of $z$ given $x$ is proportional to the product of the two distributions $p(z)p(x|z)$.&lt;/p&gt;

&lt;p&gt;Since part A taught us how to obtain a conditional distribution from a joint distribution. We can solve this problem by obtaining the joint distribution $p(z,x)=p(z)p(x|z)$ first and then plugin the solution presented in part A.&lt;/p&gt;

&lt;p&gt;For Gaussian, the multi-variant joint distribution is fully captured by the marginalized mean/variance together with the cross-variance among all factors, which, in our case, can be expressed as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{x} &amp;= \mathbb{E}[Cz+c+\mathcal{N}(0, \Sigma)] = C\nu+c\notag\\
\Sigma_{x} &amp;= \mathbb{E}[xx^T] = \mathbb{E}[(Cz-C\nu)(Cz-C\nu)^T]+\Sigma=C P C^T + \Pi\notag\\
\Sigma_{zx} &amp;= \mathbb{E}[z(Cz-C\mu)^T]=P C^T  \notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Accordingly, the joint distribution of $z$ and $x$ can be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[
\begin{array}{c}
z\\
x
\end{array}
\right]
\sim
\mathcal{N}
\left(
\left[
\begin{array}{c}
\nu\\
C\nu+c
\end{array}
\right],
\left[\begin{array}{cc}
P &amp;P C^T \\
CP &amp; C P C^T + \Pi
\end{array}\right]
\right)\notag.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, by plugging in the solution in part A, we can obtain below the expression of the mean and variance of the posterior probability $p(z|x)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z|x} &amp;= \nu + PC^T (CPC^T+\Pi)^{-1}(x-C\nu-c)\notag\\
\Sigma_{z|x} &amp;= P - PC^T (CPC^T+\Pi)^{-1}CP\notag 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;To simplify the expression as well as to gain some insights into the expression, it is necessary to group some of the terms in $K$ below and substitute the corresponding terms.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
K\triangleq PC^T(CPC^T+\Pi)^{-1},\notag
\end{align*}&lt;/script&gt;

&lt;p&gt;resulting in the rewritten form below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z|x} &amp;= {\color{red}\nu} + K(x-C\nu-c)\notag\\
\Sigma_{z|x} &amp;= {\color{red}P}-KCP =(I-KC)P.\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;It is interesting to observe that the highlighted term in the expression above is the mean and the variance of the prior distribution $p(z)$ without taking $p(x|z)$ into account. The effect of the $p(x|z)$ can be thought of as a correction to the prior distribution: the mean is shifted by $K(x-Cv-c)$ and the covariance matrix is reduced by $KCP$ (or shrunk by $(I-KC)$), leading to a refined posterior distribution $p(z|x)$. Here $K$ can be considered as a &lt;em&gt;gain&lt;/em&gt; factor, as it shifts the mean towards that dictated by $x$ and it shrinks the covariance matrix, leading to a more concentrated distributed with less amount of uncertainty.&lt;/p&gt;

&lt;p&gt;Next we will see that Kalman filter is just a repeated (or sequential) application of this Bayes’ rule on Gaussian distribution.&lt;/p&gt;

&lt;h2 id=&quot;kalman-filter&quot;&gt;Kalman filter&lt;/h2&gt;

&lt;p&gt;It’s time to assemble what we learnt from the previous parts. Let’s consider following an evolving system, where the system state $z_n$ follows linear evolving over time, whose true value is hidden from us. Every time instance, we obtain a noisy observation $x_n$ of the system state. The noisy observation $x_n$ may not be directly the state itself, but is in general an linear function of the state of interest, with added Gaussian noise. The task is to keep updating the belief on the system state, based on all the noisy observations, and the knowledge on the system evolution itself. In the degenerated case where the system does not evolve, then the problem amount to the sequential application of Bayes’ rule on the same hidden variable to fuse all the instances of noisy observations.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/linear_dynamic_system.png&quot; alt=&quot;Bivariant Gaussian&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To devise a sequence update rule on the brief of the system state based on all observations $p(z_n| x_1^n)$, let us look at the atomic case when $p(z_{n-1}|x_1^{n-1})$ — the prior belief of the previous state, and $p(x_n|z_n)$ — the noisy observation based on the current state, are given, and the task is to find $p(z_n|x_1^{n})$ — the posterior belief of the current state.&lt;/p&gt;

&lt;p&gt;In other words, we want to find an iterative procedure that update the belief on the system state, based on linear system evolution and noisy state observation. Precisely, we need to solve the following problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\underset{\substack{\\\mathcal{N}(\mu_{n-1}, V_{n-1})}}{p(z_{n-1}|x_1^{n-1})}
\underset{\text{ }}{\xrightarrow{\substack{\text{system evolution }\\p(z_n|z_{n-1})}}}  
\underset{\substack{\\\mathcal{N}(\nu_{n-1}, P_{n-1})}}{p(z_n|x_1^{n-1}) }
\underset{\text{ }}{\xrightarrow{\substack{\text{noisy observation }\\p(x_n|z_{n})}}}  
\underset{\substack{\\\mathcal{N}(\mu_{n}, V_{n})}}{p(z_{n}|x_1^{n})}
\end{align*}&lt;/script&gt;

&lt;!---
Here we assume that the noisy observations $x$ are independent given the underlying system state $z$ (this assumption is actually encoded in the graphic model above), then $p(z_{n-1}\|x_1^{n-1})$  captures all the information regarding $x_1^{n-1}$, and we can simply drop them from the expression. 
--&gt;

&lt;p&gt;The decomposed two sub-problem above correspond to part B and part C respectively, for which we can get the following solution&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$p(z_n|x_1^{n-1}) = \int p(z_n|z_{n-1}) p(z_{n-1}|x_1^{n-1})dz_{n-1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{Input: }&amp;\notag\\
z_{n-1}|x_1^{n-1}\sim&amp;\mathcal{N}(\mu_{n-1}, V_{n-1})\notag\\
z_n|z_{n-1} \sim&amp; \mathcal{N}(Az_{n-1}+a, \Gamma)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{Solution: }&amp;\notag\\
z_n|x_1^{n-1}\sim&amp; \mathcal{N}(\nu_{n-1}, P_{n-1})\notag\\
\nu_{n-1}=&amp;  A\mu_{n-1}+a\notag\\
P_{n-1}=&amp; AV_{n-1}A^T+\Gamma\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$p(z_n|x_1^n)\propto p(x_n|z_n)p(z_n|x_1^{n-1})$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{Input: }&amp;\notag\\
z_n|x_1^{n-1}\sim&amp; \mathcal{N}(\nu_{n-1},P_{n-1})\notag\\
x_n|z_n\sim&amp; \mathcal{N}(Cz_n+c,\Pi)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{Solution: }&amp;\notag\\
z_n|x_1^n \sim&amp;\mathcal{N}(\mu_n, V_n)\notag\\ 
\mu_n =&amp; \nu_{n-1} + K_n(x_n-C\nu_{n-1}-c)\notag\\
V_n =&amp; (I-K_nC)P_{n-1}\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
K_n\triangleq P_{n-1}C^T(CP_{n-1}C^T+\Pi)^{-1}\notag
\end{align*}&lt;/script&gt;

&lt;p&gt;The final solution above is the Kalman filter equation, and $K_n$ is referred to as the Kalman gain.&lt;/p&gt;

&lt;h2 id=&quot;particle-filter&quot;&gt;Particle filter&lt;/h2&gt;

&lt;p&gt;One biggest constraint for the application of Kalman filter is that it assumes a linear dynamic system where the state transition and noisy observations are linear processes, which is evident from the probabilistic-graphic-model diagram shown before. The linear assumption is necessary to make sure that all the distributions involved in the system are Gaussian, which is easy to characterize and analytically tractable.&lt;/p&gt;

&lt;p&gt;In general cases, seldom do we have a system being linear. Even with a linear system, the distribution may not be Gaussian. The most cited example for the explanation of particle filter is localization. We can fit in the problem of localization as a dynamic system with hidden variables and heterogeneous system evolutions. Here the location of the system at time-step $n$ is modeled by the hidden variable $z_n$, and any observations made by accelerometer, GPS, and various other type of sensors are captured in $x_n$. Since $z_n$ represent the belief of the system’s location in a map, it can hardly be described by a Gaussian distribution and may not even have a tractable form. On top of that, the observations made by the sensors may not be a linear function of the system location.&lt;/p&gt;

&lt;p&gt;In this type of systems, instead of trying to deriving the exact distributions of hidden variables, it is more practical to characterize them using sets of samples. The sample update are achieved by a scheme often referred to as sequential Monte Carlo sampling, which we will introduce next.&lt;/p&gt;

&lt;p&gt;Again we emphasis that the problem at hand is the inference of hidden variables in a non-linear dynamic system. The goal is to characterize the posterior probability of the system state $z_n$ given all previous observations $x_1^n\triangleq x_1,x_2,\ldots x_n$. Specifically, we want to have an iterative procedure that update the system belief at time-slot $n$: $p(z_n|x_1^n)$ from the belief in the previous time instance $n-1$ ($p(z_{n-1}|x_1^{n-1}$) by considering both the system evolution $p(z_n|z_{n-1)}$ and the updated noisy observations $p(x_n|z_n)$.&lt;/p&gt;

&lt;p&gt;Drawing similarity to Kalman filter, we can represent the problem as the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\underset{\substack{\\\text{weighted samples}}}{p(z_{n-1}|x_1^{n-1})}
\underset{\text{sampling}}{\xrightarrow{\substack{\text{system evolution }\\p(z_n|z_{n-1})}}}  
\underset{\substack{\\\text{samples}}}{p(z_n|x_1^{n-1}) }
\underset{\text{importance weighting}}{\xrightarrow{\substack{\text{noisy observation }\\p(x_n|z_{n})}}}  
\underset{\substack{\\\text{weighted samples}}}{p(z_{n}|x_1^{n})}
\end{align*}&lt;/script&gt;

&lt;p&gt;The plan of attack, as suggested by the annotations in the above equation, is to get samples from the potentially intractable probabilities. Let’s start by assuming that we have a set of samples $\{\color{red}{z_n^{(s)}}, s=1,\ldots, S\}$ that represent the distribution of $p(z_n|x_1^{n-1})$, and the task is to generate a new set of sample $\{\color{blue}{z_{n+1}^{(s)}}, s=1,\ldots, S\}$ representing the distribution of $p(z_{n+1}|x_1^n)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(z_n| x_1^n) \propto&amp;\text{ }  p(x_n | z_n) \color{red}{p(z_n | x_1^{n-1})}\notag\\
\color{blue}{p(z_{n+1}|x_1^n)} =&amp; \int p(z_n| x_1^n) p(z_{n+1}|z_n) dz_n\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since the evaluation of $\color{blue}{p(z_{n+1}|x_1^n)}$ involves taking the expectation, a corresponding sampling approach would be to approximate it using sampling from  $\color{red}{p(z_n | x_1^{n-1})}$ together with the technique of importance sampling to bridge the gap between the $p(z_n| x_1^n)$ and $\color{red}{p(z_n | x_1^{n-1})}$, resulting in the derivation below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\color{blue}{p(z_{n+1}|x_1^n)} =&amp; \int p(z_n| x_1^n) p(z_{n+1}|z_n) dz_n\notag\\
=&amp;\int  \color{red}{p(z_n| x_1^{n-1})} \frac{p(z_n| x_1^n)}{\color{red}{p(z_n| x_1^{n-1})}}  p(z_{n+1}|z_n) dz_n\notag\\
=&amp; \frac{ \int \color{red}{p(z_n| x_1^{n-1})}   p(x_n|z_n)  p(z_{n+1}|z_n) dz_n}{
\int \color{red}{p(z_n| x_1^{n-1})}   p(x_n|z_n)   dz_n
},
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;which leads to the following Monte Carlo approximation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\color{blue}{p(z_{n+1}|x_1^n)} \approx&amp;\sum_{s=1}^S \frac{p(x_n|z_n^{(s)})}{\sum_{l=1}^S p(x_n|z_n^{(l)})}p(z_{n+1}|z_n^{(s)})\notag\\
&amp;\sum_{s=1}^S w_n^{(s)}p(z_{n+1}|z_n^{(s)})\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Comparing the above equation with the one before, one can realize that the probability $p(z_n|x_1^n)$ is basically represented by a set of weighted samples, where the samples $z_n^{(s)}$ are drawn from $\color{red}{p(z_n|x_1^{n-1})}$ and the weights are defined as $w_n^{(s)}\triangleq \frac{p(x_n|z_n^{(s)})}{\sum_{l=1}^S p(x_n|z_n^{(l)})}$.&lt;/p&gt;

&lt;p&gt;Eventually, according to the above equation, to obtain samples $\color{blue}{\{x_{n+1}^{(s)}, s=1,\ldots, S\}}$ from $\color{blue}{p(z_{n+1}|x_1^n)}$, one can equivalently draw samples from $\sum_{s=1}^S w_n^{(s)}p(z_{n+1}|z_n^{(s)})$, which is itself a weighted sum of $p(z_{n+1}|z_n^{(s)})$ for each sample in $\color{red}{\{x_{n}^{(s)}, s=1,\ldots, S\}}$.&lt;/p&gt;

&lt;p&gt;With this we complete the derivation of the particle filter. In essence, it can be viewed as the sampling counterpart of Kalman filter, that generalizes to non-linear systems. The sequential Monte Carlo method is also referred to as sampling-importance-resampling in the literature.&lt;/p&gt;

&lt;p&gt;To link the math with a specific example, i recommend &lt;a href=&quot;https://www.youtube.com/watch?v=aUkBa1zMKv4&quot;&gt;this video&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;bigger-picture&quot;&gt;Bigger picture&lt;/h2&gt;

&lt;p&gt;Looking from a wider angle, both Kalman filter and Particle filter are inference algorithms in hidden Markov models with continuous random variables. Specifically, the formulation we went through corresponds to the forward procedure in HMM inference, and one can generalize it for the backward procedure as well.&lt;/p&gt;

&lt;p&gt;The forward-backward algorithm itself is a special realization of belief-propagation or message-passing-algorithm applied in a HMM system, whose probabilistic graphic model is a tree.&lt;/p&gt;

&lt;p&gt;The HMM model and the forward-backward procedure has manifestation in different areas:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;BCJR algorithm in the decoding of convolutional/Turbo code in communication systems&lt;/li&gt;
  &lt;li&gt;Baum-Welch algorithm, commonly used in speech recognition systems with discrete HMM models&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yang Yang</name></author><category term="kalman filter" /><summary type="html">Kalman filter and particle filter are concepts that are intimidating for new learners due to its involved mathmatical discription, and are straightforward once you grasp the main idea and get used to Gaussian distributions. The goal of this post is to take a journey to Kalman filter by dissecting its idea and operation into pieces that are easy to absorb, and then assemble them together to give the whole picture. As a last step, we will see that particle filter achieves the same goal for non-Gaussian system resorting to Monte Carlo sampling.</summary></entry><entry><title type="html">Griffin-Lim algorithm for waveform reconstruction</title><link href="https://yyang768osu.github.io/posts/2018/08/griffin-lim/" rel="alternate" type="text/html" title="Griffin-Lim algorithm for waveform reconstruction" /><published>2018-08-12T00:00:00-07:00</published><updated>2018-08-12T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2018/08/griffin-lim-algorithm-for-waveform-reconstruction</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2018/08/griffin-lim/">&lt;p&gt;Here’s the problem: we are given some &lt;em&gt;modified&lt;/em&gt; short-time-Fourier-transform (STFT) or &lt;em&gt;modified&lt;/em&gt; spectrogram (magnitude of STFT). The STFT/spectrogram is called &lt;em&gt;modified&lt;/em&gt; to stress the fact that it in general does not corresponds to any valid time-domain signal: it may be obtained by modifying a STFT/spectrogram of some real signal, or it may simply be generated by a neural network. The task here is to synthesize a time-domain signal from this target STFT/spectrogram, such that the STFT/spectrogram of the synthesized signal is as close to the target as possible.&lt;/p&gt;

&lt;p&gt;Let us denote the target STFT as $Y_w[mS,k]$, where $m$ is the index of the time-window, and $k$ represents the frequency axis. Here $S$ denote the step size of the time-domain window, and $mS$ can be viewed as the center of the sliding time windows. Correspondingly, if we are talking about spectrogram, the target can be denoted as  $|Y_w(mS,\omega)|$, to emphasis that there is no phase information.&lt;/p&gt;

&lt;p&gt;First, let’s align the definition and notation of the STFT/spectrogram. For a given time-domain signal $x[n]$, it is masked by a finite length window function $w$, yielding $x_w[mS,n]\triangleq w[n-mS]x[n]$, with $mS$ denoting the offset of the window from the original, and $m$ being the index of time-domain slice. The STFT $X_w[mS,k]$ is obtained by taking the DFT of each windowed signal $x_w[mS, n]$ with respect to time-index $n$: $X_w[mS,k]=\text{DFT}(x_w[mS, n])$.&lt;/p&gt;

&lt;p&gt;The objective is then to find a time domain signal $x[n]$ such that its STFT $X_w[mS,k]$ is as close to the target $Y_w[mS,k]$ as possible. Note, again, that the target $Y_w[mS,k]$ in general does not correspond to any valid time-domain signal. In other words, there is no time-domain signal that produces $Y_w[mS,k]$ as its STFT. Going back to the objective, a natural way to define the closeness is simply the sum squared distance, expressed as below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
D\left(X_w, Y_w\right) = \frac{1}{N}\sum_{m=-\infty}^{\infty}\sum_{k=0}^{N-1}|X_w[mS,k]-Y_w[mS,k]|^2\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;where $N$ denote the window-length (and correspondingly the DFT size). By Parseval’s theorem we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
D\left(X_w, Y_w\right) =&amp;\sum_{m=-\infty}^{\infty}\sum_{l=-\infty}^{\infty}|x_w[mS,l]-y_w[mS,l]|^2\notag\\
=&amp;\sum_{m=-\infty}^{\infty}\sum_{l=-\infty}^{\infty}|w[n-mS]x[n]-y_w[mS,l]|^2\notag,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $y_w[mS,l]$ is the IDFT of $Y_w[mS,k]$.&lt;/p&gt;

&lt;p&gt;This is just the least-square problem, where the solution of $x[n]$ can be obtained by taking the derivative with respect to $D\left(X_w, Y_w\right)$ and enforce it to be zero, yielding the following solution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x[n]=\frac{  \sum_{m=-\infty}^\infty w[n-Sm]y_w[mS,n]  }{  \sum_{m=-\infty}^\infty w^2[n-Sm]  }\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;With carefully designed window function, we can make the denominator to be $1$, resulting in a form where the reconstructed signal $x$ is simply the sum of the IDFT of the target STFT, weighted by the window function $w$. Two such window functions are: (1) Rectangular window $w_r$ with length $L$ being an integer multiple of the sliding step-size $S$ (2) Sinusoidal window&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
w_s[n] = \frac{2w_r[n]}{\sqrt{4a^2+2b^2}}\left[a+b\cos\left(\frac{2\pi n}{L}+\phi\right)\right]\notag.
\end{align*}&lt;/script&gt;

&lt;p&gt;with length $L$ being a multiple of four times the window shift $S$.&lt;/p&gt;

&lt;p&gt;Now that we have introduced the reconstruction of modified STFT with a least-square error estimate (LSEE) criterion, let us look at the corresponding reconstruction of the modified spectrogram. Without phase information, the least-square optimization problem is modified to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
D\left(X_w, Y_w\right) = \frac{1}{N}\sum_{m=-\infty}^{\infty}\sum_{k=0}^{N-1}\left(|X_w[mS,k]|-|Y_w[mS,k]|\right)^2\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;It can be showed that the following iterative algorithm reduces the error function above after each iteration:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Start with initial estimate of $x^{(t)}[n]$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Obtain its STFT $X_w^{(t)}[mS, k]$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Replace the magnitude of $X_w^{(t)}[mS, k]$ with the target spectrogram $|Y_w[mS, k]|$ and preserve the phase. More precisely, obtain $\widehat{X}_w^{(t)}[mS, k]$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\widehat{X}_w^{(t)}[mS, k]=|Y_w[mS, k]|\frac{X_w^{(t)}[mS, k]}{|X_w^{(t)}[mS, k]|}\notag.
\end{align*}&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;Get an updated estimate of $x^{(t+1)}[n]$ by running the LSEE algorithm with target STFT of $\widehat{X}_w^{(t)}[mS, k]$&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x^{(t+1)}[n]=\frac{  \sum_{m=-\infty}^\infty w[n-Sm]\text{IDFT}\left(\widehat{X}_w^{(t)}[mS, k]\right) }{  \sum_{m=-\infty}^\infty w^2[n-Sm]  }\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;and go back to step 1.&lt;/p&gt;

&lt;p&gt;https://pdfs.semanticscholar.org/ade8/d1511a61d78948bb0d43e207593389935421.pdf&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="text to speech" /><summary type="html">Here’s the problem: we are given some modified short-time-Fourier-transform (STFT) or modified spectrogram (magnitude of STFT). The STFT/spectrogram is called modified to stress the fact that it in general does not corresponds to any valid time-domain signal: it may be obtained by modifying a STFT/spectrogram of some real signal, or it may simply be generated by a neural network. The task here is to synthesize a time-domain signal from this target STFT/spectrogram, such that the STFT/spectrogram of the synthesized signal is as close to the target as possible.</summary></entry><entry><title type="html">A step-by-step guide to variational inference (4): variational auto encoder</title><link href="https://yyang768osu.github.io/posts/2018/08/variational_inference_4/" rel="alternate" type="text/html" title="A step-by-step guide to variational inference (4): variational auto encoder" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2018/08/variational-inference-IV-variational-auto-encoder</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2018/08/variational_inference_4/">&lt;p&gt;The variational lower bound $\mathcal{L}$ sits in the core of variational inference. It connects the density estimation problem with the Bayesian inference problem through a variational (free to vary) distribution $q$, and it converts both problems into an optimization problem. Here let’s briefly revisit the identity associated with variational lower bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\ln p(X;\theta) &amp;= \text{KL}{\big(}q|| p(Z|X;\theta){\big)} + \mathcal{L}(q,\theta)\\
\text{where }\mathcal{L}(q, \theta) &amp;=\int q(Z) \ln \frac{p(X,Z;\theta)}{q(Z)} dZ
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The identity holds for any arbitrary probability function $q$. $\mathcal{L}$ is a lower bound for the data log-likelihood $\ln p(X;\theta)$ given the non-negativity of the KL divergence. From the identify we can obtain the following two equations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\ln p(X;\theta) &amp;= \max_{q} \mathcal{L}(q,\theta)\\
p(Z|X;\theta) &amp;= \underset{q}{\arg\max} \mathcal{L}(q,\theta),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which testified the claim that both density estimation (LHS of the first equation) and Bayesian inference (LHS of the second equation) are linked with the same optimization function. There are two implications if we increases the value of $\mathcal{L}$ by tweaking the distribution $q$: (1) $\mathcal{L}$ becomes a tighter lower bound of $\ln p(X;\theta)$, meaning that it is closer to the true data log-likelihood in value (2) the distribution function $q$ itself is closer to the true posterior distribution measured in KL divergence.&lt;/p&gt;

&lt;p&gt;Often, we are also given the task of finding the ML estimate of the parameter $\theta$ (or MAP estimate of the parameter $\theta$), which requires taking the maximum of $\ln p(X;\theta)$ (or $\ln p(X|\theta) + \ln p_\text{prior}(\theta)$ for the MAP case) with respect to $\theta$, yielding the following problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\max_{\theta}\max_{q}\mathcal{L}(q, \theta).
\end{align*}&lt;/script&gt;

&lt;p&gt;By increasing the variation lower bound $\mathcal{L}$ with respect to $\theta$, by which the model is parameterized, we are essentially searching for model that can better fit to the data.&lt;/p&gt;

&lt;p&gt;It should be clear that is it desirable to maximize $\mathcal{L}$ with respect to both the variational distribution $q$ and the generative parameter $\theta$: maximize it with respect to $q$ would yield a better inference function; maximize it with respect to $\theta$ would give us a better model.&lt;/p&gt;

&lt;p&gt;Instead of allowing $q$ to be any function within the probability function space, for analytical tractability, we assume that it is parameterized by $\phi$ and is a function of the observed data $x$, denoted as $q_\phi(x)$. For the generative model, let us modified the notation slightly by assuming that the prior distribution $p(z)$ is unparameterized, and denote the conditional generative probability as $p_\theta(x|z)$, leading to the following expression of the variational lower bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(\phi, \theta, x^{(i)}) = \int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln \frac{p_\theta\left(x^{(i)}|z^{(i)}\right)p\left(z^{(i)}\right)}{q_\phi\left(z^{(i)}|x^{(i)}\right)}dz^{(i)}
\end{align*}&lt;/script&gt;

&lt;p&gt;Note that we used to express the variational lower bound in terms of the complete observed dataset $X={x^{(1)},\ldots, x^{(N)}}$ as well as the corresponding latent variables $Z={z^{(1)},\ldots, z^{(N)}}$. Since each data point and the corresponding latent variables are generated independently, it can be decomposed into the summation of $N$ terms, one for each data point $x^{(i)}$ as shown above. Those $N$ identity equations are linked through global parameter $\phi$ and $\theta$.&lt;/p&gt;

&lt;p&gt;As discussed before, to obtain a better model and to obtain a closer approximation to the true posterior inference function, one needs to differentiate and optimize $\sum_{i=1}^N\mathcal{L}(\phi, \theta, x^{(i)})$ with respect to both $\phi$, the parameter of the inference function, and $\theta$, the parameter of the model. Here’s a plan: let us calculate the gradient for the lower bound with respect to both parameters, and be done with the problem by applying our favorite stochastic gradient descent algorithm to find a solution. Actually we will show later that such a stochastic training framework is analogous to using an auto-encoder architecture with a specific regularization function.&lt;/p&gt;

&lt;p&gt;Soon enough you will realize a major challenge: it is not clear how to differentiate against $\phi$. There is very little chance for us to expect a close-form expression if we directly differentiate what is inside the integral, as the integral itself is hard even without the differentiation. We will  spend some time here to dig into the issue, which is the key to the understanding of the variational auto-encoding algorithm.&lt;/p&gt;

&lt;p&gt;Since the lower-bound exists in the form of the expectation with respect to the variational distribution $q_\phi$, the work-around here is to seek for Monte-Carlo estimation for the integral with the sampling from distribution $q_\phi\left(z^{(i)}|x^{(i)}\right)$. Let us focus on the general problem of $\nabla_\phi \mathbb{E}_{q_\phi(z|x)}\left[f(z)\right]$, for which there are two approaches that use sampling to approximate the expectation:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 1&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\nabla_\phi \mathbb{E}_{q_\phi(z|x)}\left[f(z)\right] \\
=&amp;\int \nabla_\phi q_\phi(z|x)  f(z) dz\\
=&amp;\int q_\phi(z|x) \frac{\nabla_\phi q_\phi(z|x)}{q_\phi(z|x)}  f(z) dz\\
=&amp; \int q_\phi(z|x)  \nabla_\phi \ln q_\phi(z|x) f(z) dz \\
=&amp; \mathbb{E}_{q_\phi(z|x)}\left[ \nabla_\phi \ln q_\phi(z|x) f(z)\right] \\
\text{(Monte Carlo)} \approx &amp;\frac{1}{S}\sum_{s=1}^S \nabla_\phi \ln q_\phi(z^{[s]}|x) f(z^{[s]}) 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Approach 2&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;This approach makes an additional assumption on $q_\phi(z|x)$: assume that we can obtain samples of $z$ by first sampling through a distribution $p(\epsilon)$ that is independent of $\phi$, and then apply a $(\phi,x)$-dependent transformation of  $g_\phi(\epsilon, x)$. Effectively we are assuming that the random variable $\mathcal{Z}$ is a $\phi-$dependent function of a $\phi$-independent random variable $\mathcal{E}$: $\mathcal{Z} = g_\phi(\mathcal{E},x)$. Reflecting this assumption in the differential of expectation, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\nabla_\phi \mathbb{E}_{q_\phi(z|x)}\left[f(z)\right]\\
=&amp;\nabla_\phi \int q_\phi(z|x)f(z) dz\\
\text{(parameter substitution)}=&amp;\nabla_\phi \int p(\epsilon)f(g_\phi(\epsilon, x))d\epsilon\\
=&amp; \int p(\epsilon) \nabla_\phi f(g_\phi(\epsilon, x))d\epsilon\\
\text{(Monte Carlo)}\approx&amp; \frac{1}{S}\sum_{s=1}^S  \nabla_\phi f(g_\phi(\epsilon^{[s]},x))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This seems like a good solution: the Monte Carlo sampling itself is not a function of $\phi$, and $\phi$ just appear as the parameter of the transformation function  $g_\phi$ that maps the samples from $\mathcal{E}$ to the samples in $\mathcal{Z}$. In this case $q_\phi$ is just the induced distribution as a function of the prior distribution of $\mathcal{E}$ as well as the transformation function $g_\phi$. This parameter substitution technique is branded as &lt;em&gt;the reparameterization trick&lt;/em&gt; in the original paper of variational auto encoder.&lt;/p&gt;

&lt;p&gt;To understand the implication of such assumption, let’s ask this question: is it feasible to design the prior distribution of $\mathcal{E}$ and the transformation function $g_\phi$ in any arbitrary form? You may wonder why do we even care. Well there is a hidden factor that we need to take care of before claiming victory. Looking at the variational lower bound expression, not only do we need to integrate with respect to the distribution $q_\phi$, which can be achieved using Monte Carlo by the help of this reparameterization trick, we also need to ensure a closed-form expression of the density function $q_\phi(z|x)$ itself, as it lives inside the expectation/integral. This limits the way we can choose the random variable $\mathcal{E}$ and the function $g_\phi$.&lt;/p&gt;

&lt;p&gt;To investigate on the requirement of $\mathcal{E}$ and $g_\phi$ such that the induced random variable $\mathcal{Z} = g_\phi(\mathcal{E},x)$ has a tractable density/distribution function (easy to evaluate), let’s try to express distribution $q_\phi$ as a function of $p_\epsilon$ and $g_\phi(z,x)$. For any monotonic function $g_\phi$, the induced distribution $q_\phi$ &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_variable#Functions_of_random_variables&quot;&gt;can be derived&lt;/a&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_\phi(z) = p_\epsilon\left(g_\phi^{-1}(z)\right)\left|\frac{\partial g_\phi^{-1}(z)}{\partial z}\right|.
\end{align*}&lt;/script&gt;

&lt;p&gt;To enforce a closed form expression for $q_\phi$, we have two general design choices on $p_\epsilon$ and $g_\phi$, as is evident from the expression above: (1) let $p_\epsilon$ be a uniform distribution on $[0,1]$ and $g_\phi=\text{CDF}^{-1}$ be the inverse of any distribution with closed-form cumulative distribution function. (2) let $p_\epsilon$ be any distribution with closed form density and $g_\phi$ be an easy form of monotonic function, e.g., a linear function.&lt;/p&gt;

&lt;p&gt;In the context of variational auto encoder in the original paper, the second design choice is picked: $p_\epsilon$ is chosen as the standard normal distribution and  $g_\phi$ is a linear function of $\epsilon$, whose slope and intercept is an arbitrary function of $x$ and $\phi$ characterized using a neural network. In this case the induced distribution $q_\phi$ is a normal distribution whose mean and variances is determined by a neural network with the input $x$ and parameter $\phi$.&lt;/p&gt;

&lt;p&gt;Now that we went through what &lt;em&gt;the reparameterization trick&lt;/em&gt; is, let us return back to the problem of finding the gradient of $\mathcal{L}(\phi, \theta, x^{(i)})$ with respect to $\phi$ and $\theta$. Applying the reparameterization trick, we obtain the following gradient-friendly Monte Carlo estimate of the variational lower bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}(\phi, \theta, x^{(i)}) =&amp; \int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln \frac{p_\theta\left(x^{(i)}|z^{(i)}\right)p\left(z^{(i)}\right)}{q_\phi\left(z^{(i)}|x^{(i)}\right)}dz^{(i)}\\
\text{(Monte Carlo)}\approx&amp;\frac{1}{S}\sum_{s=1}^S \ln  \frac{p_\theta\left(x^{(i)}|z^{(i)[s]}\right)p\left(z^{(i)[s]}\right)}{q_\phi\left(z^{(i)[s]}|x^{(i)}\right)}\\
\text{(Reparameterization)}=&amp;\frac{1}{S}\sum_{s=1}^S \ln  \frac{p_\theta\left(x^{(i)}|g_\phi (\epsilon^{[s]}, x^{(i)})\right)p\left(g_\phi (\epsilon^{[s]}, x^{(i)})\right)}{q_\phi\left(g_\phi (\epsilon^{[s]}, x^{(i)})|x^{(i)}\right)}\\
\text{where } \epsilon^{[s]}&amp;\text{ is drawn i.i.d. from }p_\epsilon. 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here’s an alternative way to decompose $\mathcal{L}$ and apply Monte Carlo and reparameterization, for which there is a close form expression for the second term (KL divergence) and only the first part is approximated.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}(\phi, \theta, x^{(i)}) =&amp; \int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln \frac{p_\theta\left(x^{(i)}|z^{(i)}\right)p\left(z^{(i)}\right)}{q_\phi\left(z^{(i)}|x^{(i)}\right)}dz^{(i)}\\
=&amp;\int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln p_\theta\left(x^{(i)}|z^{(i)}\right) dz^{(i)}-\text{KL}\left( q_\phi\left(z^{(i)}|x^{(i)}\right){\Big|\Big|} p\left(z^{(i)}\right)\right)\\
\text{(Monte Carlo)}\approx&amp; \frac{1}{S}\sum_{s=1}^S \ln p_\theta\left(x^{(i)}|z^{(i)[s]}\right)  -\text{KL}\left( q_\phi\left(z^{(i)}|x^{(i)}\right){\Big|\Big|} p\left(z^{(i)}\right)\right)\\
\text{(Reparameterization)}\approx&amp; \frac{1}{S}\sum_{s=1}^S \ln p_\theta\left(x^{(i)}|g_\phi (\epsilon^{[s]}, x^{(i)})\right)  -\text{KL}\left( q_\phi\left(z^{(i)}|x^{(i)}\right){\Big|\Big|} p\left(z^{(i)}\right)\right)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This decomposition leads to the interpretation of probabilistic auto-encoder, which is named variational auto-encoder as it rooted from the maximization of the variational lower bound.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/vae.png&quot; alt=&quot;Variational Auto Encoder&quot; /&gt;&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="variational inference" /><summary type="html">The variational lower bound $\mathcal{L}$ sits in the core of variational inference. It connects the density estimation problem with the Bayesian inference problem through a variational (free to vary) distribution $q$, and it converts both problems into an optimization problem. Here let’s briefly revisit the identity associated with variational lower bound</summary></entry><entry><title type="html">A step-by-step guide to variational inference (3): mean field approximation</title><link href="https://yyang768osu.github.io/posts/2018/08/variational_inference_3/" rel="alternate" type="text/html" title="A step-by-step guide to variational inference (3): mean field approximation" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2018/08/variational-inference-III-mean-field-approximation</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2018/08/variational_inference_3/">&lt;p&gt;We have learned in the previous post that E-M algorithm tries to find a ML or MAP solution to the parameters of a generative model. It is build on top of two major premises:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\ln p(X,Z;\theta)$ is in a much simpler form than $\ln P(X;\theta)$,&lt;/li&gt;
  &lt;li&gt;$\ln p(Z|X;\theta)$ is easy to obtain.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;While the first one is often true in that both $p(X|Z;\theta)$ and $p(Z;\theta)$ given as part of the model and are usually designed to be simple, the second one is a very strong assumption and does not hold in most cases. In this post, we remove the second premise, and introduce a way to obtain an approximation of $p(Z|X;\theta)$.&lt;/p&gt;

&lt;p&gt;In what follows, we modify notation slightly by assuming that there is prior distribution on any parameters of interest, and conceptually merge $\theta$ as part of the latent variable $Z$ (which is common across different data samples) and remove $\theta$ from the notation.&lt;/p&gt;

&lt;p&gt;As we discussed in the previous posts, the Bayesian inference problem is to find the posterior probability $p(Z|X)$, which is in general very hard due to the integral/summation (in most cases multi-dimensional integral/summation) in the denominator below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
p(Z|X) = \frac{p(X|Z)p(Z)}{\int p(X|Z)p(Z) dZ}.
\end{align*}&lt;/script&gt;

&lt;p&gt;We also showed that with the introduction of a variational distribution $q(Z)$, we can convert the problem of finding $p(X|Z)$ as an optimization problem below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(Z|X) &amp;= \arg\max_{q}\mathcal{L}(q).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;However, this optimization problem above is still very hard and it does not lend itself to any easy solution. Here, the objective $\mathcal{L}(q)$, called the variational lower-bound, is a functional as it maps a function into a scalar value, whose domain is the space of all functions.&lt;/p&gt;

&lt;p&gt;Since it is hard to optimize the variational lower bound as is, one may wonder, how about constraining the search space of $q$ from all potential functions to within a limited function space? Could that make the problem simpler? Even though we may not find the optimal solution after restricting the set of functions we could search from, the hope is that by doing so we can device practical algorithms with solutions that are reasonably close to the true posterior. This is exactly the idea behind mean field approximation.&lt;/p&gt;

&lt;p&gt;In the mean field method, we add a constraint to the domain of the optimization: instead of allowing $q(Z)$ to be in arbitrary form, we only look at cases when it can be factorized into a product form with disjoint latent variables in each multiplicative factor. More specifically, we divide the dimension of latent variables into $K$ groups $Z=[Z_1, Z_2, \ldots, Z_K]$ and enforce $q$ to have the form of $q(Z)=q_1(Z_1)q_2(Z_2)\ldots q_K(Z_K)$. Put it in precise math, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q^* = \underset{q=q_1 q_2 \ldots q_K}{\arg\max}\mathcal{L}(q)
\end{align*}&lt;/script&gt;

&lt;p&gt;Referring back to the equation on the decomposition of observed data log-likelihood&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\ln p(X) = \text{KL}(q || p(Z|X)) + \mathcal{L}(q),
\end{align*}&lt;/script&gt;

&lt;p&gt;we know that by maximizing $\mathcal{L}(q)$ with respect to functions with form $q(Z)=\prod_{k=1}^K q_k(Z_k)$, we are trying to find one function in the confined function space (defined as the set of functions that can be factorized as such) that is closest to the true posterior $ p(Z|X)$ measured in KL divergence. It is worth emphasizing that we are merely constraining $q(Z)$ to have this factorization form, and do not make any assumption on what each individual factor would look like.&lt;/p&gt;

&lt;p&gt;Let us plug in the factorized form of $q(Z) = \prod_{k=1}^K q_k(Z_k)$ in the expression of the variational lower bound, yielding&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}(q) =&amp; \int q(Z)\ln\frac{p(X, Z)}{q(Z)}dZ\\
=&amp; \int q(Z)\ln p(X, Z)dZ +  \int q(Z)\ln\frac{1}{q(Z)}dZ\\
=&amp; \int \prod_{k=1}^K q_k(Z_k) \ln p(X, Z)dZ +  \sum_{k=1}^K\int q_k(Z_k)\ln\frac{1}{q_k(Z_k)}dZ_k.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The second term is just the entropy of $q$, which, given the assumption that it can be decomposed into independent factors, becomes the summation of the entropy for each individual $q_k$.&lt;/p&gt;

&lt;p&gt;It may not be immediately apparent why this modified formulation is any easier to solve. Nevertheless, let us proceed by making the temporary assumption that among the $K$ factors, all are known except for one factor $q_j$. Then, we just need to maximize $\mathcal{L}$ with respect to $q_j$ with all the other factors $q_{k}, k\not=j$ as given. The variational lower bound can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\int q_j(Z_j) \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]dZ_j + \int q_j(Z_j)\ln \frac{1}{q_j(Z_j)}dZ_j + \text{constant}\\
=&amp;\int q_j(Z_j)\ln\frac{\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)}{q_j(Z_j)}dZ_j + \text{constant}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since any term that does not involve $q_j$ would not affect the solution to $\arg\max_{q_j}\mathcal{L}(q)$, we just mark those as constant. Here it comes a key observation: notice how the non-constant term resembles the definition of a negative KL divergence between $q_j(Z_j)$ and $\mathbb{E}_{q_k, k\not=j}$ $\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)$. The only issue is that $\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)$ may not be a proper probability measure that sum/integrate to $1$. Luckily, since scaling $\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)$ only amounts to adding/subtracting a constant term, we know that $\mathcal{L}(q)$ is maximized when&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_j(Z_j)  \propto \exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right),
\end{align*}&lt;/script&gt;

&lt;p&gt;or more accurately,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_j(Z_j)  = \frac{\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)}{\int \exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right) dZ_j}.
\end{align*}&lt;/script&gt;

&lt;p&gt;This result tells us that, among the $K$ factors, if we have all but one factor fixed, then the optimal solution of that left out function that maximize the variation lower bound (or equivalently, minimizes the KL divergence to the true posterior distribution) can be written in the above form as a function of all the other factors.&lt;/p&gt;

&lt;p&gt;This leads to a nice iterative solution that iteratively visits each factor, and maximize the variational lower bound with respect to the target factor treating all the other factors as known. In special cases, the normalization constant term in the dominator of the above equation could be directly inferred if the numerator term already suggests certain type of known distribution.&lt;/p&gt;

&lt;p&gt;It is interesting to note that, to apply this mean-field-approximation method, one only need to make the assumption on how to partition the latent variable dimensions into disjoint groups, one for each factor, without making any assumption on the detailed function form of any factor. The detail form of the factorized distribution would be obtained as a result of the iterative procedure.&lt;/p&gt;

&lt;p&gt;There is one caveat that we should mention. Looking at the equation above, to find the optimal factor $q_j$, assuming all the other are know, we still need to make sure that the expectation $\mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]$ results in tractable form. Given that the expectation itself is a multi-dimensional integral/summation, in general it is hard to guarantee a closed form expression. The expectation may be tractable with specific models and specific ways on which the latent variables are partitioned, which limits the domain where mean-field-approximation could be applied.&lt;/p&gt;

&lt;p&gt;Here we introduced mean field approximation from the perspective of Bayesian inference. As a final remark, it is straightforward to show that it also provide a way to evaluate observed data likelihood and thus can be useful with model-selection as well. According to the identity below, we know that as we maximize $\mathcal{L}$, not only do we obtain a variational distribution that is close to the true posterior in the KL divergence sense, we also obtained a surrogate for the log-likelihood, as the lower bound $\mathcal{L}$ is a lower bound which gets tighter as it becomes larger.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\ln p(X) = \text{KL}(q || p(Z|X)) + \mathcal{L}(q),
\end{align*}&lt;/script&gt;

&lt;p&gt;If we are given $M$ models, then one can conduct mean field method on each of them, obtain the corresponding optimized variational lower-bound, and use it as the surrogate to rate the likelihood of each model. We can even combine the prior distribution of the $M$ models, if there is any, to obtain a maximum a posterior (MAP) selection of the model.&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="variational inference" /><summary type="html">We have learned in the previous post that E-M algorithm tries to find a ML or MAP solution to the parameters of a generative model. It is build on top of two major premises:</summary></entry></feed>