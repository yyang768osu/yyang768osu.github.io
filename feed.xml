<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://yyang768osu.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yyang768osu.github.io/" rel="alternate" type="text/html" /><updated>2018-08-20T23:24:36-07:00</updated><id>https://yyang768osu.github.io/</id><title type="html">Yang Yang</title><subtitle>Engineer at Qualcomm</subtitle><author><name>Yang Yang</name></author><entry><title type="html">An introduction to Kalman filter and particle filter</title><link href="https://yyang768osu.github.io/posts/2012/08/kalman_filter_particle_filter/" rel="alternate" type="text/html" title="An introduction to Kalman filter and particle filter" /><published>2018-08-20T00:00:00-07:00</published><updated>2018-08-20T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/kalman-filter-and-particle-filter</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/kalman_filter_particle_filter/">&lt;p&gt;Kalman filter and particle filter are concepts that are intimidating for new learners due to its involved mathmatical discription, and are straightforward once you grasp the main idea and get used to Gaussian distributions. The goal of this post is to take a journey to Kalman filter by dissecting its idea and operation into pieces that are easy to absorb, and then assemble them together to give the whole picture. In the last step, we will see that particle filter achieves the same goal for non-Gaussian system resorting to Monte Carlo sampling.&lt;/p&gt;

&lt;p&gt;Below let’s walk through three simple problems and their solutions stemming from Gaussian distributions, and then stitching them together to form the problem that Kalman filter tries to solve and present its solution.&lt;/p&gt;

&lt;h2 id=&quot;a-conditional-gaussian-distribution&quot;&gt;A. Conditional Gaussian distribution&lt;/h2&gt;

&lt;p&gt;Here I assume you have a basic knowledge regarding multi-variant Gaussian distribution. A multi-variant Gaussian distribution is captured by its mean vector and covariance matrix, often denoted as $\mu$ and $\Sigma$. Below let us consider the bi-variant Gaussian vector $[z,x]^T$, with the following general notation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[
\begin{array}{c}
z\\
x
\end{array}
\right]
\sim
\mathcal{N}
\left(
\left[
\begin{array}{c}
\mu_z\\
\mu_x
\end{array}
\right],
\left[\begin{array}{cc}
\Sigma_z &amp; \Sigma_{zx} \\
\Sigma_{xz} &amp; \Sigma_{x}
\end{array}\right]
\right)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The covariance matrix is formally defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[\begin{array}{cc}
\Sigma_z &amp; \Sigma_{zx} \\
\Sigma_{xz} &amp; \Sigma_{x}
\end{array}\right]
=&amp;
\mathbb{E}\left[
\left[
\begin{array}{c}
z-\mu_z\\
x-\mu_x
\end{array}
\right]
\left[
\begin{array}{c}
z-\mu_z\\
x-\mu_x
\end{array}
\right]^T
\right]\notag\\
=&amp;
\left[
\begin{array}{cc}
\mathbb{E}[(z-\mu_z)(z-\mu_z)^T] &amp; \mathbb{E}[(z-\mu_z)(x-\mu_x)^T]\\
\mathbb{E}[(x-\mu_x)(z-\mu_z)^T] &amp; \mathbb{E}[(x-\mu_x)(x-\mu_x)^T]
\end{array}
\right]\notag.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The off-diagonal term represents the cross covariance between the two random variables $x$ and $z$, which, by checking the definition above, satisfies $\Sigma_{xz}=\Sigma^T_{zx}$. The larger the cross covariance, the more correlated the two random variables are. For correlated random variables, knowing the value of one would help us in guessing the value of the other. Let us take a look at the figure below for a concrete example&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/bivariant_normal.png&quot; alt=&quot;Bivariant Gaussian&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The figure above illustrates the joint distribution of a bivariant Gaussian distribution with $\mu_z=\mu_x=0$, $\Sigma_z=\Sigma_x=1$, and $\Sigma_{zx}=\Sigma_{xz}=0.8$. The marginal distributions of both $x$ and $z$ are $\mathcal{N}(0,1)$. The contour of the distribution forms a thin ellipse, reflecting the strong covariance $\Sigma_{zx}=\Sigma_{xz}=0.8$ between the two random variables $x$ and $z$. To testify the claim that knowing one variable would help us estimating the other,  let us take a look at the the conditional distribution of $z$ given $x=1$, and compare it with the marginal distribution of $z$. As can be seen from the figure above, the distribution of $z|x=1$ has much narrow span than $z$ with a shift in the mean. The reduction of variance of $z$ after observing $x=1$ is an evident that the observation of $x$ narrows down the potential values of $z$.&lt;/p&gt;

&lt;p&gt;An important fact here is that the conditional distribution of a joint-Gaussian distribution is also a Gaussian&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
p(z|x) = p(x,z)/p(x)\sim\mathcal{N}\left(\mu_{z|x}, \Sigma_{z|x}\right)\notag.
\end{align*}&lt;/script&gt;

&lt;p&gt;Below are two identities on the general expressions of the conditional distribution, let’s accept them as they are without bothering with any proof.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z|x} &amp;= \mu_z + \Sigma_{zx}\Sigma_{xx}^{-1}(x-\mu_x)\notag\\
\Sigma_{z|x} &amp;= \Sigma_{z} - \Sigma_{zx}\Sigma_x^{-1}\Sigma_{xz}\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The above two equations are very important, and lies in the core of many concepts such as MMSE estimator, Wiener filter and of course, Kalman filter. To see that knowing $x$ would reduce the uncertainty in $z$, here let’s just point out that the entropy (measure of uncertainty) of a Gaussian random vector is an increasing function of the determinant of the covariance-matrix, and that $\Sigma_{z|x}$ always has a smaller determinant than $\Sigma_z$ for any non-zero cross covariance $\Sigma_{zx}$, followed from the second equation with the fact that $\Sigma_{z}-\Sigma_{z|x}$ $=\Sigma_{zx}\Sigma_x^{-1}\Sigma_{xz}\succcurlyeq 0$ is a semi-positive-definite matrix.&lt;/p&gt;

&lt;p&gt;These two equations will become useful when we visit part C, and we will come back to them.&lt;/p&gt;

&lt;h2 id=&quot;b-gaussian-distribution-with-linear-transformation&quot;&gt;B. Gaussian distribution with linear transformation&lt;/h2&gt;

&lt;p&gt;In the first section we looked at the case of obtaining the conditional distribution from a joint Gaussian distribution, here let’s look at the distribution of a Gaussian vector going through a linear transform. More precisely, let us define a random variable $z_n$ as obtained from the following transform&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z_{n-1}&amp;\sim\mathbb{N}(\mu_{n-1}, V_{n-1})\notag\\
z_n |z_{n-1}&amp;\sim A z_{n-1}+a+\mathcal{N}(0,\Gamma)= \mathbb{N}(Az_{n-1}+a, \Gamma)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;One typical example for the above problem setup is the following: consider the problem of tracking the location and velocity of an object traveling in a strict line. Let us label $z_n=[\text{loc}_n, \text{vel}_n]^T$ as the location-velocity state of the object in time-step $n$. To model the estimation inaccuracy, assume that $z_n$ is a random variable with mean $\mu_{n-1}$ and variance $V_{n-1}$. Here the mean value reflect the estimated value and the variance can be viewed as capturing the amount/structure of the uncertainty in the estimate. The location-velocity estimate in the time-step $n+1$ can be modeled by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[
\begin{array}{c}
\text{loc}_{n}\\
\text{vel}_{n}
\end{array}
\right]=
\underbrace{
\left[
\begin{array}{cc}
1 &amp; 1\notag\\
0 &amp; 1\notag
\end{array}
\right]}_{
\text{loc}_{n} = \text{loc}_{n-1}+\text{vel}_{n-1}
}
\times
\left[
\begin{array}{c}
\text{loc}_{n-1}\\
\text{vel}_{n-1}
\end{array}
\right]
+
\underbrace{
\left[
\begin{array}{c}
a_\text{loc}\notag\\
a_\text{vel}\notag
\end{array}
\right]}_{\substack{
\text{external known}\\\text{change}
}
}
+
\underbrace{
\left[
\begin{array}{c}
\text{noise}_\text{loc}\notag\\
\text{noise}_\text{vel}\notag
\end{array}
\right]}_{\substack{
\text{additional noise}\\\text{in the system}
}
}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;with a one-to-one correspondence to the conditional probability restated below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z_n |z_{n-1}&amp;\sim A z_{n-1}+a+\mathcal{N}(0,\Gamma)= \mathbb{N}(Az_{n-1}+a, \Gamma).\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The problem of interest here is to characterize the distribution of $z_n$, given that it is obtained from a linear transformation of a previous estimate with an additional Gaussian noise. Precisely, what we want to solve is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(z_n) =&amp; \int p(z_{n}|z_{n-1})p(z_{n-1})dz_{n-1}\notag.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here it comes another important property of Gaussian distribution: any linear transformation of Gaussian variable is still Gaussian. With this property given to us, we can calculate the mean and variance of the updated state $z_n$, as shown below, which fully captures its distribution.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z_n} =&amp;\mathbb{E}[Az_{n-1}+a]= A\mu_{n-1}+a\notag\\
\Sigma_{z_n} =&amp;\mathbb{E}[(Az_{n-1}-A\mu_{n-1})(Az_{n-1}-A\mu_{n-1})^T]+\Gamma\notag\\
=&amp; AV_{n-1}A^T + \Gamma\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;c-bayes-theorem-for-gaussian&quot;&gt;C. Bayes’ theorem for Gaussian&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z&amp;\sim \mathcal{N}(\nu, P)\notag\\
x|z&amp;\sim \mathcal{N}(Cz+c,\Sigma) = Cz+c+\mathcal{N}(0, \Pi)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
p(z|x) = \frac{p(z)p(x|z)}{p(x)}  \propto p(z)p(x|z) \notag
\end{align*}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{x} &amp;= \mathbb{E}[Cz+c+\mathcal{N}(0, \Sigma)] = C\nu+c\notag\\
\Sigma_{x} &amp;= \mathbb{E}[xx^T] = \mathbb{E}[(Cz-C\nu)(Cz-C\nu)^T]+\Sigma=C P C^T + \Pi\notag\\
\Sigma_{zx} &amp;= \mathbb{E}[z(Cz-C\mu)^T]=P C^T  \notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[
\begin{array}{c}
z\\
x
\end{array}
\right]
\sim
\mathcal{N}
\left(
\left[
\begin{array}{c}
\nu\\
C\nu+c
\end{array}
\right],
\left[\begin{array}{cc}
P &amp;P C^T \\
CP &amp; C P C^T + \Pi
\end{array}\right]
\right)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z|x} &amp;= \nu + PC^T (CPC^T+\Pi)^{-1}(x-C\nu-c)\notag\\
\Sigma_{z|x} &amp;= P - PC^T (CPC^T+\Pi)^{-1}CP\notag 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
K\triangleq PC^T(CPC^T+\Pi)^{-1}\notag
\end{align*}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mu_{z|x} = \nu + K(x-C\nu-c)\notag\\
\Sigma_{z|x} = (I-KC)P\notag
\end{align*}&lt;/script&gt;

&lt;h2 id=&quot;kalman-filter&quot;&gt;Kalman filter&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z_{n-1}&amp;\sim\mathcal{N}(\mu_{n-1}, V_{n-1})\notag\\
z_n|z_{n-1} &amp;\sim \mathcal{N}(Az_{n-1}+a, \Gamma)= \mathcal{N}(A\mu_{n-1}+a,AV_{n-1}A^T + \Gamma)\notag\\
&amp;\sim \mathcal{N}(\nu_{n-1}, P_{n-1})\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\nu_{n-1}\triangleq&amp;  A\mu_{n-1}+a\notag\\
P_{n-1}\triangleq&amp; AV_{n-1}A^T+\Gamma\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z_n|x_n} =&amp; \nu_{n-1} + K_n(x_n-C\nu_{n-1}-c)\notag\\
\Sigma_{z_n|x_n} =&amp; (I-K_nC)P_{n-1}\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
K_n\triangleq P_{n-1}C^T(CP_{n-1}C^T+\Pi)^{-1}\notag
\end{align*}&lt;/script&gt;

&lt;h2 id=&quot;particle-filter&quot;&gt;Particle filter&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(z_n| x_1^n) \propto&amp;\text{ }  p(x_n | z_n) p(z_n | x_1^{n-1})\notag\\
p(z_{n+1}|x_1^n) =&amp; \int p(z_n| x_1^n) p(z_{n+1}|z_n) dz_n\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(z_{n+1}|x_1^n) =&amp; \int p(z_n| x_1^n) p(z_{n+1}|z_n) dz_n\notag\\
=&amp;\int \notag p(z_n| x_1^{n-1}) \frac{p(z_n| x_1^n)}{p(z_n| x_1^{n-1})}  p(z_{n+1}|z_n) dz_n\notag\\
=&amp;\int \notag p(z_n| x_1^{n-1}) \frac{p(z_n| x_1^n)}{p(z_n| x_1^{n-1})}  p(z_{n+1}|z_n) dz_n\notag\\
=&amp;\sum_{\substack{s=1\\ z_n^{(s)}\sim p(z_n|x_{1}^{n-1})}}^S \frac{p(x_n|z_n^{(s)})}{\sum_{l=1}^S p(x_n|z_n^{(l)})}p(z_{n+1}|z_n^{(s)})\notag
\end{align*} %]]&gt;&lt;/script&gt;</content><author><name>Yang Yang</name></author><category term="kalman filter" /><summary type="html">Kalman filter and particle filter are concepts that are intimidating for new learners due to its involved mathmatical discription, and are straightforward once you grasp the main idea and get used to Gaussian distributions. The goal of this post is to take a journey to Kalman filter by dissecting its idea and operation into pieces that are easy to absorb, and then assemble them together to give the whole picture. In the last step, we will see that particle filter achieves the same goal for non-Gaussian system resorting to Monte Carlo sampling.</summary></entry><entry><title type="html">Kalman Filter And Particle Filter</title><link href="https://yyang768osu.github.io/kalman-filter-and-particle-filter/" rel="alternate" type="text/html" title="Kalman Filter And Particle Filter" /><published>2018-08-20T00:00:00-07:00</published><updated>2018-08-20T00:00:00-07:00</updated><id>https://yyang768osu.github.io/kalman-filter-and-particle-filter</id><content type="html" xml:base="https://yyang768osu.github.io/kalman-filter-and-particle-filter/"></content><author><name>Yang Yang</name></author></entry><entry><title type="html">Griffin-Lim algorithm for waveform reconstruction</title><link href="https://yyang768osu.github.io/posts/2012/08/griffin-lim/" rel="alternate" type="text/html" title="Griffin-Lim algorithm for waveform reconstruction" /><published>2018-08-12T00:00:00-07:00</published><updated>2018-08-12T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/griffin-lim-algorithm-for-waveform-reconstruction</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/griffin-lim/">&lt;p&gt;The problem that Griffin-Lim algorithm tries to solve is the following. We are given some modified short-time-fourier transform (STFT) or modified spectrogram (only magnitude, no phase information). The STFT/Spectrogram is called ``modified’’ to stress the fact that it in general does not corresponds to any valid time-domain signal: it may be obtained by modifying a STFT/Spectrogram from some real signal, or they may simply be generated by a neural network. The task here is to synthesis a time-domain signal from this target STFT/Spectrogram, such that the STFT/Spectrogram of the synthesized signal is as close to the target as possible.&lt;/p&gt;

&lt;p&gt;Let us denote the target STFT as $Y_w[mS,k]$, where $m$ is the index of the time-window, and $k$ represents the frequency axis. Here $S$ denote the step size of the time-domain window, and $mS$ can be viewed as the center of the sliding time windows. Correspondingly, if we are talking about spectrogram, then the target can be denoted as  $|Y_w(mS,\omega)|$, to emphasis that there is no phase information.&lt;/p&gt;

&lt;p&gt;First, let’s align the definition/notation of STFT/Spectrogram for a given time-domain signal $x[n]$. The time-domain signal first goes through a finite length mask window $w$ where a piece of it is taken out: $x_w[mS,n]\triangleq w[n-mS]x[n]$, with $mS$ denoting the offset of the window from the original, and $m$ being the index of time-domain slice. The STFT $X_w[mS,k]$ is obtained by taking the DFT of each windowed signal $x_w[mS, n]$ with respect to time-index $n$: $X_w[mS,k]=\text{DFT}(x_w[mS, n])$.&lt;/p&gt;

&lt;p&gt;The objective is then to find a time domain signal $x[n]$ such that its STFT $X_w[mS,k]$ is as close to the target $Y_w[mS,k]$ as possible. Note, again, that the target $Y_w[mS,k]$ in general does not correspond to any valid time-domain signal. In other words, there is no such a time-domain signal that produces $Y_w[mS,k]$ as its STFT. Going back to the objective, a natural way to define the closeness is simply the sum squared distance, expressed as below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
D\left(X_w, Y_w\right) = \frac{1}{N}\sum_{m=-\infty}^{\infty}\sum_{k=0}^{N-1}|X_w[mS,k]-Y_w[mS,k]|^2\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;where $N$ denote the window-length (and correspondingly the DFT size). By Parseval’s theorem we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
D\left(X_w, Y_w\right) =&amp;\sum_{m=-\infty}^{\infty}\sum_{l=-\infty}^{\infty}|x_w[mS,l]-y_w[mS,l]|^2\notag\\
=&amp;\sum_{m=-\infty}^{\infty}\sum_{l=-\infty}^{\infty}|w[n-mS]x[n]-y_w[mS,l]|^2\notag,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $y_w[mS,l]$ is the IDFT of $Y_w[mS,k]$.&lt;/p&gt;

&lt;p&gt;This is just the least-square problem, where the solution of $x[n]$ can be obtained by taking the derivative with respect to $D\left(X_w, Y_w\right)$ and enforce it to be zero, yielding the following solution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x[n]=\frac{  \sum_{m=-\infty}^\infty w[n-Sm]y_w[mS,n]  }{  \sum_{m=-\infty}^\infty w^2[n-Sm]  }\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;With carefully designed window function, we can make the denominator to be $1$, resulting in a form where the reconstructed signal $x$ is simply the sum of the IDFT of the target STFT, weighted by the window function $w$.&lt;/p&gt;

&lt;p&gt;Two qualified window functions are: (1) Rectangular window $w_r$ with length $L$ being an integer multiple of the sliding step-size $S$ (2) Sinusoidal window&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
w_s[n] = \frac{2w_r[n]}{\sqrt{4a^2+2b^2}}\left[a+b\cos\left(\frac{2\pi n}{L}+\phi\right)\right]\notag.
\end{align*}&lt;/script&gt;

&lt;p&gt;with length $L$ being a multiple of four times the window shift $S$.&lt;/p&gt;

&lt;p&gt;Now that we have introduced the reconstruction of modified STFT with a least-square error estimate (LSEE) criterion, let us look at the corresponding reconstruction of the modified STFT-magnitude, a.k.a. spectrogram. Without phase information, the least-square optimization problem is modified to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
D\left(X_w, Y_w\right) = \frac{1}{N}\sum_{m=-\infty}^{\infty}\sum_{k=0}^{N-1}\left(|X_w[mS,k]|-|Y_w[mS,k]|\right)^2\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;It can be showed that the following iterative algorithm reduces the error function above after each iteration:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Start with initial estimate of $x^{(t)}[n]$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Obtain its STFT $X_w^{(t)}[mS, k]$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Replace the magnitude of $X_w^{(t)}[mS, k]$ with the target spectrogram $|Y_w[mS, k]|$ and preserve the phase. More precisely, obtain $\widehat{X}_w^{(t)}[mS, k]$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\widehat{X}_w^{(t)}[mS, k]=|Y_w[mS, k]|\frac{X_w^{(t)}[mS, k]}{|X_w^{(t)}[mS, k]|}\notag.
\end{align*}&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;Get an updated estimate of $x^{(t+1)}[n]$ by running the LSEE algorithm with target STFT of $\widehat{X}_w^{(t)}[mS, k]$&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x^{(t+1)}[n]=\frac{  \sum_{m=-\infty}^\infty w[n-Sm]\text{IDFT}\left(\widehat{X}_w^{(t)}[mS, k]\right) }{  \sum_{m=-\infty}^\infty w^2[n-Sm]  }\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;and go back to step 1.&lt;/p&gt;

&lt;p&gt;https://pdfs.semanticscholar.org/ade8/d1511a61d78948bb0d43e207593389935421.pdf&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="text to speech" /><summary type="html">The problem that Griffin-Lim algorithm tries to solve is the following. We are given some modified short-time-fourier transform (STFT) or modified spectrogram (only magnitude, no phase information). The STFT/Spectrogram is called ``modified’’ to stress the fact that it in general does not corresponds to any valid time-domain signal: it may be obtained by modifying a STFT/Spectrogram from some real signal, or they may simply be generated by a neural network. The task here is to synthesis a time-domain signal from this target STFT/Spectrogram, such that the STFT/Spectrogram of the synthesized signal is as close to the target as possible.</summary></entry><entry><title type="html">A step-by-step guide to variational inference (4): variational auto encoder</title><link href="https://yyang768osu.github.io/posts/2012/08/variational_inference_4/" rel="alternate" type="text/html" title="A step-by-step guide to variational inference (4): variational auto encoder" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/variational-inference-IV-variational-auto-encoder</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/variational_inference_4/">&lt;p&gt;The variational lower bound $\mathcal{L}$ sits in the core of variational inference. It connects the density estimation problem with the Bayesian inference problem through a variational (free to vary) distribution $q$, and it converts both problems into an optimization problem. Here let’s briefly revisit the identity associated with variational lower bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\ln p(X;\theta) &amp;= \text{KL}{\big(}q|| p(Z|X;\theta){\big)} + \mathcal{L}(q,\theta)\\
\text{where }\mathcal{L}(q, \theta) &amp;=\int q(Z) \ln \frac{p(X,Z;\theta)}{q(Z)} dZ
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The identity holds for any arbitrary probability function $q$. $\mathcal{L}$ is a lower bound for the data log-likelihood $\ln p(X;\theta)$ given the non-negativity of the KL divergence. From the identify we can obtain the following two equations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\ln p(X;\theta) &amp;= \max_{q} \mathcal{L}(q,\theta)\\
p(Z|X;\theta) &amp;= \underset{q}{\arg\max} \mathcal{L}(q,\theta),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which testified the claim that both density estimation (LHS of the first equation) and Bayesian inference (LHS of the second equation) are linked with the same optimization function. There are two implications if we increases the value of $\mathcal{L}$ by tweaking the distribution $q$: (1) $\mathcal{L}$ becomes a tighter lower bound of $\ln p(X;\theta)$, meaning that it is closer to the true data log-likelihood in value (2) the distribution function $q$ itself is closer to the true posterior distribution measured in KL divergence.&lt;/p&gt;

&lt;p&gt;Often, we are also given the task of finding the ML estimate of the parameter $\theta$ (or MAP estimate of the parameter $\theta$), which requires taking the maximum of $\ln p(X;\theta)$ (or $\ln p(X|\theta) + \ln p_\text{prior}(\theta)$ for the MAP case) with respect to $\theta$, yielding the following problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\max_{\theta}\max_{q}\mathcal{L}(q, \theta).
\end{align*}&lt;/script&gt;

&lt;p&gt;By increasing the variation lower bound $\mathcal{L}$ with respect to $\theta$, by which the model is parameterized, we are essentially searching for model that can better fit to the data.&lt;/p&gt;

&lt;p&gt;It should be clear that is it desirable to maximize $\mathcal{L}$ with respect to both the variational distribution $q$ and the generative parameter $\theta$: maximize it with respect to $q$ would yield a better inference function; maximize it with respect to $\theta$ would give us a better model.&lt;/p&gt;

&lt;p&gt;Instead of allowing $q$ to be any function within the probability function space, for analytical tractability, we assume that it is parameterized by $\phi$ and is a function of the observed data $x$, denoted as $q_\phi(x)$. For the generative model, let us modified the notation slightly by assuming that the prior distribution $p(z)$ is unparameterized, and denote the conditional generative probability as $p_\theta(x|z)$, leading to the following expression of the variational lower bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(\phi, \theta, x^{(i)}) = \int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln \frac{p_\theta\left(x^{(i)}|z^{(i)}\right)p\left(z^{(i)}\right)}{q_\phi\left(z^{(i)}|x^{(i)}\right)}dz^{(i)}
\end{align*}&lt;/script&gt;

&lt;p&gt;Note that we used to express the variational lower bound in terms of the complete observed dataset $X={x^{(1)},\ldots, x^{(N)}}$ as well as the corresponding latent variables $Z={z^{(1)},\ldots, z^{(N)}}$. Since each data point and the corresponding latent variables are generated independently, it can be decomposed into the summation of $N$ terms, one for each data point $x^{(i)}$ as shown above. Those $N$ identity equations are linked through global parameter $\phi$ and $\theta$.&lt;/p&gt;

&lt;p&gt;As discussed before, to obtain a better model and to obtain a closer approximation to the true posterior inference function, one needs to differentiate and optimize $\sum_{i=1}^N\mathcal{L}(\phi, \theta, x^{(i)})$ with respect to both $\phi$, the parameter of the inference function, and $\theta$, the parameter of the model. Here’s a plan: let us calculate the gradient for the lower bound with respect to both parameters, and be done with the problem by applying our favorite stochastic gradient descent algorithm to find a solution. Actually we will show later that such a stochastic training framework is analogous to using an auto-encoder architecture with a specific regularization function.&lt;/p&gt;

&lt;p&gt;Soon enough you will realize a major challenge: it is not clear how to differentiate against $\phi$. There is very little chance for us to expect a close-form expression if we directly differentiate what is inside the integral, as the integral itself is hard even without the differentiation. We will  spend some time here to dig into the issue, which is the key to the understanding of the variational auto-encoding algorithm.&lt;/p&gt;

&lt;p&gt;Since the lower-bound exists in the form of the expectation with respect to the variational distribution $q_\phi$, the work-around here is to seek for Monte-Carlo estimation for the integral with the sampling from distribution $q_\phi\left(z^{(i)}|x^{(i)}\right)$. Let us focus on the general problem of $\nabla_\phi \mathbb{E}_{q_\phi(z|x)}\left[f(z)\right]$, for which there are two approaches that use sampling to approximate the expectation:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 1&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\nabla_\phi \mathbb{E}_{q_\phi(z|x)}\left[f(z)\right] \\
=&amp;\int \nabla_\phi q_\phi(z|x)  f(z) dz\\
=&amp;\int q_\phi(z|x) \frac{\nabla_\phi q_\phi(z|x)}{q_\phi(z|x)}  f(z) dz\\
=&amp; \int q_\phi(z|x)  \nabla_\phi \ln q_\phi(z|x) f(z) dz \\
=&amp; \mathbb{E}_{q_\phi(z|x)}\left[ \nabla_\phi \ln q_\phi(z|x) f(z)\right] \\
\text{(Monte Carlo)} \approx &amp;\frac{1}{S}\sum_{s=1}^S \nabla_\phi \ln q_\phi(z^{[s]}|x) f(z^{[s]}) 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Approach 2&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;This approach makes an additional assumption on $q_\phi(z|x)$: assume that we can obtain samples of $z$ by first sampling through a distribution $p(\epsilon)$ that is independent of $\phi$, and then apply a $(\phi,x)$-dependent transformation of  $g_\phi(\epsilon, x)$. Effectively we are assuming that the random variable $\mathcal{Z}$ is a $\phi-$dependent function of a $\phi$-independent random variable $\mathcal{E}$: $\mathcal{Z} = g_\phi(\mathcal{E},x)$. Reflecting this assumption in the differential of expectation, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\nabla_\phi \mathbb{E}_{q_\phi(z|x)}\left[f(z)\right]\\
=&amp;\nabla_\phi \int q_\phi(z|x)f(z) dz\\
\text{(parameter substitution)}=&amp;\nabla_\phi \int p(\epsilon)f(g_\phi(\epsilon, x))d\epsilon\\
=&amp; \int p(\epsilon) \nabla_\phi f(g_\phi(\epsilon, x))d\epsilon\\
\text{(Monte Carlo)}\approx&amp; \frac{1}{S}\sum_{s=1}^S  \nabla_\phi f(g_\phi(\epsilon^{[s]},x))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This seems like a good solution: the Monte Carlo sampling itself is not a function of $\phi$, and $\phi$ just appear as the parameter of the transformation function  $g_\phi$ that maps the samples from $\mathcal{E}$ to the samples in $\mathcal{Z}$. In this case $q_\phi$ is just the induced distribution as a function of the prior distribution of $\mathcal{E}$ as well as the transformation function $g_\phi$. This parameter substitution technique is branded as &lt;em&gt;the reparameterization trick&lt;/em&gt; in the original paper of variational auto encoder.&lt;/p&gt;

&lt;p&gt;To understand the implication of such assumption, let’s ask this question: is it feasible to design the prior distribution of $\mathcal{E}$ and the transformation function $g_\phi$ in any arbitrary form? You may wonder why do we even care. Well there is a hidden factor that we need to take care of before claiming victory. Looking at the variational lower bound expression, not only do we need to integrate with respect to the distribution $q_\phi$, which can be achieved using Monte Carlo by the help of this reparameterization trick, we also need to ensure a closed-form expression of the density function $q_\phi(z|x)$ itself, as it lives inside the expectation/integral. This limits the way we can choose the random variable $\mathcal{E}$ and the function $g_\phi$.&lt;/p&gt;

&lt;p&gt;To investigate on the requirement of $\mathcal{E}$ and $g_\phi$ such that the induced random variable $\mathcal{Z} = g_\phi(\mathcal{E},x)$ has a tractable density/distribution function (easy to evaluate), let’s try to express distribution $q_\phi$ as a function of $p_\epsilon$ and $g_\phi(z,x)$. For any monotonic function $g_\phi$, the induced distribution $q_\phi$ &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_variable#Functions_of_random_variables&quot;&gt;can be derived&lt;/a&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_\phi(z) = p_\epsilon\left(g_\phi^{-1}(z)\right)\left|\frac{\partial g_\phi^{-1}(z)}{\partial z}\right|.
\end{align*}&lt;/script&gt;

&lt;p&gt;To enforce a closed form expression for $q_\phi$, we have two general design choices on $p_\epsilon$ and $g_\phi$, as is evident from the expression above: (1) let $p_\epsilon$ be a uniform distribution on $[0,1]$ and $g_\phi=\text{CDF}^{-1}$ be the inverse of any distribution with closed-form cumulative distribution function. (2) let $p_\epsilon$ be any distribution with closed form density and $g_\phi$ be an easy form of monotonic function, e.g., a linear function.&lt;/p&gt;

&lt;p&gt;In the context of variational auto encoder in the original paper, the second design choice is picked: $p_\epsilon$ is chosen as the standard normal distribution and  $g_\phi$ is a linear function of $\epsilon$, whose slope and intercept is an arbitrary function of $x$ and $\phi$ characterized using a neural network. In this case the induced distribution $q_\phi$ is a normal distribution whose mean and variances is determined by a neural network with the input $x$ and parameter $\phi$.&lt;/p&gt;

&lt;p&gt;Now that we went through what &lt;em&gt;the reparameterization trick&lt;/em&gt; is, let us return back to the problem of finding the gradient of $\mathcal{L}(\phi, \theta, x^{(i)})$ with respect to $\phi$ and $\theta$. Applying the reparameterization trick, we obtain the following gradient-friendly Monte Carlo estimate of the variational lower bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}(\phi, \theta, x^{(i)}) =&amp; \int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln \frac{p_\theta\left(x^{(i)}|z^{(i)}\right)p\left(z^{(i)}\right)}{q_\phi\left(z^{(i)}|x^{(i)}\right)}dz^{(i)}\\
\text{(Monte Carlo)}\approx&amp;\frac{1}{S}\sum_{s=1}^S \ln  \frac{p_\theta\left(x^{(i)}|z^{(i)[s]}\right)p\left(z^{(i)[s]}\right)}{q_\phi\left(z^{(i)[s]}|x^{(i)}\right)}\\
\text{(Reparameterization)}=&amp;\frac{1}{S}\sum_{s=1}^S \ln  \frac{p_\theta\left(x^{(i)}|g_\phi (\epsilon^{[s]}, x^{(i)})\right)p\left(g_\phi (\epsilon^{[s]}, x^{(i)})\right)}{q_\phi\left(g_\phi (\epsilon^{[s]}, x^{(i)})|x^{(i)}\right)}\\
\text{where } \epsilon^{[s]}&amp;\text{ is drawn i.i.d. from }p_\epsilon. 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here’s an alternative way to decompose $\mathcal{L}$ and apply Monte Carlo and reparameterization, for which there is a close form expression for the second term (KL divergence) and only the first part is approximated.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}(\phi, \theta, x^{(i)}) =&amp; \int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln \frac{p_\theta\left(x^{(i)}|z^{(i)}\right)p\left(z^{(i)}\right)}{q_\phi\left(z^{(i)}|x^{(i)}\right)}dz^{(i)}\\
=&amp;\int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln p_\theta\left(x^{(i)}|z^{(i)}\right) dz^{(i)}-\text{KL}\left( q_\phi\left(z^{(i)}|x^{(i)}\right){\Big|\Big|} p\left(z^{(i)}\right)\right)\\
\text{(Monte Carlo)}\approx&amp; \frac{1}{S}\sum_{s=1}^S \ln p_\theta\left(x^{(i)}|z^{(i)[s]}\right)  -\text{KL}\left( q_\phi\left(z^{(i)}|x^{(i)}\right){\Big|\Big|} p\left(z^{(i)}\right)\right)\\
\text{(Reparameterization)}\approx&amp; \frac{1}{S}\sum_{s=1}^S \ln p_\theta\left(x^{(i)}|g_\phi (\epsilon^{[s]}, x^{(i)})\right)  -\text{KL}\left( q_\phi\left(z^{(i)}|x^{(i)}\right){\Big|\Big|} p\left(z^{(i)}\right)\right)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This decomposition leads to the interpretation of probabilistic auto-encoder, which is named variational auto-encoder as it rooted from the maximization of the variational lower bound.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/vae.png&quot; alt=&quot;Variational Auto Encoder&quot; /&gt;&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="variational inference" /><summary type="html">The variational lower bound $\mathcal{L}$ sits in the core of variational inference. It connects the density estimation problem with the Bayesian inference problem through a variational (free to vary) distribution $q$, and it converts both problems into an optimization problem. Here let’s briefly revisit the identity associated with variational lower bound</summary></entry><entry><title type="html">A step-by-step guide to variational inference (3): mean field approximation</title><link href="https://yyang768osu.github.io/posts/2012/08/variational_inference_3/" rel="alternate" type="text/html" title="A step-by-step guide to variational inference (3): mean field approximation" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/variational-inference-III-mean-field-approximation</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/variational_inference_3/">&lt;p&gt;We have learned in the previous post that E-M algorithm tries to find a ML or MAP solution to the parameters of a generative model. It is build on top of two major premises:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\ln p(X,Z;\theta)$ is in a much simpler form than $\ln P(X;\theta)$,&lt;/li&gt;
  &lt;li&gt;$\ln p(Z|X;\theta)$ is easy to obtain.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;While the first one is often true in that both $p(X|Z;\theta)$ and $p(Z;\theta)$ given as part of the model and are usually designed to be simple, the second one is a very strong assumption and does not hold in most cases. In this post, we remove the second premise, and introduce a way to obtain an approximation of $p(Z|X;\theta)$.&lt;/p&gt;

&lt;p&gt;In what follows, we modify notation slightly by assuming that there is prior distribution on any parameters of interest, and conceptually merge $\theta$ as part of the latent variable $Z$ (which is common across different data samples) and remove $\theta$ from the notation.&lt;/p&gt;

&lt;p&gt;As we discussed in the previous posts, the Bayesian inference problem is to find the posterior probability $p(Z|X)$, which is in general very hard due to the integral/summation (in most cases multi-dimensional integral/summation) in the denominator below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
p(Z|X) = \frac{p(X|Z)p(Z)}{\int p(X|Z)p(Z) dZ}.
\end{align*}&lt;/script&gt;

&lt;p&gt;We also showed that with the introduction of a variational distribution $q(Z)$, we can convert the problem of finding $p(X|Z)$ as an optimization problem below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(Z|X) &amp;= \arg\max_{q}\mathcal{L}(q).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;However, this optimization problem above is still very hard and it does not lend itself to any easy solution. Here, the objective $\mathcal{L}(q)$, called the variational lower-bound, is a functional as it maps a function into a scalar value, whose domain is the space of all functions.&lt;/p&gt;

&lt;p&gt;Since it is hard to optimize the variational lower bound as is, one may wonder, how about constraining the search space of $q$ from all potential functions to within a limited function space? Could that make the problem simpler? Even though we may not find the optimal solution after restricting the set of functions we could search from, the hope is that by doing so we can device practical algorithms with solutions that are reasonably close to the true posterior. This is exactly the idea behind mean field approximation.&lt;/p&gt;

&lt;p&gt;In the mean field method, we add a constraint to the domain of the optimization: instead of allowing $q(Z)$ to be in arbitrary form, we only look at cases when it can be factorized into a product form with disjoint latent variables in each multiplicative factor. More specifically, we divide the dimension of latent variables into $K$ groups $Z=[Z_1, Z_2, \ldots, Z_K]$ and enforce $q$ to have the form of $q(Z)=q_1(Z_1)q_2(Z_2)\ldots q_K(Z_K)$. Put it in precise math, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q^* = \underset{q=q_1 q_2 \ldots q_K}{\arg\max}\mathcal{L}(q)
\end{align*}&lt;/script&gt;

&lt;p&gt;Referring back to the equation on the decomposition of observed data log-likelihood&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\ln p(X) = \text{KL}(q || p(Z|X)) + \mathcal{L}(q),
\end{align*}&lt;/script&gt;

&lt;p&gt;we know that by maximizing $\mathcal{L}(q)$ with respect to functions with form $q(Z)=\prod_{k=1}^K q_k(Z_k)$, we are trying to find one function in the confined function space (defined as the set of functions that can be factorized as such) that is closest to the true posterior $ p(Z|X)$ measured in KL divergence. It is worth emphasizing that we are merely constraining $q(Z)$ to have this factorization form, and do not make any assumption on what each individual factor would look like.&lt;/p&gt;

&lt;p&gt;Let us plug in the factorized form of $q(Z) = \prod_{k=1}^K q_k(Z_k)$ in the expression of the variational lower bound, yielding&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}(q) =&amp; \int q(Z)\ln\frac{p(X, Z)}{q(Z)}dZ\\
=&amp; \int q(Z)\ln p(X, Z)dZ +  \int q(Z)\ln\frac{1}{q(Z)}dZ\\
=&amp; \int \prod_{k=1}^K q_k(Z_k) \ln p(X, Z)dZ +  \sum_{k=1}^K\int q_k(Z_k)\ln\frac{1}{q_k(Z_k)}dZ_k.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The second term is just the entropy of $q$, which, given the assumption that it can be decomposed into independent factors, becomes the summation of the entropy for each individual $q_k$.&lt;/p&gt;

&lt;p&gt;It may not be immediately apparent why this modified formulation is any easier to solve. Nevertheless, let us proceed by making the temporary assumption that among the $K$ factors, all are known except for one factor $q_j$. Then, we just need to maximize $\mathcal{L}$ with respect to $q_j$ with all the other factors $q_{k}, k\not=j$ as given. The variational lower bound can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\int q_j(Z_j) \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]dZ_j + \int q_j(Z_j)\ln \frac{1}{q_j(Z_j)}dZ_j + \text{constant}\\
=&amp;\int q_j(Z_j)\ln\frac{\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)}{q_j(Z_j)}dZ_j + \text{constant}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since any term that does not involve $q_j$ would not affect the solution to $\arg\max_{q_j}\mathcal{L}(q)$, we just mark those as constant. Here it comes a key observation: notice how the non-constant term resembles the definition of a negative KL divergence between $q_j(Z_j)$ and $\mathbb{E}_{q_k, k\not=j}$ $\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)$. The only issue is that $\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)$ may not be a proper probability measure that sum/integrate to $1$. Luckily, since scaling $\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)$ only amounts to adding/subtracting a constant term, we know that $\mathcal{L}(q)$ is maximized when&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_j(Z_j)  \propto \exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right),
\end{align*}&lt;/script&gt;

&lt;p&gt;or more accurately,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_j(Z_j)  = \frac{\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)}{\int \exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right) dZ_j}.
\end{align*}&lt;/script&gt;

&lt;p&gt;This result tells us that, among the $K$ factors, if we have all but one factor fixed, then the optimal solution of that left out function that maximize the variation lower bound (or equivalently, minimizes the KL divergence to the true posterior distribution) can be written in the above form as a function of all the other factors.&lt;/p&gt;

&lt;p&gt;This leads to a nice iterative solution that iteratively visits each factor, and maximize the variational lower bound with respect to the target factor treating all the other factors as known. In special cases, the normalization constant term in the dominator of the above equation could be directly inferred if the numerator term already suggests certain type of known distribution.&lt;/p&gt;

&lt;p&gt;It is interesting to note that, to apply this mean-field-approximation method, one only need to make the assumption on how to partition the latent variable dimensions into disjoint groups, one for each factor, without making any assumption on the detailed function form of any factor. The detail form of the factorized distribution would be obtained as a result of the iterative procedure.&lt;/p&gt;

&lt;p&gt;There is one caveat that we should mention. Looking at the equation above, to find the optimal factor $q_j$, assuming all the other are know, we still need to make sure that the expectation $\mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]$ results in tractable form. Given that the expectation itself is a multi-dimensional integral/summation, in general it is hard to guarantee a closed form expression. The expectation may be tractable with specific models and specific ways on which the latent variables are partitioned, which limits the domain where mean-field-approximation could be applied.&lt;/p&gt;

&lt;p&gt;Here we introduced mean field approximation from the perspective of Bayesian inference. As a final remark, it is straightforward to show that it also provide a way to evaluate observed data likelihood and thus can be useful with model-selection as well. According to the identity below, we know that as we maximize $\mathcal{L}$, not only do we obtain a variational distribution that is close to the true posterior in the KL divergence sense, we also obtained a surrogate for the log-likelihood, as the lower bound $\mathcal{L}$ is a lower bound which gets tighter as it becomes larger.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\ln p(X) = \text{KL}(q || p(Z|X)) + \mathcal{L}(q),
\end{align*}&lt;/script&gt;

&lt;p&gt;If we are given $M$ models, then one can conduct mean field method on each of them, obtain the corresponding optimized variational lower-bound, and use it as the surrogate to rate the likelihood of each model. We can even combine the prior distribution of the $M$ models, if there is any, to obtain a maximum a posterior (MAP) selection of the model.&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="variational inference" /><summary type="html">We have learned in the previous post that E-M algorithm tries to find a ML or MAP solution to the parameters of a generative model. It is build on top of two major premises:</summary></entry><entry><title type="html">A step-by-step guide to variational inference (2): expectation maximization</title><link href="https://yyang768osu.github.io/posts/2012/08/varitional_inference_2/" rel="alternate" type="text/html" title="A step-by-step guide to variational inference (2): expectation maximization" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/variational-inference-II-expectation-maximization</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/varitional_inference_2/">&lt;p&gt;In the previous post we went through the derivation of variational lower-bound, and showed how it helps convert the Bayesian inference and density estimation problem to an optimization problem. Let’s briefly recap the problem setup and restate some key points.&lt;/p&gt;

&lt;p&gt;Consider a very general generative graphic model where each data point $x^{(n)}$ is generated from a latent variable $z^{(n)}$ conforming to a given distribution $p(X|Z;\theta)$, with $z^{(n)}$ itself drawn from a given prior distribution $p(Z; \theta)$. $\theta$ captures the set of variables that the two probabilities are parameterized with. Two fundamental problems are to (1) estimate the density of existing dataset $X$, i.e. $p(X;\theta)$ and (2) derive the posterior probability of the latent variable $Z$ given the observed data $X$, i.e., $p(Z|X;\theta)$. The exact solution of both problems requires the evaluation of the often intractable integral $\int P(X| Z;\theta)P(Z;\theta)dZ$.&lt;/p&gt;

&lt;p&gt;With the introduction of a variational/free distribution function $q(Z)$, we have the following identity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\ln p(X;\theta) = \text{KL}{\big(}q||p(Z|X;\theta){\big)} + \mathcal{L}(q,\theta),
\end{align*}&lt;/script&gt;

&lt;p&gt;which says that the marginalized probability of dataset $X$ can be decomposed into a sum of two terms with the first one being the KL divergence between $q(Z)$ and the true posterior distribution $p(Z|X;\theta)$ and the second one expressed below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(q,\theta)=\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ,
\end{align*}&lt;/script&gt;

&lt;p&gt;which is referred to as the variational lower bound: it is called a lower-bound as it is always less than $\ln p(X;\theta)$ due to the non-negativity of KL divergence, and it is called variational as it is itself a functional that maps a variational/free distribution function $q$ to a scalar value. This identity is quite exquisite in that it turns both the density estimation problem and the latent variable inference problem into an optimization problem, evident from the two equations below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\ln p(X;\theta) &amp;= \max_{q}\mathcal{L}(q,\theta)\\
p(Z|X;\theta) &amp;= \arg\max_{q}\mathcal{L}(q,\theta).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The problem that Expectation Maximization algorithm designs to solve is the maximum-likelihood (ML) estimate of the parameter $\theta$. Mind you that $\theta$ is the parameter of the graphic model, and the task is to find a $\theta$ such that the model best explain the existing data. In precise term, the problem is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\max_{\theta}\ln p(X;\theta).
\end{align*}&lt;/script&gt;

&lt;p&gt;Now, resorting to the variational lower bound, equivalently we can also focus on the following maximization-maximization problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\max_{\theta}\ln p(X;\theta) = \max_{\theta}\max_{q}\mathcal{L}(q,\theta),
\end{align*}&lt;/script&gt;

&lt;p&gt;A natural question is: why would this be any easier to evaluate compared with maximizing $\ln p(X;\theta)$ head on? did we increase our burden by considering a nested-maximization optimization problem rather than a single-maximization one?&lt;/p&gt;

&lt;p&gt;To answer we need to have the objective function under scrutiny. Looking at the detailed expression of $\mathcal{L}(q,\theta)$, the main hurdle is the evaluation the log-likelihood of the joint observed-latent variable $p(Z,X;\theta)$. We want to emphasis that the two probability distributions $p(Z;\theta)$ and $p(X|Z;\theta)$ are given as part of the model assumption, and they usually come in the form of well-known distributions, e.g., Gaussian, multinomial, exponential, etc. Thus, the joint likelihood of observed and hidden variable $p(Z,X;\theta)=p(Z;\theta)p(X|Z;\theta)$ is in an amenable form. Also, quite often, taking logarithm on it would break up all the multiplicative terms as summation, resulting in quite tractable from. Better yet, the parameters $\theta$ that we need to compute gradient with, may naturally be decomposed into different terms in the summation, making the calculation of derivative easy with respect to individual parameters.&lt;/p&gt;

&lt;p&gt;On the other hand, to compute the marginalized likelihood of the observed data only, i.e., $P(X;\theta)$, one need to sum or integrate out the effect of $Z$ from $p(Z,X;\theta)$, which may lead to complicated expression. While the evaluation of $P(X;\theta)$ may still be fine when, e.g., the marginalization only requires the summation of a finite number of terms (which is the case for the Gaussian mixture model), the real deal breaker is the difficulty in taking derivative of the log-likelihood with respective to the parameters: taking logarithm on $P(X;\theta)$ almost surely won’t result in nice decomposition, as the logarithm is gated by the integral or summation, and the log-sum expression is a lot harder to break when we compute the derivative with respect to the parameters $\theta$.&lt;/p&gt;

&lt;p&gt;Coming back to the maximization-maximization problem, it is natural to devise an iterative algorithm that maximize the objective function $\mathcal{L}(q,\theta)$ with alternating axis:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{E step: }&amp;q^{(t)} = \arg\max_{q}\mathcal{L}(q,\theta^{(t)})\\
\text{M step: }&amp;\theta^{(t+1)} = \arg\max_{\theta}\mathcal{L}(q^{(t)}, \theta) 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;It is worth mentioning that the first optimization problem is in general a very difficult one, as it requires searching through the whole function space. According to the derivation of the variational lower bound derivation we know that the optimal solution is the posterior distribution $p(Z|X;\theta^{(t)})$, which is hard to obtain. Actually finding an approximated posterior by maximizing the variational lower bound is the main theme in variational inference. Techniques of mean-field-approximation, and variational auto-encoder, which we cover in subsequent posts, targets at this problem.&lt;/p&gt;

&lt;p&gt;To proceed, we make a very strong assumption that $p(Z|X;\theta^{(t)})$ can be easily obtained. As we will see later that with certain simple model (e.g., Gaussian mixture model), it is indeed a valid assumption, nevertheless it is the key assumption that significantly limits the application of the expectation maximization algorithm.&lt;/p&gt;

&lt;p&gt;Anyway, for now let us live with this strong assumption, then the E-step results in the following  expression&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\text{E step: }q^{(t)} = p(Z|X;\theta^{(t)}).
\end{align*}&lt;/script&gt;

&lt;p&gt;Coming back to the second maximization problem (M-step), with $q^{(t)}$ fixed, we can decompose the variational lower bound as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(q^{(t)}, \theta) = \int q^{(t)}(Z)\ln p(X,Z;\theta)dZ + \int q^{(t)}(Z) \ln\frac{1}{q^{(t)}(Z)}dZ.
\end{align*}&lt;/script&gt;

&lt;p&gt;The second term above is just a constant term reflecting the entropy of $q^{(t)}$, so let us ignore it, and then the second maximization problem reduces to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{M step: }\theta^{(t+1)} =&amp;\max_{\theta} \int p(Z|X;\theta^{(t)}) \ln P(Z,X;\theta)dZ. 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The maximization target above can be viewed as finding the expectation of complete data (combining observed variable and latent variable) log likelihood, where the expectation is with respect to a fixed distribution on the latent variable $Z$.&lt;/p&gt;

&lt;p&gt;Let’s put the two steps together and review the whole iterative process. We are given a model with a set of parameters captured in $\theta$. The task is find the values of the parameters $\theta$ such that the model best explain the existing observed data at hand. At the beginning, we take a random guess on the value of the parameters. With that initial parameters, we find the posterior probability of the latent variable for each data point $x$ in the training data set $X$. Then, using that posterior probability, we calculate the expected complete-data log-likelihood, and try to find parameters $\theta$ so that the complete-data log-likelihood is maximized. With $\theta$ updated, we refresh our calculation of the posterior probability and iterative the process.&lt;/p&gt;

&lt;p&gt;In fact, K-means clustering algorithm is one instance of expectation-maximization procedure with certain model assumption. It is helpful to think of the E-M iterative process from the perspective of K-means clustering: for K-means clustering, the latent variable is one-dimensional with value from $1$ to $K$, implying the registration to one of the $K$ clusters. The parameter of the model is the center of the clusters, denoted as $\theta={c_1, \ldots, c_K}$. In the initial setup, we randomly set these $K$ cluster centers. For each data, we assign it to the nearest cluster, which is effectively assigning its latent variable. This step corresponds to finding the posterior distribution (E-step), with one of the clustering having probability $1$. After each data is assigned to its cluster with the initial values of the cluster centers, which gives us complete data in the form of (observed data, latent variable) pair, the next step is to adjust the center based on its constituent. This step corresponds to the maximizing of the expected complete-data log-likelihood (M-step), although this expectation is taken in a degenerated form as the posterior probability for the latent variable is in the form of $0/1$.&lt;/p&gt;

&lt;p&gt;We finish the treatment of E-M algorithm with the following closing remarks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The E-M iterative algorithm is guaranteed to reach a local maximum on the log-likelihood of the observed data $p(X;\theta)$, as both steps increases it.&lt;/li&gt;
  &lt;li&gt;It is not necessary to find the maximum in the M-step. So long as the updated $\theta$ increase the complete-data log-likelihood, we are still in the right direction.&lt;/li&gt;
  &lt;li&gt;So far we focused on finding the maximum-likelihood (ML) solution to $\theta$ (local maximum). In the case when there is prior distribution $p_\text{prior}(\theta)$ on $\theta$, we can use the same process to find a maximum-a-posterior (MAP) solution (local maximum), utilizing the fact that $p(\theta|X) \propto p(X|\theta)p_\text{prior}(\theta)$. The problem is modified as&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\max_{\theta}\ln p(\theta|X) = \max_{\theta}\left(\max_{q}\mathcal{L}(q,\theta) {\color{red} + \ln p_\text{prior}(\theta)}\right),
\end{align*}&lt;/script&gt;

&lt;p&gt;with slightly modified procedure below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{E step: }&amp;q^{(t)} = p(Z|X,\theta^{(t)})\\
\text{M step: }&amp;\theta^{(t+1)} =\max_{\theta} \int p(Z|X,\theta^{(t)}) \ln P(Z,X|\theta)dZ {\color{red} + \ln p_\text{prior}(\theta)}. 
\end{align*} %]]&gt;&lt;/script&gt;</content><author><name>Yang Yang</name></author><category term="variational inference" /><summary type="html">In the previous post we went through the derivation of variational lower-bound, and showed how it helps convert the Bayesian inference and density estimation problem to an optimization problem. Let’s briefly recap the problem setup and restate some key points.</summary></entry><entry><title type="html">A step-by-step guide to variational inference (1): variational lower bound</title><link href="https://yyang768osu.github.io/posts/2012/08/variational_inference_1/" rel="alternate" type="text/html" title="A step-by-step guide to variational inference (1): variational lower bound" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/variational-inference-I-varitional-lower-bound</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/variational_inference_1/">&lt;p&gt;In machine learning, graphic models are often used to describe the factorization of probability distributions. The detailed form of the graphic model encodes one’s belief/hypothesis regarding the underlying structure of the data. In this article, we confine the discussion to a general form of directed graphic model as illustrate below.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/dgm.png&quot; alt=&quot;generative model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let $X=\{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\}$ denotes the dataset of interest, be it a set of images, a set of sound clips, or a set of document, depending on the problem at hand. The model describes a way for which the data is generated: we first sample a hidden/latent variable $z$ from the distribution $P(Z;\theta)$, and then sampled a data point $x$ from the distribution $P(X|Z;\theta)$ given the latent variable. The two probabilities are defined as we like and are given as part of the graphic models. Here we assume that the two probabilities are parameterized by a set of variables $\theta$, although in general we could merge it as part of latent variable $Z$ as well (as a global latent variable, the value of which is shared among all data samples), if there is a prior distribution for it. It is worth noting that the $P(Z;\theta)$ and $P(X|Z;\theta)$ could be further factorized, whichever way we design them to be, however, in this article we will just focus on the general setting.&lt;/p&gt;

&lt;p&gt;There are two intertwined problems associated with this form of generative models: density estimation, and Bayesian inference. For the problem of density estimation, we want to estimate the probability that the model assigns to all the data $X$ we are given in training, or more precisely $p(X; \theta)$. The value of $p(X; \theta)$ explains how likely it is for the model to generate the given training data. The larger the $p(X; \theta)$, the better our model explains the existing data. For models that are parameterized by $\theta$, fitting the model to best match the training data amounts to finding a value of $\theta$ that maximize the density $p(X; \theta)$.&lt;/p&gt;

&lt;p&gt;For the problem of Bayesian inference, we want to infer the posterior probability of the unobserved hidden/latent variable given the observed data, or more precisely $p(Z|X;\theta)$. It is easy to see that these two problems are naturally intertwined from Bayes rule: $p(Z|X) = \frac{p(X|Z)p(Z)}{P(X)}$: since $p(X|Z)$ and $p(Z)$ are already given as part of the model assumption, if we solve one of the two problems, then the other one can be solved as well. Conceptually, the solution to the problems can be viewed as trying to find a reverse graph in the generative model.&lt;/p&gt;

&lt;p&gt;Let’s now take a step back and ask the question: why do we even bother with the introduction of latent/hidden variable? Can we just propose a model that captures $p(X;\theta)$ directly, and save the trouble of Bayesian inference for the latent variable all together?  Anyway, even with the direct characterization of  $p(X;\theta)$, the discussion above should still holds: the larger the  $p(X;\theta)$, the better our model explains the given data, and with a good model we can apply sampling to generate artificial data.&lt;/p&gt;

&lt;p&gt;The significance of the hidden/latent variables are two-fold:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The adoption of hidden/latent variables allows one to construct complex marginal data distributions $p(X)$ from simple and easy to evaluate distribution functions. For example, with $p(Z;\theta)$ being the multinomial distribution and $p(X|Z;\theta)$ being the normal distribution, we arrive at the Gaussian mixture model $p(X;\theta)=\int P(X|Z;\theta)P(Z;\theta)dZ$, which can characterize a wide range of complex distribution and has significantly more expressiveness power compared with just Gaussian or multinomial distribution alone. It is evident that a model that characterizes more complex distributions can better fit the data, especially with the high-dimensional complicated data we are usually focusing on.&lt;/li&gt;
  &lt;li&gt;The hidden/latent variables can be viewed as general features extracted from the data, which can be utilized for any downstream tasks. The hidden/latent variables normally has much lower dimension compared with the data itself, and they represent low-dimensional message that conveys condensed information regarding the corresponding data. If a model can fit the data well, meaning that the likelihood is high for the model to generate the training data by sampling $X$ conditioned on a sampling of $Z$, then one can argue that $Z$ should capture the essence of the data. It is interesting to note how the above two points echo the previous discussion regarding the inter-connection between density estimation and Bayesian inference.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the Bayesian inference problem, as stated before, the task is to find the posterior distribution of the unobserved variable $Z$ given then observed variable $X$. Instead of tackling this problem head on by deriving $p(Z|X;\theta)=\frac{p(X|Z;\theta)p(Z;\theta)}{\int p(X|Z;\theta)p(Z;\theta)dZ}$, which is often intractable, let us introduce another distribution $q(Z)$ with the goal of mimicking $p(Z|X;\theta)$, and look at what the KL divergence between the two could decompose into:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\text{KL}{\big(}q||p(Z|X;\theta){\big)}\\
=&amp;\int q(Z) \ln \frac{q(Z)}{p(Z|X;\theta)}dZ\\
=&amp;\int q(Z) \ln \frac{q(Z)}{P(Z|X;\theta)}\frac{p(X;\theta)}{p(X;\theta)}dZ\\
=&amp;\int q(Z) \ln \frac{q(Z)p(X;\theta)}{P(Z,X;\theta)}dZ\\
=&amp;\int q(Z) \ln p(X;\theta)dZ-\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ\\
=&amp;\ln p(X;\theta)-\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Making the short-hand notation of&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(q,\theta)=\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ,
\end{align*}&lt;/script&gt;

&lt;p&gt;we can simplify the above equation as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\ln p(X;\theta) = \text{KL}{\big(}q||p(Z|X;\theta){\big)} + \mathcal{L}(q,\theta).
\end{align*}&lt;/script&gt;

&lt;p&gt;The above equation is the cornerstone for a broad range of variational methods, which we will keep coming back to for later posts. Let’s stare at it for a while, observe it from different angles, and learn to appreciate its elegancy.&lt;/p&gt;

&lt;p&gt;We should first observe that the three terms have fixed polarity: KL divergence is always nonnegative, whereas the log-likelihood term on the LHS of the equation, as well as the expression $\mathcal{L}(q,\theta)$ is always non-positive. At first glance into the definition of $\mathcal{L}$, it may look like it can be written in the form of negative KL divergence. However, one should note that $P(Z,X;\theta)$ is not a proper probability on $Z$ as $\int p(X,Z;\theta)dZ = p(X;\theta)&amp;lt;1$.&lt;/p&gt;

&lt;p&gt;Given that the KL divergence term is always nonnegative, $\mathcal{L}(q,\theta)$ yield a lower bound on the log-likelihood of the data. In precise term, we have $\ln p(X;\theta) \geq \mathcal{L}(q,\theta)$.&lt;/p&gt;

&lt;p&gt;The term $\mathcal{L}(q,\theta)$ can be viewed as a functional that maps a probability distribution function into a value. 
Since the analysis and optimization of functional falls into the realm of calculus of variations, the distribution function $q$ itself is often called the variational distribution, and the lower bound $\mathcal{L}(q,\theta)$ is referred to as the variational lower-bound.&lt;/p&gt;

&lt;p&gt;It is important to realize that the above equation is another manifestation of the inter-connection between the data likelihood $p(X;\theta)$ and the posterior distribution of latent variable $p(Z|X;\theta)$, this time linked through the variational distribution function $q$. For a fixed parameter $\theta$, if we increase the variational lower bound $\mathcal{L}(q,\theta)$ by adjusting $q$, then the updated lower-bound is closer to the log-likelihood of the data. At the same time, since an increment in $\mathcal{L}(q,\theta)$ would infer a decrement of $\text{KL}(q||p(Z|X;\theta))$, we know that the updated variational distribution $q$ is closer to the true posterior distribution measured in KL divergence. To precisely capture these observations, we arrive at the following two equations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\ln p(X;\theta) &amp;= \max_{q}\mathcal{L}(q,\theta)\\
p(Z|X;\theta) &amp;= \arg\max_{q}\mathcal{L}(q,\theta).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This is the core of variational inference: with an introduction of a variational distribution $q$, we can turn both the log-likelihood calculation (i.e., density estimation) problem and the Bayesian inference problem into an optimization problem, and attack it with different optimization algorithms. This inference-optimization duality provides a very powerful tool. It is the backbone of many of the variational inference related methods such as expectation-maximization, mean-field approximation, and variational auto-encoder, which we will discuss in details in the subsequent posts.&lt;/p&gt;

&lt;p&gt;As a closing note below we list two alternative proofs for the variational lower-bound.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\ln p(X;\theta)\\
=&amp;\ln\frac{p(X,Z;\theta)}{p(Z|X;\theta)}\\
=&amp;\int q(Z)\ln\frac{p(X,Z;\theta)}{p(Z|X;\theta)}dZ\\
=&amp;\int q(Z)\ln\frac{p(X,Z;\theta) q(Z)}{p(Z|X;\theta) q(Z)}dZ\\
=&amp;\int q(Z)\ln\frac{p(X,Z;\theta)}{q(Z)}dZ+\int q(Z)\ln\frac{q(Z)}{p(Z|X;\theta)}dZ.\\
=&amp;\mathcal{L}(q,\theta) + \text{KL}{\big(}q||p(Z|X;\theta){\big)}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\ln p(X;\theta)\\
=&amp;\ln \int p(X,Z;\theta)dZ\\
=&amp;\ln \int q(Z)\frac{p(X,Z;\theta)}{q(Z)}dZ \\
\geq &amp; \int q(Z)\ln\frac{p(X,Z;\theta)}{q(Z)}dZ.
\end{align*} %]]&gt;&lt;/script&gt;</content><author><name>Yang Yang</name></author><category term="variational inference" /><summary type="html">In machine learning, graphic models are often used to describe the factorization of probability distributions. The detailed form of the graphic model encodes one’s belief/hypothesis regarding the underlying structure of the data. In this article, we confine the discussion to a general form of directed graphic model as illustrate below.</summary></entry></feed>