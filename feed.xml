<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://yyang768osu.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yyang768osu.github.io/" rel="alternate" type="text/html" /><updated>2020-06-29T19:49:29-07:00</updated><id>https://yyang768osu.github.io/</id><title type="html">Yang Yang</title><subtitle>Engineer at Qualcomm</subtitle><author><name>Yang Yang</name></author><entry><title type="html">Notes on Video Frame Synthesis</title><link href="https://yyang768osu.github.io/posts/2020/06/video-frame-synthesis/" rel="alternate" type="text/html" title="Notes on Video Frame Synthesis" /><published>2020-06-27T00:00:00-07:00</published><updated>2020-06-27T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2020/06/notes-on-video-frame-synthesis</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2020/06/video-frame-synthesis/">&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Paper&lt;/th&gt;
      &lt;th&gt;Organization&lt;/th&gt;
      &lt;th&gt;Conference&lt;/th&gt;
      &lt;th&gt;Arxiv&lt;/th&gt;
      &lt;th&gt;Citation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ac&quot;&gt;AdaptConv&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Portland SU&lt;/td&gt;
      &lt;td&gt;CVPR-2017&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.07514&quot;&gt;2017-05-22&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;149&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#asc&quot;&gt;AdaptSepConv&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Portland SU&lt;/td&gt;
      &lt;td&gt;ICCV-2017&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.01692&quot;&gt;2017-08-05&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;209&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#dvf&quot;&gt;DeepVoxFlow&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;CUHK UIUC PonyAI Google&lt;/td&gt;
      &lt;td&gt;ICCV-2017&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1702.02463&quot;&gt;2017-08-05&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;279&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ssm&quot;&gt;SuperSloMo&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;UMass NVIDIA UC-Merced&lt;/td&gt;
      &lt;td&gt;CVPR-2018&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1712.00080&quot;&gt;2018-07-13&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;142&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#pa&quot;&gt;PhaseNet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;ETH Disney&lt;/td&gt;
      &lt;td&gt;CVPR-2018&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.00884&quot;&gt;2018-04-03&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;47&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#da&quot;&gt;DepthAware&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;SJTU UC-Merced Google&lt;/td&gt;
      &lt;td&gt;CVPR-2019&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.00830&quot;&gt;2019-04-01&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#quad&quot;&gt;Quadratic&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;CMU SenseTime BNU UC-Merced&lt;/td&gt;
      &lt;td&gt;NeurIPS-2019&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.00627&quot;&gt;2019-11-02&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#aim2019&quot;&gt;AIM2019&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;ICCVW-2019&lt;/td&gt;
      &lt;td&gt;ICCVW-2019&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.07783&quot;&gt;2019-11-19&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;22&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Why video frame synthesis is needed?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;noval view synthesis/interpolation&lt;/li&gt;
  &lt;li&gt;frame rate up-conversion&lt;/li&gt;
  &lt;li&gt;slow motion effect&lt;/li&gt;
  &lt;li&gt;frame recovery in video streaming&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Chanllenging scenarios for video synthesis&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;occlusion&lt;/li&gt;
  &lt;li&gt;motion blurs&lt;/li&gt;
  &lt;li&gt;abrupt brightness/lighting change&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ac&quot;&gt;AdaptConv&lt;/h2&gt;
&lt;p&gt;Simon Niklaus, Long Mai, Feng Liu, “&lt;em&gt;Video Frame Interpolation via Adaptive Convolution&lt;/em&gt;,” CVPR-2017&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;asc&quot;&gt;AdaptSepConv&lt;/h2&gt;
&lt;p&gt;Simon Niklaus, Long Mai, Feng Liu, “&lt;em&gt;Video Frame Interpolation via Adaptive Separable Convolution&lt;/em&gt;,” ICCV-2017&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;dvf&quot;&gt;DeepVoxFlow&lt;/h2&gt;
&lt;p&gt;Ziwei Liu, Raymond A. Yeh, Xiaoou Tang, Yiming Liu, Aseem Agarwala, “&lt;em&gt;Video Frame Synthesis using Deep Voxel Flow&lt;/em&gt;,” ICCV-2017&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ssm&quot;&gt;SuperSloMo&lt;/h2&gt;
&lt;p&gt;Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik Learned-Miller, Jan Kautz, “&lt;em&gt;Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation&lt;/em&gt;,” CVPR-2018&lt;/p&gt;

&lt;h3 id=&quot;main-idea&quot;&gt;Main Idea&lt;/h3&gt;

&lt;p&gt;Very crude estimation of $f_{t\to 1}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
f_{t\to 1}&amp;\approx (1-t)f_{0\to 1}\\
f_{t\to 1}&amp;\approx -(1-t)f_{1\to 0}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Weighing the two crude estimation by the relative distance to $t$ yields the estimate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\hat{f}_{t\to 1}&amp;\triangleq (1-t)^2 f_{0\to 1} - t(1-t)f_{1\to 0}\\\
\hat{f}_{t\to 0}&amp;\triangleq t^2 f_{1\to 1} - t(1-t)f_{0\to 1}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/frame_synthesis/SuperSloMo.png&quot; alt=&quot;SuperSloMo&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;network-architecture&quot;&gt;Network architecture&lt;/h3&gt;
&lt;p&gt;Both flow computation and flow interpolation network adopt U-Net architecture. There are 6 hierarchies in the encoder, consisting of two convolutional and one Leaky ReLU (α=0.1) layers. At the end of each hierarchy except the last one, an average pooling layer with a stride of 2 is used to decrease the spatial dimension. There are 5 hierarchies in the decoder part. At the beginning of each hierarchy, a bilinear upsampling layer is used to increase the spatial dimension by a factor of 2, followed by two convolutional and Leaky ReLU layers.&lt;/p&gt;

&lt;p&gt;Emprical observations:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For flow estimation network it is crucial to have large filters in the first few layers of the encoder to capture long range motion&lt;/li&gt;
  &lt;li&gt;Concatenating the output of two encoder to the last decoder yields slightly better performance&lt;/li&gt;
  &lt;li&gt;It is slightly more advantageous to output optical flow residual rather than the refined optical flow&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;loss-function&quot;&gt;Loss Function&lt;/h3&gt;

&lt;p&gt;A combination of reconstruction loss, warping loss, perceptual loss (&lt;code class=&quot;highlighter-rouge&quot;&gt;conv4_3&lt;/code&gt; of VGG16), and smoothness loss&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;pa&quot;&gt;PhaseNet&lt;/h2&gt;
&lt;p&gt;Simone Meyer, Abdelaziz Djelouah, Brian McWilliams, Alexander Sorkine-Hornung, Markus Gross, Christopher Schroers, “&lt;em&gt;PhaseNet for Video Frame Interpolation&lt;/em&gt;,” CVPR-2018&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;da&quot;&gt;DepthAware&lt;/h2&gt;
&lt;p&gt;Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, Ming-Hsuan Yang, “&lt;em&gt;Depth-Aware Video Frame Interpolation&lt;/em&gt;,” CVPR-2019&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/frame_synthesis/DepthAware.png&quot; alt=&quot;Depth Aware Video Frame Interpolation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Motivation&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Most existing effort relies on large amount of training data and modeling capacity to implicitly infer the occlusion, which may not be effective to handle a wide variety of scenes in the wild.&lt;/li&gt;
  &lt;li&gt;The authors propose to explicitly detect occlusion by exploiting the depth information for video frame interpolation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;depth-aware-flow-projection&quot;&gt;Depth Aware Flow Projection&lt;/h3&gt;

&lt;p&gt;Similar to &lt;a href=&quot;#flowreversal&quot;&gt;flow reversal layer&lt;/a&gt; in &lt;a href=&quot;#quad&quot;&gt;Quadratic Video Interpolation&lt;/a&gt; paper, a flow reversal logic is introduced, with the difference that depth information is used to weigh the contribution from different displayments.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
f_{v\to u}(x) &amp;= \frac{
\sum_{y + f_{u \to v}(y) \in \mathcal{N}(x)} w\left(y\right) \left(-f_{u \to v}(y)\right)
}
{
\sum_{y + f_{u \to v}(y) \in \mathcal{N}(x)} w\left(y\right) 
}, \text{where}\\
w(y)&amp;=\frac{1}{\text{Depth of }y\text{ at frame $u$}}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;To fill in the holes of the projected flow, an &lt;em&gt;outside-in strategy&lt;/em&gt; is used&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
f_{v\to u}(x) =&amp; 1/\mathcal{N}(x)\sum_{y\in \mathcal{N}(x)} f_{v\to u}(x), \text{ where}\\
&amp; \mathcal{N}(x)\text{ is the 4 neighbors of }x.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;flow-estimation-network&quot;&gt;Flow Estimation Network&lt;/h3&gt;
&lt;p&gt;PWCNet with pre-trained weight&lt;/p&gt;

&lt;h3 id=&quot;depth-estimation-network&quot;&gt;Depth Estimation Network&lt;/h3&gt;
&lt;p&gt;Hourglass architecture, &lt;a href=&quot;https://arxiv.org/abs/1804.00607&quot;&gt;Megadepth&lt;/a&gt; model with pre-trained weight&lt;/p&gt;

&lt;h3 id=&quot;context-extraction-network&quot;&gt;Context Extraction Network&lt;/h3&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/frame_synthesis/DepthAware_contextextractionnetwork.png&quot; alt=&quot;DAIN context extraction network&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;kernel-estimation-network&quot;&gt;Kernel Estimation Network&lt;/h3&gt;
&lt;p&gt;U-Net architecture to estimate 4×4 local ker- nels for each pixel.&lt;/p&gt;

&lt;h3 id=&quot;adapative-warping&quot;&gt;Adapative Warping&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.08768&quot;&gt;Combining&lt;/a&gt; kernal based and flow based warping.&lt;/p&gt;

&lt;h3 id=&quot;frame-synthesis-network&quot;&gt;Frame Synthesis Network&lt;/h3&gt;
&lt;p&gt;Input: warped depth maps, warped contextual features, projected flows, and interpolation kernels.&lt;/p&gt;

&lt;h3 id=&quot;loss-function-1&quot;&gt;Loss Function&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\sum_{x}\rho\left(\hat{I}_t(x)-I_t(x)\right)\text{ where }\rho\text{ is the Charbonnier penalty function.}
\end{align*}&lt;/script&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;quad&quot;&gt;Quadratic&lt;/h2&gt;
&lt;p&gt;Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, Ming-Hsuan Yang, “&lt;em&gt;Quadratic video interpolation&lt;/em&gt;,” NeurIPS-2019&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/frame_synthesis/QuadraticVideoInterpolation.png&quot; alt=&quot;Quadratic Video Interpolation&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;quadratic-flow-prediction&quot;&gt;Quadratic Flow Prediction&lt;/h3&gt;

&lt;p&gt;Consider the motion model of $f_{0\to t}(x) = \int_0^t (v(x) + a(x) \tau)d\tau = v(x) t + \frac{1}{2}a(x) t^2$ where pixel $x$ at time $0$ is moving with instantaneous velocity of $v(x)$ and constant acceleration $a(x)$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
f_{0\to 1}(x) = v(x) + \frac{1}{2}a(x)\\
f_{0\to -1}(x) = -v(x) + \frac{1}{2}a(x).
\end{align*}&lt;/script&gt;

&lt;p&gt;Then $v_0$ and $a$ can be solved by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
v(x) = \frac{1}{2}\left(f_{0\to 1}(x) - f_{0\to -1}(x)\right)\\
a(x) = \left(f_{0\to 1}(x)(x) + f_{0\to -1}(x)\right).
\end{align*}&lt;/script&gt;

&lt;p&gt;So $f_{0\to t}$ can be expressed as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
f_{0\to t} = \frac{1}{2}\left(f_{0\to 1} - f_{0\to -1}\right) t + \frac{1}{2}\left(f_{0\to 1} + f_{0\to -1}\right) t^2,
\end{align*}&lt;/script&gt;

&lt;p&gt;which is in constrast to its expression with constant velocity assumption below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
f_{0\to t} = f_{0\to 1}t,
\end{align*}&lt;/script&gt;

&lt;p&gt;Since $f_{0\to -1}$ can be estimated by $I_0$ and $I_{-1}$, we know that utilizing an additional frame in the past $I_{-1}$ allows us to form a more accurate estimate of $f_{0\to t}$ with second order motion information. Similarly for $f_{1\to t}$ we can take advantage of an additional frame in the future and model it as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
f_{1\to t} = \frac{1}{2}\left(f_{1\to 0} - f_{1\to 2}\right) t + \frac{1}{2}\left(f_{1\to 0} + f_{1\to 2}\right) t^2,
\end{align*}&lt;/script&gt;

&lt;p&gt;In order to interpolate frame $I_t$ from $I_0$ and $I_1$, we need to have $f_{t\to 0}$ and $f_{t\to 1}$ for backward warping, but so far we only formulated the expression of $f_{0\to t}$ and $f_{1\to 0}$.&lt;/p&gt;

&lt;h3 id=&quot;flowreversal&quot;&gt;Flow Reversal/Projection&lt;/h3&gt;

&lt;p&gt;Estimating $f_{v\to u}$ from $f_{u\to v}$ is referred as flow reversal or flow projection. In this paper, the author proposes the following operation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
f_{v\to u}(x) = \frac{
\sum_{y + f_{u \to v}(y) \in \mathcal{N}(x)} w\left(\|y+f_{u \to v}(y) - x\|_2\right) \left(-f_{u \to v}(y)\right)
}
{
\sum_{y + f_{u \to v}(y) \in \mathcal{N}(x)} w\left(\|y+f_{u \to v}(y) - u\|_2\right) 
},
\end{align*}&lt;/script&gt;

&lt;p&gt;where $w(d) = e^{-d^2/\sigma^2}$ is the Gaussian weight for each flow. Note that there are $x$ that does not correspond to any $y+f_{u\to v}(y)$, leaving holes in $f_{v\to u}$, which corresponds to pixels that is visible in $v$ but occluded in $u$.&lt;/p&gt;

&lt;h3 id=&quot;frame-synthesis-step-1-adapative-flow-filter&quot;&gt;Frame Synthesis Step 1: Adapative Flow Filter&lt;/h3&gt;

&lt;p&gt;Adaptive flow filter network is a 23-layer U-Net where the encoder is a 12-layer conv net with 5 average pooling.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Input: a concantenation of $I_0$, $I_1$, $I^t_0$, $I^t_1$, $f_{0\to 1}$, $f_{1\to 0}, $f_{t\to 0}, and $f_{t\to 1}$. Here $I^t_0$ and $I^t_1$ are warped image from $I_0$ and $I_1$ using unfiltered flow.&lt;/li&gt;
  &lt;li&gt;Output: $\delta, r$ where $\delta \in [-k, k]^2$ with $k\text{tanh}$.&lt;/li&gt;
  &lt;li&gt;Flow Filtering:
&lt;script type=&quot;math/tex&quot;&gt;\begin{align*}
f'_{t\to 0 }(x) = f_{t\to 0}(x+\delta(x)) + r(x)
\end{align*}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Intuition: spatially-variant and nonlinear refinement of $f_{t\to 0}$, which could be seen as a learnable median filter in spirit.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;frame-synthesis-step-2-fusion-mask-prediction&quot;&gt;Frame Synthesis Step 2: Fusion Mask Prediction&lt;/h3&gt;

&lt;p&gt;Fusion mask prediction network is a three layer CNN&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Input: warped image using filtered flow&lt;/li&gt;
  &lt;li&gt;Output: mask $m$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;frame-synthesis-step-3-warping-and-fusion&quot;&gt;Frame Synthesis Step 3: Warping and Fusion&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\hat{I}_t(x) = 
\frac{
(1-t)m(x)I_0(x + f'_{t\to 0}(x)) + t(1-m(x))I_1(x + f'_{t\to 1}(x))
}
{
(1-t)m(x) + t(1-m(x))
}.
\end{align*}&lt;/script&gt;

&lt;h3 id=&quot;loss-function-2&quot;&gt;Loss Function&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\|\hat{I}_t-I_t\|_1+\lambda \|\phi(\hat{I}_t - \phi(I)_t)\|_2
\end{align*}&lt;/script&gt;

&lt;p&gt;where $\phi(\cdot)$ is extracted from &lt;code class=&quot;highlighter-rouge&quot;&gt;conv4_3&lt;/code&gt; feature extractor of the VGG16 model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;aim2019&quot;&gt;AIM2019&lt;/h2&gt;
&lt;p&gt;Andreas Lugmayr, Martin Danelljan, Radu Timofte, Manuel Fritsche, Shuhang Gu, Kuldeep Purohit, Praveen Kandula, Maitreya Suin, A N Rajagopalan, Nam Hyung Joon, Yu Seung Won, Guisik Kim, Dokyeong Kwon, Chih-Chung Hsu, Chia-Hsiang Lin, Yuanfei Huang, Xiaopeng Sun, Wen Lu, Jie Li, Xinbo Gao, Sefi Bell-Kligler, “&lt;em&gt;AIM 2019 Challenge on Real-World Image Super-Resolution: Methods and Results&lt;/em&gt;,” ICCVW-2019&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model&lt;/th&gt;
      &lt;th&gt;Time-specific&lt;/th&gt;
      &lt;th&gt;Extrapolation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ac&quot;&gt;AdaptConv&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Yes&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#asc&quot;&gt;AdaptSepConv&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#dvf&quot;&gt;DeepVoxFlow&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Yes?&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ssm&quot;&gt;SuperSloMo&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#pa&quot;&gt;PhaseNet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#da&quot;&gt;DepthAware&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#quad&quot;&gt;Quadratic&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
      &lt;td&gt;No&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>Yang Yang</name></author><category term="computer vision" /><summary type="html">Paper Organization Conference Arxiv Citation AdaptConv Portland SU CVPR-2017 2017-05-22 149 AdaptSepConv Portland SU ICCV-2017 2017-08-05 209 DeepVoxFlow CUHK UIUC PonyAI Google ICCV-2017 2017-08-05 279 SuperSloMo UMass NVIDIA UC-Merced CVPR-2018 2018-07-13 142 PhaseNet ETH Disney CVPR-2018 2018-04-03 47 DepthAware SJTU UC-Merced Google CVPR-2019 2019-04-01 35 Quadratic CMU SenseTime BNU UC-Merced NeurIPS-2019 2019-11-02 5 AIM2019 ICCVW-2019 ICCVW-2019 2019-11-19 22</summary></entry><entry><title type="html">Understanding and Implementing Asymmetric Numeral System (ANS)</title><link href="https://yyang768osu.github.io/posts/2020/06/asymmetric-numeral-system/" rel="alternate" type="text/html" title="Understanding and Implementing Asymmetric Numeral System (ANS)" /><published>2020-06-26T00:00:00-07:00</published><updated>2020-06-26T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2020/06/understanding-and-implementing-ans</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2020/06/asymmetric-numeral-system/">&lt;p&gt;Consider a alphabet with $N$ different symbols $\mathcal{A}=\{0, 1, \ldots, N-1\}. $Let us imagine an coding algorithm where a sequence of these symbols are coded into a sequence of bits, represented by an integer $s$, from which we can decode each symbol sequentially by breaking down $s$ into one symbol $x$ and another integer $s’$ capturing the remaining information. In such case, the encoding and decoding operation can be represented by the following &lt;code class=&quot;highlighter-rouge&quot;&gt;push&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;pop&lt;/code&gt; operation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{encode/push  }e:&amp; \mathbb{N}\times\mathcal{A} \to \mathbb{N}\\
\text{decode/pop   }d:&amp; \mathbb{N} \to \mathbb{N}\times\mathcal{A}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;There are two design goals for such a codec:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;validity: for valid encoding and decoding, we want to make sure that $e$ and $d$ are bijections and inverse of each other ($e=d^{-1}$).&lt;/li&gt;
  &lt;li&gt;efficiency: for coding efficiency, we want the final codword length to approximate the entropy of the data source&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let us focus on the decoding process $d$ and denote the mapping from an integer $s$ using $d$ as $d(s) = s’, x$ where $x\in\mathcal{A}$. To optimal performance we know that the codword lenght used to represent $x$ should be roughly $1/\log(P(x))$ where $P$ denote the probability mass function over the alphabet $\mathcal{A}$. In other words, we desire the number of bits in $s$ ($\log(s)$) to be $1/\log(P(x))$ more than that of $s’$. To make it precise, an efficient codec should satisfy the following&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\log(s) - \log(s') \approx \frac{1}{\log P(x)}\text{ for }d(s) = x, s'.
\end{align*}&lt;/script&gt;

&lt;p&gt;To express it another way&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\frac{s'}{s} = P(x)\text{ for }d(s) = x, s'.
\end{align*}&lt;/script&gt;

&lt;p&gt;The question now is: how can we design a bijective mapping from $\mathbb{N}$ and $\mathbb{N}\times\mathcal{A}$ satisfying the above goal? Here comes the core idea of asymmetric numerical system. Let us assume that we have access to a function that maps each of the number in $\mathbb{N}$ into one of the symbols in $\mathcal{A}$, denoted as $h:\mathbb{N}\to\mathcal{A}$ with the property that for any interger $s$ and symbol $x$, there are roughtly $P(x)\times s$ numbers below $s$ with label $x$, or more precisely put,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\frac{|\{n\in\mathbb{N}, n&lt;s, h(n) = x\}|}{s} \approx P(x) \text{ for any }s\in\mathbb{N}\text{ and }x\in\mathcal{A}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;With such a mapping $h$ available, we can define the bijective decoder mapping $d$ to be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
d(s)=&amp;s',x\text{ where} \\
s'=&amp;\left|\left\{n\in\mathbb{N},n&lt;s,h(n)=h(s)\right\}\right|,\\
x=&amp;h(s).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and it is easy to check that our two design goals are satisfied, and now we have shifted our task to finding such a labeling function $h$ such thatit leads to easy computation of $d$ and $e$.&lt;/p&gt;

&lt;h2 id=&quot;mapping-of-natural-numbers-to-symbols-h&quot;&gt;Mapping of natural numbers to symbols ($h$)&lt;/h2&gt;

&lt;p&gt;In r-ANS (range-ANS) design, the pmf $P:\mathcal{A}\to[0, 1]$ is quantized into integers $p(x)$ where&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\sum_{x\in\mathcal{A}}p(x)=2^r\\
&amp;p(x)/2^r\approx P(x). 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The mapping $h$ is design as the following: we divide the natural numbers into chunks with length $2^r$. Within each chunk, start with symbol $0\in\mathcal{A}$, we map the first $p(0)$ numbers to $0$; then the subsequent $p(1)$ numbers are mapped to $1$, so on and so forth.&lt;/p&gt;

&lt;p&gt;Let us define $c:\mathcal{A} \to \mathcal{N}$ with $c(x)=\sum_{a\in\mathcal{A}, a&amp;lt;x}p(a)$. Then the number from $c(x) to c(x)+p(x)$ within a length $2^r$ chunk is labeled as $x$.&lt;/p&gt;

&lt;p&gt;With this mapping, can express $d$ and $e$ into the following arithmetics that are easy to compute:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
d(s) =&amp; p(x)\times(s//2^r) + (s\text{ mod }2^r-c(x)), x\triangleq h(s) \\
e(s', x) =&amp; 2^r \times (s'//p(x)) + (s'\text{ mod }p(x) + c(x))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;with this comes the first implementation of ANS&lt;/p&gt;

&lt;h2 id=&quot;ans-without-rescaling-flawed-version&quot;&gt;ANS without rescaling (flawed version)&lt;/h2&gt;

&lt;h3 id=&quot;encoder&quot;&gt;Encoder&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ans_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;decoder&quot;&gt;Decoder&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ans_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# this loop can be improved by binary search&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running this encoder decoder pair through tests, one will realize that there is a small issue with the encoder and decoder function $e$ and $d$. Specifically, if both $s’$ and $x$ are $0$, then $s=e(0, 0)=0$. This means that any frontloaded $0$-sequence will be just be coded into $0$, and decoder has no ways of knowing how many $0$ symbols are there in the front of the sequence, if any! To solve this issue, we need to additionaly guarantee that $e$ results in strictly increasing integer. A fixed version is provided in the next section.&lt;/p&gt;

&lt;h2 id=&quot;ans-without-rescaling-correct-version&quot;&gt;ANS without rescaling (correct version)&lt;/h2&gt;

&lt;h3 id=&quot;encoder-1&quot;&gt;Encoder&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ans_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; ANS encoder (no rescaling)

    Parameters
    ----------
    symbols : list of int
        list of input symbols represented by index
        value should not be larger than len(p)
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r

    Warnings
    --------
    int type for all the input arguments should be python int type,
    which has aribitrary precision

    Returns
    -------
    s : integer representation of the encoded message

    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;decoder-1&quot;&gt;Decoder&lt;/h3&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ans_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; ANS encoder (no rescaling)

    Parameters
    ----------
    s : int
        integer representation of the encoded message
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r

    Warnings
    --------
    int type for all the input arguments should be python int type,
    which has arbitrary precision

    Returns
    -------
    decoded_symbols : list of int
        list of decoded symbols

    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# this loop can be improved by binary search&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;test&quot;&gt;Test&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# initialize data distribution and input length&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;106&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;150&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# randomly sample input&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# encode&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# decode&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# statistics&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;average_bps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# sanity check&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# display results&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;encoded integer        : {s}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;average bits per symbol: {average_bps:.5f} bits/symbol&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data source entropy    : {entropy:.5f} bits/symbol&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Test output&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;integer&lt;/span&gt;        &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;125621967822099623663819958660494947377946858741001513589&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;average&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.86357&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.79865&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;ans-with-rescaling&quot;&gt;ANS with rescaling&lt;/h2&gt;

&lt;p&gt;The above ANS implementation takes advantage of the fact that python integer has arbitrary precision, which allows us to encode a sequence that is arbitrarily long without overflowing. This poses a complexity issue: the encoding operation gets increasingly hard to compute as integer $s$ gets larger. Without resolving this reliance on infinite precision integer arithmetic, we cannot implement it using lower-level language with more hardware friendly instructions.&lt;/p&gt;

&lt;p&gt;One immediate idea is to limit the range of $s$, say with a maximum bit-width of $r_s$. Since the encoding process will necessary increase the valu of $s$, we then need to scale down its value before additional encoding, to a point where we can avoid overflow. In other words, before carrying out the encoding operation os $e(s’, x)$, $s’$ should satisfy&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;&amp;2^r\times (s'//p(x)) + (s'\text{ mod }p(x) + c(x)) &amp;&lt; 2^{r_s}\\
\Longleftrightarrow&amp;&amp; s'//p(x) + \underbrace{(s'\text{ mod }p(x) + c(x)) / 2^r}_{&lt;1}&amp;&lt; 2^{r_s-r}\\
\Longleftrightarrow&amp;&amp; s'//p(x) &amp;&lt; 2^{r_s-r}\\
\Longleftrightarrow&amp;&amp; s' &amp;&lt; (2^{r_s-r}+1)\times p(x)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;In the rescaling implemenetation, scaling down is achieved by extracting $r_t$ least significant bits, packing these bits into an integer $t$, and saving this integer to a stack $t_\text{stack}$, achieved through the following logic&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t_stack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we have guaranteed that encoder output will be an integer $s&amp;lt;2^{r_s}$ accompanied by a stack of integers $t&amp;lt;2^{r_t}$, the next question is, how to perform decoding? An easy answer would be to do the exact inverse of encoding, but to do that we need to know exactly when to perform up-scaling. The key is to realize that after the above loop is performed, it is guaranteed that $e(s’, x)$ is always larger than or equal to $2^{r_s-r_t}$ (assuming $r_t &amp;gt; r$), and thus during decoding we just need to upscale $s$ whenever it falls below $2^{r_s-r_t}$.&lt;/p&gt;

&lt;p&gt;After the while loop terminates, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;&amp; 2^{r_t} s' + (2^{r_t}-1) &amp;\geq (2^{r_s-r}+1) p(x)\\
\Longleftrightarrow &amp;&amp; 2^{r_t}s' + 2^{r_t} &amp;&gt; (2^{r_s-r}+1)p(x)\\
\Longleftrightarrow &amp;&amp; 2^{r_t}s'&amp;&gt; 2^{r_s-r}p(x) + p(x) - 2^{r_t}\\
\Longleftrightarrow &amp;&amp;        s'&amp;&gt; 2^{r_s-r-r_t}p(x) + \underbrace{p(x)/2^{r_t}}_{&lt;1} - 1\\
\Longleftrightarrow &amp;&amp;        s'&amp;\geq 2^{r_s-r-r_t}p(x)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Plugging in the above inequality to $e(s’, x)$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
e(s', x) \geq 2^{r_s -r_t}.
\end{align*}&lt;/script&gt;

&lt;p&gt;Now we are ready to implement ANS with rescaling.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ans_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; ANS encoder

    Parameters
    ----------
    symbols : list of int
        list of input symbols represented by index
        value should not be larger than len(p)
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r
    r_s : int
        bit-width precision of encoded integer s
    r_t : int
        bit-width precision of integers in stack t_stack

    Returns
    -------
    s : int
        s &amp;lt; 2 ** r_s
    t_stack : list of int
        each int &amp;lt; 2 ** r_t

    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t_stack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;t_stack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_stack&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ans_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; ANS encoder

    Parameters
    ----------
    s : int
        (s, t_stack) together represent the encoded message; s &amp;lt; 2 ** r_s
    t_stack : list of int
        (s, t_stack) together represent the encoded message; t &amp;lt; 2 ** r_t
    p : list of int
        quantized pmf of all input alphabet, sum(p) == 2 ** r
    c : list of int
        quantized cdf of all input alphabet, len(c) = len(p)
        c[0] = 0, and c[-1] = sum(p[:-1])
    r : int
        bit-width precision of the quantized pmf
        sum(p) == 2 ** r
    r_s : int
        bit-width precision of encoded integer s
    r_t : int
        bit-width precision of integers in stack t_stack

    Returns
    -------
    decoded_symbols : list of int
        list of decoded symbols

    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# this loop can be improved by binary search&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; \
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_stack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reversed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;## Test code&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# initialize data distribution and input length&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;106&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;150&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# randomly sample input&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# encode&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_stack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans_encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# decode&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# statistics&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;average_bps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t_stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_length&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# sanity check&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# display results&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;average bits per symbol: {average_bps:.5f} bits/symbol&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;data source entropy    : {entropy:.5f} bits/symbol&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;test-results&quot;&gt;Test results&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;average&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;per&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.80960&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entropy&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.79865&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Yang Yang</name></author><category term="compression" /><summary type="html">Consider a alphabet with $N$ different symbols $\mathcal{A}=\{0, 1, \ldots, N-1\}. $Let us imagine an coding algorithm where a sequence of these symbols are coded into a sequence of bits, represented by an integer $s$, from which we can decode each symbol sequentially by breaking down $s$ into one symbol $x$ and another integer $s’$ capturing the remaining information. In such case, the encoding and decoding operation can be represented by the following push and pop operation:</summary></entry><entry><title type="html">Understanding and Implementing Arithmetic Coding (AC)</title><link href="https://yyang768osu.github.io/posts/2020/06/arithmetic-coding/" rel="alternate" type="text/html" title="Understanding and Implementing Arithmetic Coding (AC)" /><published>2020-06-24T00:00:00-07:00</published><updated>2020-06-24T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2020/06/understanding-and-implementing-ac</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2020/06/arithmetic-coding/">&lt;h2 id=&quot;arithmetic-encoder-infinite-precision&quot;&gt;Arithmetic Encoder (infinite precision)&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 0&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# a &amp;lt; 1/2 and b &amp;gt; 1/2&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# a &amp;lt;= 1/4 or b &amp;gt;= 3/4&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 2a&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 2b&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;arithmetic-decoder-infinite-precision&quot;&gt;Arithmetic Decoder (infinite precision)&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bit_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bit&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;binary_block_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bit_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binary_block_size&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decode_one_symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binary_block_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decode_one_symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binary_block_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;decode_one_symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    Parameters
    ----------
    z_0: lower end of the current binary block
    z_1: higher end of the current binary block
    a: lower end of the current sub-interval
    b: higher end of the current sub-interval
    c: CDF starts with a 0.0
    d: CDF that ends with 1.0

    Returns
    -------
    if [z_0, z_1] is not contained in any of the symbols inside [a, b]:
        return None
    else:
        return the decoded index

    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_0&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;arithmetic-encoder-with-rescaling-infinite-precision&quot;&gt;Arithmetic Encoder with Rescaling (infinite precision)&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 1&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# a &amp;lt; 1/2 and b &amp;gt; 1/2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# a &amp;lt;= 1/4 or b &amp;gt;= 3/4&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 2a&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 2b&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;arithmetic-encoder-with-rescaling-finite-precision&quot;&gt;Arithmetic Encoder with Rescaling (finite precision)&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_precision&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;symbols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pmf_precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;symbol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pmf_precision&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 1&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# a &amp;lt; 1/2 and b &amp;gt; 1/2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_quarter&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_quarter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_quarter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_quarter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# a &amp;lt;= 1/4 or b &amp;gt;= 3/4&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_quarter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 2a&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# case 2b&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;arithmetic-decoder-with-rescaling-finite-precision&quot;&gt;Arithmetic Decoder with Rescaling (finite precision)&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;range_precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;next_bit_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;z_gap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_precision&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_precision&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pmf_precision&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pmf_precision&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z_gap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;low&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;decoded_symbols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_bit_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_bit_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;next_bit_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;z_gap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_bit_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_bit_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;next_bit_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;z_gap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_quarter&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_quarter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_quarter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_quarter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range_quarter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_bit_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_bit_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;next_bit_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;z_gap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Yang Yang</name></author><category term="compression" /><summary type="html">Arithmetic Encoder (infinite precision)</summary></entry><entry><title type="html">Variable Bitrate Method for End-to-End Lossy Compression</title><link href="https://yyang768osu.github.io/posts/2020/06/variable-bitrate/" rel="alternate" type="text/html" title="Variable Bitrate Method for End-to-End Lossy Compression" /><published>2020-06-23T00:00:00-07:00</published><updated>2020-06-23T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2020/06/variable-bit-rate-end-to-end-lossy-compression</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2020/06/variable-bitrate/">&lt;table id=&quot;summary&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Paper&lt;/th&gt;
      &lt;th&gt;Organization&lt;/th&gt;
      &lt;th&gt;Conference&lt;/th&gt;
      &lt;th&gt;Arxiv&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#vbr&quot;&gt;VariableBitrateRNN&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Google&lt;/td&gt;
      &lt;td&gt;ICLR-2016&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06085&quot;&gt;2016-05-01&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#psa&quot;&gt;PrimingSpatialAdapt&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Google&lt;/td&gt;
      &lt;td&gt;CVPR-2018&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.10114&quot;&gt;2017-05-29&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#cae&quot;&gt;ConditionlAE&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Samsung&lt;/td&gt;
      &lt;td&gt;ICCV-2019&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.04802&quot;&gt;2019-09-11&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ca&quot;&gt;ContentAdapt&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Huawei&lt;/td&gt;
      &lt;td&gt;CVPRW-2020&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://bit.ly/3etV3L0&quot;&gt;non-arxiv&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#dz&quot;&gt;DeadZone&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Fujitsu&lt;/td&gt;
      &lt;td&gt;CVPRW-2020&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.05855&quot;&gt;2020-04-26&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#qsf&quot;&gt;QualityScalingFactor&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;NanjingU&lt;/td&gt;
      &lt;td&gt;ICASSP-2020&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://bit.ly/2Z19UpD&quot;&gt;non-arxiv&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#bac&quot;&gt;BayesianAC&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;UCI&lt;/td&gt;
      &lt;td&gt;ICML-2020&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.08158&quot;&gt;2020-02-18&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#yoto&quot;&gt;YOTO&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Google Brain&lt;/td&gt;
      &lt;td&gt;ICLR-2020&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://openreview.net/forum?id=HyxY6JHKwr&quot;&gt;non-arxiv&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;vbr&quot;&gt;VariableBitrateRNN&lt;/h2&gt;
&lt;p&gt;George Toderici, Sean M. O’Malley, Sung Jin Hwang, Damien Vincent, David Minnen, Shumeet Baluja, Michele Covell, Rahul Sukthankar, “&lt;em&gt;Variable Rate Image Compression with Recurrent Neural Networks&lt;/em&gt;, “ ICLR-2016&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;psa&quot;&gt;PrimingSpatialAdapt&lt;/h2&gt;
&lt;p&gt;Nick Johnston, Damien Vincent, David Minnen, Michele Covell, Saurabh Singh, Troy Chinen, Sung Jin Hwang, Joel Shor, George Toderici, “&lt;em&gt;Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks,&lt;/em&gt;”, CVPR-2018&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;cae&quot;&gt;ConditionalAE&lt;/h2&gt;
&lt;p&gt;Yoojin Choi, Mostafa El-Khamy, Jungwon Lee, “&lt;em&gt;Variable Rate Deep Image Compression With a Conditional Autoencoder&lt;/em&gt;,” ICCV-2019&lt;/p&gt;

&lt;p&gt;Introduce two rate control parameters&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Lagrangian multiplier $\lambda$ (coarse rate adaptation)
    &lt;ul&gt;
      &lt;li&gt;$\lambda$ is chosen from a predefined &lt;em&gt;finite set&lt;/em&gt; of values.&lt;/li&gt;
      &lt;li&gt;It is injected to the network through conditional convolution block (for both encoder, decoder and prior model). During training $\lambda$ is randomly sampled&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Quantization bin size $\Delta$ (fine rate adaptation)
    &lt;ul&gt;
      &lt;li&gt;Only the quantizer and prior probability evaluation take $\Delta$ as input&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Remarks&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Main idea is to solve multiple objectives using one conditional network, instead of solving them individually using separate non-conditional networks.&lt;/li&gt;
  &lt;li&gt;Since both $\lambda$ and $\Delta$ are parameters to the decoding process, they need to be included as part of the bitstream and decoded first.&lt;/li&gt;
  &lt;li&gt;Universal quantization with dithering is applied instead of adding uniform noise to approximate quantization noise during training, and all latent are dithered with a single uniform random variable. Authors reported better performance with this dithering based universal quantization, but no intuition/reasoning is provided.&lt;/li&gt;
  &lt;li&gt;Single model performs on par with JointARHP and ContextAdapt in the &lt;a href=&quot;/posts/2020/06/e2e_lossy_image_compression/&quot;&gt;End-to-End image compression post&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/variable_bitrate/ConditionalAE_conditionalconvolution.png&quot; alt=&quot;Conditional convolution&quot; /&gt;&lt;/p&gt;
&lt;p&gt;$\lambda$-dependent channel-wise scaling and offset&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/variable_bitrate/ConditionalAE_plot.png&quot; alt=&quot;Performance&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/variable_bitrate/ConditionalAE_architecture.png&quot; alt=&quot;Network architecture&quot; /&gt;&lt;/p&gt;
&lt;p&gt;CConv stands for conditional convolution. Context model is used for both prior and hyper-prior.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ca&quot;&gt;ContentAdapt&lt;/h2&gt;
&lt;p&gt;Tiansheng Guo, Jing Wang, Ze Cui, Yihui Feng, Yunying Ge, Bo Bai, “&lt;em&gt;Variable Rate Image Compression with Content Adaptive Optimization&lt;/em&gt;,” CVPRW-2020&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/variable_bitrate/ContentAdapt.png&quot; alt=&quot;Network architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;dz&quot;&gt;DeadZone&lt;/h2&gt;
&lt;p&gt;Jing Zhou, Akira Nakagawa, Keizo Kato, Sihan Wen, Kimihiko Kazui, Zhiming Tan, “&lt;em&gt;Variable Rate Image Compression Method with Dead-zone Quantizer&lt;/em&gt;,” CVPRW-2020&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;qsf&quot;&gt;QualityScalingFactor&lt;/h2&gt;
&lt;p&gt;Tong Chen, Zhan Ma, “&lt;em&gt;Variable Bitrate Image Compression with Quality Scaling Factors&lt;/em&gt;”, ICASSP-2020&lt;/p&gt;

&lt;p&gt;Motivating experiment/observation&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First train a simple VAE network similiar to at a high bitrate, and retrained this network to get multiple models at different but lower bitrates.&lt;/li&gt;
  &lt;li&gt;An interesting observation is that similar but scaled patterns are retained for feature maps at a specific channel across a variety of bit rates, leading to a reasonable conclusion that we could apply the scaling after the identical convolutions without resorting to the complete retraining (see right figure below).&lt;/li&gt;
&lt;/ul&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/variable_bitrate/QualityScalingFactor.png&quot; alt=&quot;Quality Scaling Factor&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Idea/Remarks&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Finetune per-channel quantization steps for each R-D point. Encoder, decoder, prior model are fixed.&lt;/li&gt;
  &lt;li&gt;Per-channel quantization steps need to be transmitted to the decoder as well (not mentioned in the paper).&lt;/li&gt;
  &lt;li&gt;This scheme is similar to setting $\Delta$ in &lt;a href=&quot;#cae&quot;&gt;ConditionalAE&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;bac&quot;&gt;BayesianAC&lt;/h2&gt;
&lt;p&gt;Yibo Yang, Robert Bamler, Stephan Mandt, “&lt;em&gt;Variable-Bitrate Neural Compression via Bayesian Arithmetic Coding&lt;/em&gt;,” ICML-2020&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;yoto&quot;&gt;YOTO&lt;/h2&gt;
&lt;p&gt;Alexey Dosovitskiy, Josip Djolonga, “&lt;em&gt;You Only Train Once: Loss-Conditional Training of Deep Networks&lt;/em&gt;,” ICLR-2020&lt;/p&gt;

&lt;p&gt;Similar idea as &lt;a href=&quot;#cae&quot;&gt;ConditionalAE&lt;/a&gt; – $\lambda$ serves as conditional input to the convolutional layers (channel-wise scale and offset, obtained using a MLP). $\lambda$ is sampled from a log-normal distribution (not sure why).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="compression" /><summary type="html">Paper Organization Conference Arxiv VariableBitrateRNN Google ICLR-2016 2016-05-01 PrimingSpatialAdapt Google CVPR-2018 2017-05-29 ConditionlAE Samsung ICCV-2019 2019-09-11 ContentAdapt Huawei CVPRW-2020 non-arxiv DeadZone Fujitsu CVPRW-2020 2020-04-26 QualityScalingFactor NanjingU ICASSP-2020 non-arxiv BayesianAC UCI ICML-2020 2020-02-18 YOTO Google Brain ICLR-2020 non-arxiv</summary></entry><entry><title type="html">End-to-End Lossy Image Compression</title><link href="https://yyang768osu.github.io/posts/2020/06/e2e_lossy_image_compression/" rel="alternate" type="text/html" title="End-to-End Lossy Image Compression" /><published>2020-06-18T00:00:00-07:00</published><updated>2020-06-18T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2020/06/notes-on-end-to-end-lossy-image-compression</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2020/06/e2e_lossy_image_compression/">&lt;table id=&quot;summary&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Paper&lt;/th&gt;
      &lt;th&gt;Organization&lt;/th&gt;
      &lt;th&gt;Conference&lt;/th&gt;
      &lt;th&gt;Arxiv&lt;/th&gt;
      &lt;th&gt;Citation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#e2e&quot;&gt;End2End&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;NYU&lt;/td&gt;
      &lt;td&gt;ICLR-2017&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.01704&quot;&gt;2017-03-03&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;336&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#cpm&quot;&gt;CondProbMod&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;ETH&lt;/td&gt;
      &lt;td&gt;CVPR-2018&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.04260&quot;&gt;2019-06-04&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#hp&quot;&gt;Hyperprior&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;google&lt;/td&gt;
      &lt;td&gt;ICLR-2018&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.01436&quot;&gt;2018-05-01&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;181&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ms&quot;&gt;MultiScale&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;UTokyo&lt;/td&gt;
      &lt;td&gt;ACCV-2018&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1805.06386&quot;&gt;2018-05-16&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#jarhp&quot;&gt;JointARHP&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;google&lt;/td&gt;
      &lt;td&gt;NeurIPS-2018&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.02736&quot;&gt;2018-09-08&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;104&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#nlaic&quot;&gt;NLAIC&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;NanjingU&amp;amp;NYU&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.09757&quot;&gt;2019-04-22&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ca&quot;&gt;ContentAdapt&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Disney&lt;/td&gt;
      &lt;td&gt;CVPRW-2019&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.01223&quot;&gt;2019-06-05&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ce&quot;&gt;ComputeEff&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;google&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.08771&quot;&gt;2019-12-18&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ilvm&quot;&gt;IntegerLVM&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;google&lt;/td&gt;
      &lt;td&gt;ICLR-2019&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://bit.ly/30Q4fW2&quot;&gt;non-arxiv&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ctxa&quot;&gt;ContextAdapt&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;ETRI&lt;/td&gt;
      &lt;td&gt;ICLR-2019&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.10452&quot;&gt;2019-05-06&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;43&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#bm&quot;&gt;Benchmark&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;PKU&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.03711&quot;&gt;2020-02-19&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#att&quot;&gt;Attention&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Waseda&lt;/td&gt;
      &lt;td&gt;CVPR-2020&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.01568&quot;&gt;2020-03-30&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#car&quot;&gt;ChannelAR&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;google&lt;/td&gt;
      &lt;td&gt;ICIP-2020&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://bit.ly/2zOei2Q&quot;&gt;non-arxiv&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href=&quot;#ii&quot;&gt;ImproveInference&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;UCI&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.04240&quot;&gt;2020-06-09&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;e2e&quot;&gt;End2End&lt;/h2&gt;
&lt;p&gt;Johannes Ballé, Valero Laparra, Eero P. Simoncelli, “&lt;em&gt;End-to-end Optimized Image Compression&lt;/em&gt;,” ICLR-2018&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In traditional &lt;em&gt;transform coding&lt;/em&gt;, transform, quantizer, and entropy code are separately optimized.&lt;/li&gt;
  &lt;li&gt;This work propose a framework for end-to-end optimization of an image compression model with
    &lt;ol&gt;
      &lt;li&gt;GDN (Generalized divisive normalization) as nonlinear transform which is proven effective in Gaussianizing image density&lt;/li&gt;
      &lt;li&gt;Uniform scalar quantization, modeled by adding uniform noise for gradient computation&lt;/li&gt;
      &lt;li&gt;Continous prior distribution
        &lt;ul&gt;
          &lt;li&gt;With a continous latent pdf of $p_y$, the quantized latent pmf can be modelled with $p_y\circledast\text{uniform-pdf}$ evaluated at quantized points&lt;/li&gt;
          &lt;li&gt;$p_y\circledast\text{uniform-pdf}(y_i) = \text{CDF}_y(y_i+1/2) - \text{CDF}_y(y_i-1/2)$.&lt;/li&gt;
          &lt;li&gt;In this work, $p_y$ is modeled by a piecewise linear function (linear spline)&lt;/li&gt;
          &lt;li&gt;Entropy coding is achieved using CABAC with a tree context and exponential Golomb code (for value outside of the max/min range)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Connection to variational autoencoder (denote $p$ as the generative distribution, $q$ as the variational distribution)
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
D_\text{KL}(q_{y|x}||p_{y|x}) =&amp; \mathbb{E}_{y\sim q}\log q(y|x) - \mathbb{E}_{y\sim q}\log p(y|x)\notag\\
=&amp;\mathbb{E}_{y\sim q}\log q(y|x) - \mathbb{E}_{y\sim q}\log \frac{p(y)p(x|y)}{p(x)}\notag\\
=&amp;\underbrace{\mathbb{E}_{y\sim q}\log q(y|x)}_{\text{constant}} - \underbrace{\mathbb{E}_{y\sim q}\log p(y)}_{\text{rate term}} - \underbrace{\mathbb{E}_{y\sim q}\log p(x|y)}_{\text{distortion term}} + \underbrace{\log p(x)}_{\text{constant}}.
\end{align*} %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Provisional study shows that GDN/IGDN can be replaced by ReLU at the cost of substantially larger number of parameters/layers&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;cpm&quot;&gt;CondProbMod&lt;/h2&gt;
&lt;p&gt;Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, Luc Van Gool, “&lt;em&gt;Conditional Probability Models for Deep Image Compression&lt;/em&gt;,” CVPR-2018&lt;/p&gt;

&lt;h3 id=&quot;quantization-method&quot;&gt;Quantization method&lt;/h3&gt;
&lt;p&gt;For a set of centers $\{c_1, c_2, \ldots, c_L\}$
&lt;script type=&quot;math/tex&quot;&gt;\begin{align*}
\text{forward: }\hat{z_i} \triangleq Q(z_i) \triangleq \arg\min_j ||z_i-c_j||\text{ }\text{ }\text{ }
\text{backward: }\tilde{z_i} \triangleq \sum_{j=1}^L\frac{\exp(\sigma||z_i - c_j||)}{\sum_{l=1}^L \exp(\sigma ||z_i-c_l||)}c_j
\end{align*}&lt;/script&gt;&lt;/p&gt;
&lt;h3 id=&quot;entropy-model&quot;&gt;Entropy model&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;3D extension to PixelCNN with fully factorized latents.&lt;/li&gt;
  &lt;li&gt;They find it beneficial to learn an importance map to help the CNN attend to different re- gions of the image with different amounts of bits. While&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;training-methodology&quot;&gt;Training methodology&lt;/h3&gt;
&lt;p&gt;Iterate between the following two steps&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Take a gradient step for encoder, decoder, and quantizer w.r.t. loss of $\text{distortion} + \lambda\times\text{rate-on-masked-latent}$&lt;/li&gt;
  &lt;li&gt;Take a gradient step for entropy model w.r.t. loss of $\text{distortion} + \lambda\times\text{rate-on-all-latent}$
    &lt;ul&gt;
      &lt;li&gt;Use hinge loss on rate term&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/CondProbMod.png&quot; alt=&quot;Conditional Probablity Model for Deep Image Compression&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;hp&quot;&gt;Hyperprior&lt;/h2&gt;
&lt;p&gt;Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, Nick Johnston, “&lt;em&gt;Variational Image Compression With a Scale Hyperprior,&lt;/em&gt;” ICLR-2018&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/Hyperprior.png&quot; alt=&quot;HyperPrior&quot; /&gt;&lt;/p&gt;
&lt;p&gt;($N=128, M=192$ for 5 lower values; $N=192, M=320$ for 3 higher values)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Two interpretations of hyperprior/hyper-latent
    &lt;ul&gt;
      &lt;li&gt;hyper-latent can be viewed as side information that signal modifications to the entropy model so that it fits better to a specific data sample instead of using a fixed entropy model that is fit to an ensemble of data. The amount of side information can be small compared to the reduction in codelength achieved by matching entropy model more closely to a particular image.&lt;/li&gt;
      &lt;li&gt;hyper-prior can be viewed as hierarchical latent variable model where the distribution of latent $\hat{y}$ is model by another latent variable model
        &lt;ul&gt;
          &lt;li&gt;Small caveat: for the proposed scheme to truly be a hierarchical latent variable model, the input to the hyper-encoder should be $\hat{y}$, instead of $y$.
  $ &lt;a href=&quot;#ii&quot;&gt;ImprovedInference&lt;/a&gt; mentioned that changing the input to the hyper encoder from $y$ to $\hat{y}$ hurt performance (more than what bits-back can save for lossless compression of $\hat{y}$).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Probablistic modelling of latent $y$ and hyper-latent $z$
    &lt;ul&gt;
      &lt;li&gt;Distribution of latent $y$ is modelled by a Gaussian distribution, whose standard deviation (subsequently referred to as &lt;code class=&quot;highlighter-rouge&quot;&gt;scale&lt;/code&gt;) is given as the output of the hyper-decoder. Each element (across spatial and channel dimension) could have a different scale. The pmf of the quantized latent $\hat{y}$ is then &lt;script type=&quot;math/tex&quot;&gt;\text{CDF}_{\text{Gaussian}}\left(\frac{\hat{y}+1/2}{\text{scale}}\right) - \text{CDF}_{\text{Gaussian}}\left(\frac{\hat{y}-1/2}{\text{scale}}\right)&lt;/script&gt;.
        &lt;ul&gt;
          &lt;li&gt;Inactive latent will be $0$ accompanied by a super small scale (so that the pmf of $0$ is close to $1.0$)&lt;/li&gt;
          &lt;li&gt;Marginalized latent distribution is a Gaussian scale mixture (GSM)&lt;/li&gt;
          &lt;li&gt;Compared with &lt;a href=&quot;#e2e&quot;&gt;End2End&lt;/a&gt;, the entropy model is image-dependent and spatial adaptive&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Distribution of hyper-latent $z$ is modelled by a non-parametric, fully factorized, univariant density model. The density is defined by its CDF $f_K\circ f_{K-1} \circ \ldots \circ f_1$ with $f_{K}(x)=\text{sigmoid}(H_Kx+b_K)$ and $f_k(x)=H_kx+b_k + a_k\odot \text{tanh}(H_k x + b_k)$. $\{(H_k, b_k)\}_k$ are enforced to be positive using &lt;code class=&quot;highlighter-rouge&quot;&gt;softplus&lt;/code&gt; and $\{a_k\}_k$ are enforced to be larger than $-1$ using &lt;code class=&quot;highlighter-rouge&quot;&gt;tanh&lt;/code&gt; (to make sure that $c$ is a monotonic increasing function)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ms&quot;&gt;MultiScale&lt;/h2&gt;
&lt;p&gt;Ken Nakanishi, Shin-ichi Maeda, Takeru Miyato, Daisuke Okanohara, “&lt;em&gt;Neural Multi-scale Image Compression&lt;/em&gt;,” ACCV-2018&lt;/p&gt;

&lt;p&gt;Key ideas:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Multi-scale lossy autoencoder
    &lt;ul&gt;
      &lt;li&gt;Latent is formed by quantizing feature maps at different resolution/scale&lt;/li&gt;
      &lt;li&gt;Densest latent is factored out first; coarest scale is factored out the last&lt;/li&gt;
      &lt;li&gt;Multi-scale latent are unpooled to have the same resolution as the densest feature map and then concatenated&lt;/li&gt;
      &lt;li&gt;space-to-depth and depth-to-space are used instead of strided convolution and transposed convolution&lt;/li&gt;
      &lt;li&gt;Quantization scheme is the same as that in &lt;a href=&quot;#cpm&quot;&gt;CondProbMod&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Parallel multi-scale entropy model
    &lt;ul&gt;
      &lt;li&gt;Block based auto-regressive entropy model, where each block is defined as spatially subsampled latent (last figure below). The rationale is that spatial correlation between pixels decreases as the pixels are distant from each other.&lt;/li&gt;
      &lt;li&gt;This can be viewed as channel autoregressive prior model if there is a space-to-depth reshaping upfront.&lt;/li&gt;
      &lt;li&gt;Entropy model is separately optimized after training the auto-encoder (due to memory and computation constraint of training)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/MultiScale.png&quot; alt=&quot;MultiScale&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/MultiScale_network.png&quot; alt=&quot;MultiScale network architecture&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/MultiScale_prior.png&quot; alt=&quot;MultiScale prior architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;jarhp&quot;&gt;JointARHP&lt;/h2&gt;
&lt;p&gt;David Minnen, Johannes Ballé, George Toderici, “&lt;em&gt;Joint Autoregressive and Hierarchical Priors for Learned Image Compression&lt;/em&gt;,” NeurIPS-2018&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/JointARHP.png&quot; alt=&quot;Joint AutoRegressive and Hyperprior&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/JointARHP_network.png&quot; alt=&quot;Network architecture of Joint AutoRegressive and Hyperprior&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First learning based approach to outperform BPG on both PSNR and MS-SSIM.
    &lt;ul&gt;
      &lt;li&gt;Even the model trained with MSE outperform BPG on MS-SSIM&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Extend the GSM(Gaussian scale mixture)-based entropy model to GMM(Gaussian mixture model)-based one, by having hyper-decoder generate both scale and mean. This change itself leads to performance better than BPG.
    &lt;ul&gt;
      &lt;li&gt;Exchanging Gaussian distribution to Logistic distribution has almost no effect&lt;/li&gt;
      &lt;li&gt;Exchanging Gaussian distribution to Laplacian decreases performance more substantially&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Empirically, $\hat{z}$ comprises only a very small percentage of the total file size, so AR model is not applied to hyper latent.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Entropy Parameter&lt;/em&gt; network uses 1x1 convolution to avoid information leakage (prior of a latent depending on undecoded ones)&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/JointARHP_latent.png&quot; alt=&quot;Latent analysis&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Latents for the channel with the higest entropy&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;nlaic&quot;&gt;NLAIC&lt;/h2&gt;
&lt;p&gt;Haojie Liu, Tong Chen, Peiyao Guo, Qiu Shen, Xun Cao, Yao Wang, Zhan Ma, “&lt;em&gt;Non-local Attention Optimized Deep Image Compression&lt;/em&gt;, “ 2019-04-22 Arxiv&lt;/p&gt;

&lt;p&gt;Two contributions (on top of &lt;a href=&quot;#jarhp&quot;&gt;JointARHP&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Introduce a non-local attention module for analysis and synthesis transform&lt;/li&gt;
  &lt;li&gt;Adopt 3D masked convolution to exploit correlation across channels (&lt;a href=&quot;#jarhp&quot;&gt;JointARHP&lt;/a&gt; used 2D masked convolution)
    &lt;ul&gt;
      &lt;li&gt;This part is the same as &lt;a href=&quot;#cpm&quot;&gt;CondProbMod&lt;/a&gt; though&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is observed that the percentage of bits spent on hyper-latent $\hat{z}$ decreases as overall rate increases.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/NLAIC.png&quot; alt=&quot;NLAIC&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ca&quot;&gt;ContentAdapt&lt;/h2&gt;
&lt;p&gt;Joaquim Campos, Simon Meierhans, Abdelaziz Djelouah, Christopher Schroers, “&lt;em&gt;Content Adaptive Optimization for Neural Image Compression&lt;/em&gt;,” CVPRW-2019&lt;/p&gt;

&lt;p&gt;Key idea&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Optimize the latent representation &lt;em&gt;individually&lt;/em&gt; on a per image basis, during encoding process
    &lt;ul&gt;
      &lt;li&gt;The key benefit of the proposed solution lies in the ability to achieve an improved compression performance while the neural compression network and the predictive model are kept fixed and the computing time on the decoder side remains unchanged. This is particularly beneficial in situations such as streaming, where the encoding complexity is not the limiting factor when compared to the transmission and decoding.&lt;/li&gt;
      &lt;li&gt;Optimize for rate distortion metric on a single image over the latent&lt;/li&gt;
      &lt;li&gt;Results are obtained with 1500 training steps, which takes 5 mins per HD image on a Titan Xp GPU&lt;/li&gt;
      &lt;li&gt;About 0.5dB improvement on Tecnick dataset at the same bitrate&lt;/li&gt;
      &lt;li&gt;Experiments are based on &lt;a href=&quot;#jarhp&quot;&gt;JointARHP&lt;/a&gt; model and &lt;a href=&quot;#e2e&quot;&gt;End2End&lt;/a&gt; model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Compare with two other adaptation method that requires model update (1) Fine-tune the entire model (2) Fine-tune only the entropy model
    &lt;ul&gt;
      &lt;li&gt;Fine-tune on entropy model only leads to small gains&lt;/li&gt;
      &lt;li&gt;Per image latent adaptation sometimes can be as competitive as model fine-tuned on specific sequences.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that &lt;em&gt;content adaptive&lt;/em&gt; is different from &lt;em&gt;context adaptive&lt;/em&gt;, the latter describes an entropy model where the distribution of a latent depends on some previously decoded latents (i.e., auto-regressive entropy model).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ce&quot;&gt;ComputeEff&lt;/h2&gt;
&lt;p&gt;Nick Johnston, Elad Eban, Ariel Gordon, Johannes Ballé, “&lt;em&gt;Computationally Efficient Neural Image Compression&lt;/em&gt;,” 2019-12-18 Arxiv&lt;/p&gt;

&lt;p&gt;Two contributions:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Simplify generalized divisive normalization (GDN) module
    &lt;ul&gt;
      &lt;li&gt;GDN in full flexible form
&lt;script type=&quot;math/tex&quot;&gt;\begin{align*}
z_i  = x_i/{\big (}\beta_i + \sum_j \gamma_{ij}|x_j|^{\alpha_{ij}}{\big )}^{\epsilon_i}
\end{align*}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;GDN used in prior image compression works ($\alpha_{ij}=2$ and $\epsilon_i=0.5$)&lt;/li&gt;
      &lt;li&gt;More implementation friendly GDN with no performance loss ($\alpha_{ij}=1$ and $\epsilon_i=1$)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Apply automatic network optimization techniques to reduce the computation complexity of the mean-scale architecture (&lt;a href=&quot;#hp&quot;&gt;Hyperprior&lt;/a&gt; + mean estimate from hyper-decoder)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ilvm&quot;&gt;IntegerLVM&lt;/h2&gt;
&lt;p&gt;Johannes Ballé, Nick Johnston, David Minnen, “&lt;em&gt;Integer Networks for Data Compression with Latent-Variable Models&lt;/em&gt;,” ICLR-2019&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Range coding can fail catastrophically if the computation of prior differs slightly from between the transmitter and receiver, which is common scenario when floating point math is used and sender and receiver operates on different hardware or software platform, as &lt;em&gt;numerical round-off&lt;/em&gt; is platform dependent.&lt;/li&gt;
  &lt;li&gt;This work proposes to use integer arithmetic in prior modeling NNs by restricting all data types to be integral and all operations implementated using either basic arithmetic or lookup tables, in order to prevent computational non-determinism in computation of prior.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The operation at a single layer is abstracted as 
&lt;script type=&quot;math/tex&quot;&gt;\begin{align*}
v = (Hx + b)\oslash c\text{ and }
w = g(v)
\end{align*}&lt;/script&gt;
with $b, v, c$ share the same bitwidth (bitwidth of the accumulator) and $H, w$ share the same bitwidth. Some details on how these integer numbers are parametrized:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;H = \left[
\begin{array}{c}
Q(h_1'\times s(h_1'))\\
\cdots\\
Q(h_N'\times s(h_N'))\\
\end{array}
\right],
s(h') = \max\left\{0, \min\left\{\frac{2^{K-1}-1}{\max h_i}, \frac{-2^{K-1}}{\min h_i}, 1/\epsilon\right\}\right\}\\
&amp;b = Q(2^K b'), c=Q(2^K r(c'))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;For backward propagation, it is proposed to use straight through gradient for quantization operation $Q(\cdot)$, gradient of division for rounding division, and gradient of a scaled accumulative of a generalized Gaussian for ReLU.&lt;/p&gt;

&lt;p&gt;In NN based prior modeling, NN generates parameters of a certain distribution family. In case of &lt;a href=&quot;#hp&quot;&gt;Hyperprior&lt;/a&gt;, the distribution is zero-mean Gaussian and the parameter is the scale of the Gaussian. Here one only need to make sure the parameter generation part is deterministic. If the parametrized pmf itself cannot be computed deterministically (as is the case for Gaussian), one can precompute all possible values and express it as a look up table over discrete latent values and the discrete parameters.&lt;/p&gt;

&lt;p&gt;For the scale of Gaussian, the discretization is chosen to be logarithmic, which minimizes redundancy for a given number of quantiazation levels.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ctxa&quot;&gt;ContextAdapt&lt;/h2&gt;
&lt;p&gt;Jooyoung Lee, Seunghyun Cho, Seung-Kwon Beack, “&lt;em&gt;Context-adaptive Entropy Model for End-to-end Optimized Image Compression&lt;/em&gt;,” ICLR-2019&lt;/p&gt;

&lt;p&gt;Technical wise this paper is almost identical to &lt;a href=&quot;#jarhp&quot;&gt;JointARHP&lt;/a&gt; work. One difference is that latent $y$ is split into two parts, one modelled with proposed joint auto-regressive and hyper-prior model, the other with a simple entropy model. Please refer to &lt;a href=&quot;https://openreview.net/forum?id=HyxKIiAqYQ&quot;&gt;open review&lt;/a&gt; for more insight.&lt;/p&gt;

&lt;p&gt;Hyper-prior is referred to as &lt;em&gt;bit-consuming context&lt;/em&gt; and auto-regressive part is referred to as &lt;em&gt;bit-free context&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;bm&quot;&gt;Benchmark&lt;/h2&gt;
&lt;p&gt;Yueyu Hu, Wenhan Yang, Zhan Ma, Jiaying Liu, “&lt;em&gt;Learning End-to-End Lossy Image Compression: A Benchmark&lt;/em&gt;,” 2020-02-19 Arxiv&lt;/p&gt;

&lt;p&gt;Key contributions&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Provide a comprehensive overview of existing end-to-end learned image compression methods (up to &lt;a href=&quot;#ca&quot;&gt;ContextAdapt&lt;/a&gt;)
    &lt;ul&gt;
      &lt;li&gt;Summary of test results on Kodak, Tecnick, and CLIC across many different solutions&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Propose a method with one more hierarchy of latent on top of the hyperprior architecture and aggregate latents from all levels as input to the synthesis network&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/Benchmark.png&quot; alt=&quot;Benchmark&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/Benchmark_aggregation.png&quot; alt=&quot;Benchmark Aggregation&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/Benchmark_Kodak.png&quot; alt=&quot;Benchmark Kodak&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/Benchmark_Tecnick.png&quot; alt=&quot;Benchmark Tecnick&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/Benchmark_CLIC.png&quot; alt=&quot;Benchmark CLIC&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;att&quot;&gt;Attention&lt;/h2&gt;
&lt;p&gt;Zhengxue Cheng, Heming Sun, Masaru Takeuchi, Jiro Katto, “&lt;em&gt;Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules&lt;/em&gt;,” CVPR-2020&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/Attention.png&quot; alt=&quot;Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules&quot; /&gt;&lt;/p&gt;
&lt;p&gt;(a) &lt;a href=&quot;#e2e&quot;&gt;End2End&lt;/a&gt; (b) &lt;a href=&quot;#hp&quot;&gt;Hyperprior&lt;/a&gt; (c) &lt;a href=&quot;#jarhp&quot;&gt;JointARHP&lt;/a&gt;/&lt;a href=&quot;#ctxa&quot;&gt;ContextAdapt&lt;/a&gt; (d) &lt;a href=&quot;#att&quot;&gt;Attention&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This paper builds on top of &lt;a href=&quot;#jarhp&quot;&gt;JointARHP&lt;/a&gt; with the following key contributions&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Use discretized Gaussian mixture model to parametrize $p(\hat{y}|\hat{z})$
    &lt;ul&gt;
      &lt;li&gt;Ablation study shows no gain beyond $K=3$ ($K$ denotes the number of mixture components)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adopt a simplified attention module into the analysis and synthesis transform
    &lt;ul&gt;
      &lt;li&gt;No non-local block&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Make some architecture changes on top of &lt;a href=&quot;#jarhp&quot;&gt;JointARHP&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Use subpixel convolution (aka &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.pixelshuffle&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;depth_to_space&lt;/code&gt;) instead of transposed convolution&lt;/li&gt;
      &lt;li&gt;Use four 3x3 convolution with residual connection as replacement of the 5x5 convolution&lt;/li&gt;
      &lt;li&gt;Pad image height and width to a multiple of 64 using reflect padding before feeding into the learned network ($y$ is downsampled by 16, $z$ is additionally downsampled by 4)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It claimed to be the first work that achieves comparable performance with VVC&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;VTM 5.2 is used; RGB is first converted to 8-bit YUV444, and then fed into VTM. QP={22, 27, 32, 37, 42, 47}&lt;/li&gt;
&lt;/ul&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/Attention_network.png&quot; alt=&quot;Network architecture&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Detailed network structure&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/Attention_simplified_attention.png&quot; alt=&quot;Simplified attention module&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Simplified attention module (no non-local block; just multiplicative operation)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;car&quot;&gt;ChannelAR&lt;/h2&gt;
&lt;p&gt;David Minnen, Saurabh Singh, “&lt;em&gt;Channel-wise AutoregRessive Entropy Models for Learned Image Compression&lt;/em&gt;” ICIP-2020&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/ChannelAR.png&quot; alt=&quot;Channel-wise autoregressive entropy models for learned image compression&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Three contributions&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Channel Conditioning (CC)
    &lt;ul&gt;
      &lt;li&gt;Split latent tensor along the channel dimension and condition the entropy parameters for each slice on previously decoded slices.&lt;/li&gt;
      &lt;li&gt;In a model with $N$ slices, each slice contains $H\times W\times C/N$ values&lt;/li&gt;
      &lt;li&gt;Increasing the number of splits from 0 to 4 yields significant gains, while moving from 5 to 10 splits yields relatively little additional savings.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Latent Residual Prediction (LRP)
    &lt;ul&gt;
      &lt;li&gt;Unclear how it is done&lt;/li&gt;
      &lt;li&gt;LRP has almost no benefit for models that do not use channel-conditioning&lt;/li&gt;
      &lt;li&gt;Regardless of the number of CC splits, LRP slightly reduces RD performance at high bit rates. At low bit rates, the benefit of LRP increases with the number of CC splits.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rounding based training
    &lt;ul&gt;
      &lt;li&gt;The mixed approach uses the same uniform noise for learning entropy models but replaces the noisy tensor with a rounded one whenever the quantized tensor is passed to a synthesis transform.&lt;/li&gt;
      &lt;li&gt;The benefit is minimal at higher quality levels but becomes significant at lower bit rates.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Best model achieves an average BD rate saving of 13.9% over BPG and 6.7% over &lt;a href=&quot;#hp&quot;&gt;Hyperprior&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;ii&quot;&gt;ImproveInference&lt;/h2&gt;
&lt;p&gt;Yibo Yang, Robert Bamler, Stephan Mandt, “&lt;em&gt;Improving Inference for Neural Image Compression&lt;/em&gt;,” 2020-06-09 Arxiv&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/image_compression/ImproveInference.png&quot; alt=&quot;Improve Inference&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Propose an algorithm with the following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(a) A fixed generative model (decoder) with hyper-latent, latent, and output as $z$, $y$ and $x$ respectively&lt;/li&gt;
  &lt;li&gt;(b) Conventional amortized inference based encoding&lt;/li&gt;
  &lt;li&gt;(d) &lt;a href=&quot;#ca&quot;&gt;Content adapative&lt;/a&gt; procedure: both latent and hyper-latent are finetuned for a specific image&lt;/li&gt;
  &lt;li&gt;(e) Finetune $y$ with the following loss: assuming $z$ follows a Gaussian posterior (with parameter $\mu_z$ and $\sigma_z$ that are jointly finetuned) with rate of $y$ characterized by NELBO.
    &lt;ul&gt;
      &lt;li&gt;Can we stop here? no, because $\mu_z$ and $\sigma_z$ cannot be derived at the decoder side so NELBO cannot be fulfilled with bits-back&lt;/li&gt;
      &lt;li&gt;$\hat{y}$ is fixed now.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(f) With a fixed random seed, finetune $\mu_z$ and $\sigma_z$ to minimize NELBO with a fixed $\hat{y}$. This finetuning procedure is reproducible at the decoder side so that we can get back number of bits correponds to the entropy of discrete Gaussian with parameter $\mu_z$ and $\sigma_z$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;#summary&quot;&gt;go back&lt;/a&gt;&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="compression" /><summary type="html">Paper Organization Conference Arxiv Citation End2End NYU ICLR-2017 2017-03-03 336 CondProbMod ETH CVPR-2018 2019-06-04 128 Hyperprior google ICLR-2018 2018-05-01 181 MultiScale UTokyo ACCV-2018 2018-05-16 9 JointARHP google NeurIPS-2018 2018-09-08 104 NLAIC NanjingU&amp;amp;NYU - 2019-04-22 11 ContentAdapt Disney CVPRW-2019 2019-06-05 0 ComputeEff google - 2019-12-18 1 IntegerLVM google ICLR-2019 non-arxiv 2 ContextAdapt ETRI ICLR-2019 2019-05-06 43 Benchmark PKU - 2020-02-19 1 Attention Waseda CVPR-2020 2020-03-30 2 ChannelAR google ICIP-2020 non-arxiv 0 ImproveInference UCI - 2020-06-09 0</summary></entry><entry><title type="html">Notes on Optical Flow</title><link href="https://yyang768osu.github.io/posts/2020/06/optical_flow/" rel="alternate" type="text/html" title="Notes on Optical Flow" /><published>2020-06-16T00:00:00-07:00</published><updated>2020-06-16T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2020/06/notes-on-optical-flow</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2020/06/optical_flow/">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#definition-of-optical-flow&quot; id=&quot;markdown-toc-definition-of-optical-flow&quot;&gt;Definition of Optical Flow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#useful-resources&quot; id=&quot;markdown-toc-useful-resources&quot;&gt;Useful Resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#traditional-approach&quot; id=&quot;markdown-toc-traditional-approach&quot;&gt;Traditional Approach&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#brightness-constancy-assumption&quot; id=&quot;markdown-toc-brightness-constancy-assumption&quot;&gt;Brightness Constancy Assumption&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#small-motion-assumption&quot; id=&quot;markdown-toc-small-motion-assumption&quot;&gt;Small Motion Assumption&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#brightness-constancy-equation&quot; id=&quot;markdown-toc-brightness-constancy-equation&quot;&gt;Brightness Constancy Equation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#how-to-solve-brightness-constancy-equation&quot; id=&quot;markdown-toc-how-to-solve-brightness-constancy-equation&quot;&gt;How to solve Brightness Constancy Equation?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#formulation-of-horn-shunck-optical-flow&quot; id=&quot;markdown-toc-formulation-of-horn-shunck-optical-flow&quot;&gt;Formulation of Horn-Shunck Optical flow&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#discrete-optical-flow-estimation&quot; id=&quot;markdown-toc-discrete-optical-flow-estimation&quot;&gt;Discrete Optical Flow Estimation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dataset&quot; id=&quot;markdown-toc-dataset&quot;&gt;Dataset&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#middlebury-link-paper&quot; id=&quot;markdown-toc-middlebury-link-paper&quot;&gt;Middlebury (link, paper)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mpi-sintel-link-paper&quot; id=&quot;markdown-toc-mpi-sintel-link-paper&quot;&gt;MPI Sintel (link, paper)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#kitti-link-paper&quot; id=&quot;markdown-toc-kitti-link-paper&quot;&gt;KITTI (link, paper)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#flying-chairs-link-paper&quot; id=&quot;markdown-toc-flying-chairs-link-paper&quot;&gt;Flying Chairs (link, paper)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#flying-things-3d-link-paper&quot; id=&quot;markdown-toc-flying-things-3d-link-paper&quot;&gt;Flying Things 3D (link, paper)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#evaluation-metric&quot; id=&quot;markdown-toc-evaluation-metric&quot;&gt;Evaluation Metric&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#angular-error-ae&quot; id=&quot;markdown-toc-angular-error-ae&quot;&gt;Angular Error (AE)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#end-point-error-epe&quot; id=&quot;markdown-toc-end-point-error-epe&quot;&gt;End Point Error (EPE)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#end-to-end-regression-based-optical-flow-estimation&quot; id=&quot;markdown-toc-end-to-end-regression-based-optical-flow-estimation&quot;&gt;End-to-end regression based optical flow estimation&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#some-useful-concepts&quot; id=&quot;markdown-toc-some-useful-concepts&quot;&gt;Some useful concepts&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#overview-of-different-models&quot; id=&quot;markdown-toc-overview-of-different-models&quot;&gt;Overview of different models&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#flownet-iccv-2015-paper&quot; id=&quot;markdown-toc-flownet-iccv-2015-paper&quot;&gt;FlowNet (ICCV 2015) paper&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#flownet-20-cvpr-2017-paper&quot; id=&quot;markdown-toc-flownet-20-cvpr-2017-paper&quot;&gt;FlowNet 2.0 (CVPR 2017) paper&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#spynet-cvpr-2017-paper-code&quot; id=&quot;markdown-toc-spynet-cvpr-2017-paper-code&quot;&gt;SPyNet (CVPR 2017) paper code&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pwcnet-cvpr-2018-paper-code-video&quot; id=&quot;markdown-toc-pwcnet-cvpr-2018-paper-code-video&quot;&gt;PWCNet (CVPR 2018) paper code video&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#irr-pwcnet-cvpr-2019-paper&quot; id=&quot;markdown-toc-irr-pwcnet-cvpr-2019-paper&quot;&gt;IRR-PWCNet (CVPR 2019) paper&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pwcnet-fusion-wacv-2019-paper&quot; id=&quot;markdown-toc-pwcnet-fusion-wacv-2019-paper&quot;&gt;PWCNet Fusion (WACV 2019) paper&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#scopeflow-cvpr-2020-paper-code&quot; id=&quot;markdown-toc-scopeflow-cvpr-2020-paper-code&quot;&gt;ScopeFlow (CVPR 2020) paper code&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#maskflownet-cvpr-2020-paper-code&quot; id=&quot;markdown-toc-maskflownet-cvpr-2020-paper-code&quot;&gt;MaskFlownet (CVPR 2020) paper code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;definition-of-optical-flow&quot;&gt;Definition of Optical Flow&lt;/h2&gt;

&lt;p&gt;Distribution of apparent velocities of movement of brightness pattern in an image.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Where do we need it?
    &lt;ul&gt;
      &lt;li&gt;Action recognition&lt;/li&gt;
      &lt;li&gt;Motion segmentation&lt;/li&gt;
      &lt;li&gt;Video compression&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;useful-resources&quot;&gt;Useful Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;CMU Computer Vision 16-385
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~16385/s17/Slides/14.1_Brightness_Constancy.pdf&quot;&gt;Brightness Constancy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~16385/s17/Slides/14.2_OF__ConstantFlow.pdf&quot;&gt;Optical Flow : Constant Flow&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~16385/s15/lectures/Lecture21.pdf&quot;&gt;Optical Flow : Lucas-Kanade&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~16385/s17/Slides/14.3_OF__HornSchunck.pdf&quot;&gt;Optical Flow : Horn-Shunck&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CMU Computer Vision 16-720
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://16720.courses.cs.cmu.edu/lec/motion_lec12.pdf&quot;&gt;Motion and Flow&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://16720.courses.cs.cmu.edu/lec/flow.pdf&quot;&gt;Estimating Optical Flow 1&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://16720.courses.cs.cmu.edu/lec/flow_lec13.pdf&quot;&gt;Estimating Optical Flow 2&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Papers
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1504.06852&quot;&gt;FlowNet: Learning Optical Flow with Convolutional Networks (ICCV 2015)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.01925&quot;&gt;FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks (CVPR 2017)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.00850&quot;&gt;Optical Flow Estimation using a Spatial Pyramid Network (CVPR 2017)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.02853&quot;&gt;Optical Flow Estimation in the Deep Learning Age (2020/04/06)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1709.02371&quot;&gt;PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume (CVPR 2018)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.05290&quot;&gt;Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation (CVPR 2019)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.10066&quot;&gt;A fusion approach for multi-frame optical flow estimation (WACV 2019)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.10770&quot;&gt;ScopeFlow: Dynamic Scene Scoping for Optical Flow (CVPR 2020)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.10955&quot;&gt;MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask (CVPR 2020)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;traditional-approach&quot;&gt;Traditional Approach&lt;/h2&gt;

&lt;h3 id=&quot;brightness-constancy-assumption&quot;&gt;Brightness Constancy Assumption&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
I(x(t), y(t), t) = C
\end{align*}&lt;/script&gt;

&lt;h3 id=&quot;small-motion-assumption&quot;&gt;Small Motion Assumption&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;I(x(t+\delta t), y(t+\delta t), t+ \delta t) = I(x(t), y(t), t) + \frac{dI}{dt}\delta t, \text{(higher order term ignored)}\\
&amp;\text{where }\frac{dI}{dt} = \frac{\partial I}{\partial x}\frac{d x}{d t} + \frac{\partial I}{\partial y}\frac{d y}{d t} + \frac{\partial I}{\partial t}\triangleq I_x u + I_y v + I_t \triangleq \nabla^T I [u, v]^T + I_t
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\nabla I = [I_x, I_y]^T$ : spatial derivative&lt;/p&gt;

&lt;p&gt;$I_t$ : temporal derivative&lt;/p&gt;

&lt;p&gt;$[u, v]$ : optical flow velocities&lt;/p&gt;

&lt;h3 id=&quot;brightness-constancy-equation&quot;&gt;Brightness Constancy Equation&lt;/h3&gt;
&lt;p&gt;Combining the above two assumptions, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\nabla I [u, v]^T + I_t = 0.
\end{align*}&lt;/script&gt;

&lt;h3 id=&quot;how-to-solve-brightness-constancy-equation&quot;&gt;How to solve Brightness Constancy Equation?&lt;/h3&gt;
&lt;p&gt;Temporal derivative $I_t$ can be estimated by frame difference; spatial derivative $\nabla I$ can be estimated using spatial filters. Since there are two unknowns ($u$ and $v$), the system is underdetermined.&lt;/p&gt;

&lt;p&gt;Two ways to enforce additional constraints:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lucas-Kanade Optical Flow (1981) : assuming local patch has constant flow
    &lt;ul&gt;
      &lt;li&gt;LS can be applied to solve this overdetermined set of equations&lt;/li&gt;
      &lt;li&gt;If there is lack of sptial gradient in a local path, then the set of equations could still be underdetermined. This is referred to as the &lt;code class=&quot;highlighter-rouge&quot;&gt;aperture&lt;/code&gt; problem&lt;/li&gt;
      &lt;li&gt;If applied to only tractable patches, these are called sparse flow&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Horn-Schunck Optical Flow (1981) : assuming a smooth flow field&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;formulation-of-horn-shunck-optical-flow&quot;&gt;Formulation of Horn-Shunck Optical flow&lt;/h3&gt;

&lt;p&gt;Brightness constancy constraint/loss :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
E_d(i, j) = \left[I_x(i,j) u(i,j)+I_y(i,j)v(i,j) + I_t(i,j)\right]^2
\end{align*}&lt;/script&gt;

&lt;p&gt;Smoothness constraint/loss :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
E_s(i, j) = \frac{1}{4}\left[(u(i,j)-u(i+1,j))^2, (u(i,j)-u(i,j+1))^2, (v(i,j)-v(i+1,j)^2, (v(i,j)-v(i,j+1))^2\right]
\end{align*}&lt;/script&gt;

&lt;p&gt;Solving for optical flow :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\text{min}_{\bf{u}, \bf{v}} \sum_{i,j} E_d(i,j) + \lambda E_s(i,j)
\end{align*}&lt;/script&gt;

&lt;p&gt;Gradient descent can be used to solve the above optimization problem.&lt;/p&gt;

&lt;h3 id=&quot;discrete-optical-flow-estimation&quot;&gt;Discrete Optical Flow Estimation&lt;/h3&gt;

&lt;p&gt;Brightness Constancy Equation assumes small motion, which is in general not the case. If the movement is beyond 1 pixel, then higher order terms in the Taylor expansion of $I(x(t), y(t), t)$ could dominate. There are two solutions&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;To reduce the resolution using coarse-to-fine architecture&lt;/li&gt;
  &lt;li&gt;Resort to discrete optical flow estimation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For case-2, we obtain optical flow estimate by minimizing the following objective&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;E({\bf{z}}) = \sum_{i\in\mathcal{I}}D(z_i) + \sum_{(i,j)\in \mathcal{N}}S(z_i, z_j)\\
&amp;\text{where } z_i\triangleq (u_i, v_j), \mathcal{I}\triangleq\text{set of all pixels }, \mathcal{N}\triangleq\text{set of all neighboring pixels}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The above can be viewed as energy minimization in a Markov random field.&lt;/p&gt;

&lt;h2 id=&quot;dataset&quot;&gt;Dataset&lt;/h2&gt;

&lt;p&gt;Table 1 from &lt;a href=&quot;https://arxiv.org/pdf/1504.06852.pdf&quot;&gt;FlowNet&lt;/a&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Entry&lt;/th&gt;
      &lt;th&gt;Frame Pairs&lt;/th&gt;
      &lt;th&gt;Frames with ground truth&lt;/th&gt;
      &lt;th&gt;Ground-truth density per frame&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Middlebury&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;KITTI2012&lt;/td&gt;
      &lt;td&gt;194&lt;/td&gt;
      &lt;td&gt;194&lt;/td&gt;
      &lt;td&gt;50%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MPI Sintel&lt;/td&gt;
      &lt;td&gt;1041&lt;/td&gt;
      &lt;td&gt;1041&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Flying Chairs&lt;/td&gt;
      &lt;td&gt;22872&lt;/td&gt;
      &lt;td&gt;22972&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Flying Things 3D&lt;/td&gt;
      &lt;td&gt;22872&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;middlebury-link-paper&quot;&gt;Middlebury (&lt;a href=&quot;http://vision.middlebury.edu/flow/&quot;&gt;link&lt;/a&gt;, &lt;a href=&quot;http://vision.middlebury.edu/flow/floweval-ijcv2011.pdf&quot;&gt;paper&lt;/a&gt;)&lt;/h3&gt;
&lt;p&gt;Contains only 8 image pairs for training, with ground truth flows generated using four different techniques. Displacements are very small, typically below 10 pixels. (Section 4.1 in &lt;a href=&quot;https://arxiv.org/pdf/1504.06852.pdf&quot;&gt;FlowNet&lt;/a&gt;)&lt;/p&gt;

&lt;h3 id=&quot;mpi-sintel-link-paper&quot;&gt;MPI Sintel (&lt;a href=&quot;http://sintel.is.tue.mpg.de&quot;&gt;link&lt;/a&gt;, &lt;a href=&quot;http://files.is.tue.mpg.de/black/papers/ButlerECCV2012-corrected.pdf&quot;&gt;paper&lt;/a&gt;)&lt;/h3&gt;

&lt;p&gt;Computer-animated action movie. There are three render passes with varying degree of realism&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Albedo render pass&lt;/li&gt;
  &lt;li&gt;Clean pass (adds natural shading, cast shadows, specular reflections, and more complex lighting effects)&lt;/li&gt;
  &lt;li&gt;Final pass (adds motion blur,  focus blur, and atmospherical effect)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Contains 1064 training / 564 withheld test flow fields&lt;/p&gt;

&lt;h3 id=&quot;kitti-link-paper&quot;&gt;KITTI (&lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/&quot;&gt;link&lt;/a&gt;, &lt;a href=&quot;http://ww.cvlibs.net/publications/Geiger2013IJRR.pdf&quot;&gt;paper&lt;/a&gt;)&lt;/h3&gt;

&lt;p&gt;Contains 194 training image pairs and includes large displacements, but contains only a very special motion type. The ground truth is obtained from real world scenes by simultaneously recording the scenes with a camera and a 3D laser scanner. This assumes that the scene is rigid and that the motion stems from a moving observer. Moreover, motion of distant objects, such as the sky, cannot be captured, resulting in sparse optical flow ground truth.&lt;/p&gt;

&lt;h3 id=&quot;flying-chairs-link-paper&quot;&gt;Flying Chairs (&lt;a href=&quot;https://lmb.informatik.uni-freiburg.de/resources/datasets/FlyingChairs.en.html&quot;&gt;link&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1504.06852&quot;&gt;paper&lt;/a&gt;)&lt;/h3&gt;

&lt;p&gt;Contains about 22k image pairs of chairs superimposed on random background images from Flickr. Random affine transformations are applied to chairs and background to obtain the second image and ground truth flow fields. The dataset contains only planar motions. (Section 3 in &lt;a href=&quot;https://arxiv.org/abs/1612.01925&quot;&gt;FlowNet 2.0&lt;/a&gt;)&lt;/p&gt;

&lt;h3 id=&quot;flying-things-3d-link-paper&quot;&gt;Flying Things 3D (&lt;a href=&quot;https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html&quot;&gt;link&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/pdf/1512.02134.pdf&quot;&gt;paper&lt;/a&gt;)&lt;/h3&gt;

&lt;p&gt;A natural extension of the FlyingChairs dataset, having 22,872 larger 3D scenes with more complex motion patterns.&lt;/p&gt;

&lt;h2 id=&quot;evaluation-metric&quot;&gt;Evaluation Metric&lt;/h2&gt;

&lt;h3 id=&quot;angular-error-ae&quot;&gt;Angular Error (AE)&lt;/h3&gt;
&lt;p&gt;AE between $(u_0, v_0)$ and $(u_1, v_1)$ is the angle in 3D space between $(u_0, v_0, 1.0)$ and $(u_1, v_1, 1.0)$. Error in large flow is penalized less than errors in small flow. (Section 4.1 in &lt;a href=&quot;http://vision.middlebury.edu/flow/flowEval-iccv07.pdf&quot;&gt;link&lt;/a&gt;)&lt;/p&gt;

&lt;h3 id=&quot;end-point-error-epe&quot;&gt;End Point Error (EPE)&lt;/h3&gt;
&lt;p&gt;EPE between $(u_0, v_0)$ and $(u_1, v_1)$ is $\sqrt{(u_0-u_1)^2 + (v_0-v_1)^2}$ (Euclidean distance).&lt;/p&gt;

&lt;p&gt;For Sintel MPI, papers also often reports detailed breakdown of EPE for pixels with different distance to motion boundaries ($d_{0-10}$, $d_{10-60}$, $d_{60-140}$) and different velocities ($s_{0-10}$, $s_{10-40}$, $s_{40+}$).&lt;/p&gt;

&lt;h2 id=&quot;end-to-end-regression-based-optical-flow-estimation&quot;&gt;End-to-end regression based optical flow estimation&lt;/h2&gt;

&lt;h3 id=&quot;some-useful-concepts&quot;&gt;Some useful concepts&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Backward warping&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$I_1(\cdot, \cdot)\triangleq I(\cdot, \cdot, t=1), I_2(\cdot, \cdot)\triangleq I(\cdot, \cdot, t=2)$. Optical flow field $u, v$ satisfies $I_1(x, y) = I_2(x+u, y+v)$. In other words, $u,v$ tells us where each pixel in $I_1$ is coming from, compared with $I_2$, and given $u, v$, we know how to move around (warp) the pixels in $I_2$ to obtain $I_1$. Here $I_1$ is often referred to as the source image and $I_2$ the target image – flow vector is defined per source image. Specifically, we can define a &lt;code class=&quot;highlighter-rouge&quot;&gt;warp&lt;/code&gt; operation as below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;I_{\text{source}} = \texttt{warp}(I_\text{target}, f) \text{ where}\\
&amp;I_{\text{source}}(x, y) = I_\text{target}(x+u, y+v)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Compositivity of backward warping&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\texttt{warp}(I_\text{target}, f_a+f_b) = \texttt{warp}\left(\texttt{warp}(I_\text{target}, f_a), f_b\right)
\end{align*}&lt;/script&gt;

&lt;h3 id=&quot;overview-of-different-models&quot;&gt;Overview of different models&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Model Name&lt;/th&gt;
      &lt;th&gt;Num of parameters&lt;/th&gt;
      &lt;th&gt;inference speed&lt;/th&gt;
      &lt;th&gt;Training time&lt;/th&gt;
      &lt;th&gt;MPI Sintel final test EPE&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;FlowNetS&lt;/td&gt;
      &lt;td&gt;32M&lt;/td&gt;
      &lt;td&gt;87.72fps&lt;/td&gt;
      &lt;td&gt;4days&lt;/td&gt;
      &lt;td&gt;7.218&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FlowNetC&lt;/td&gt;
      &lt;td&gt;32M&lt;/td&gt;
      &lt;td&gt;46.10fps&lt;/td&gt;
      &lt;td&gt;6days&lt;/td&gt;
      &lt;td&gt;7.883&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FlowNet2.0&lt;/td&gt;
      &lt;td&gt;162M&lt;/td&gt;
      &lt;td&gt;11.79fps&lt;/td&gt;
      &lt;td&gt;14days&lt;/td&gt;
      &lt;td&gt;6.016&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SPyNet&lt;/td&gt;
      &lt;td&gt;1.2M&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;8.360&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PWCNet&lt;/td&gt;
      &lt;td&gt;8.7M&lt;/td&gt;
      &lt;td&gt;35.01fps&lt;/td&gt;
      &lt;td&gt;4.8days&lt;/td&gt;
      &lt;td&gt;5.042&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p class=&quot;notice&quot;&gt;The EPE column is taken from Table 2 of &lt;a href=&quot;https://arxiv.org/abs/2004.02853&quot;&gt;an overview paper&lt;/a&gt;. The inference speed (on Pascal Titan X) and training time column is taken from Table 7 of &lt;a href=&quot;https://arxiv.org/abs/1709.02371&quot;&gt;PWCNet paper&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;flownet-iccv-2015-paper&quot;&gt;FlowNet (ICCV 2015) &lt;a href=&quot;https://arxiv.org/abs/1504.06852&quot;&gt;paper&lt;/a&gt;&lt;/h3&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/optical_flow/FlowNet_encoder.png&quot; alt=&quot;FlowNet encoder&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/optical_flow/FlowNet_decoder.png&quot; alt=&quot;FlowNet decoder&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first end-to-end CNN architecture for estimating optical flow. Two variants:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;FlowNetS
    &lt;ul&gt;
      &lt;li&gt;A pair of input images is simply concatenated and then input to the U-shaped network that directly outputs optical flow.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;FlowNetC
    &lt;ul&gt;
      &lt;li&gt;FlowNetC has a shared encoder for both images, which extracts a feature map for each input image, and a cost volume is constructed by measuring patch-level similarity between the two feature maps with a correlation operation. The result is fed into the subsequent network layers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Multi-scale training loss is applied. Both models still underperform energy-based approaches.&lt;/p&gt;

&lt;h3 id=&quot;flownet-20-cvpr-2017-paper&quot;&gt;FlowNet 2.0 (CVPR 2017) &lt;a href=&quot;https://arxiv.org/abs/1612.01925&quot;&gt;paper&lt;/a&gt;&lt;/h3&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/optical_flow/FlowNet_2.png&quot; alt=&quot;FlowNet 2.0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Key ideas:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;By stacking multiple FlowNet style networks, one can sequentially refine the output from previous network modules.&lt;/li&gt;
  &lt;li&gt;It is helpful to pre-train networks on a less challenging synthetic dataset first and then further train on a more challenging synthetic dataset with 3D motion and photometric effects&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;End-to-end based approach starts to outperform energy-based ones.&lt;/p&gt;

&lt;h3 id=&quot;spynet-cvpr-2017-paper-code&quot;&gt;SPyNet (CVPR 2017) &lt;a href=&quot;https://arxiv.org/abs/1611.00850&quot;&gt;paper&lt;/a&gt; &lt;a href=&quot;https://github.com/anuragranj/spynet&quot;&gt;code&lt;/a&gt;&lt;/h3&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/optical_flow/SPyNet.png&quot; alt=&quot;SPyNet&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Key idea:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Incorporate classic &lt;code class=&quot;highlighter-rouge&quot;&gt;coarse-to-fine&lt;/code&gt; concepts into CNN network and update residual flow over mulitple pyramid levels (5 image pyramid levels are used). Networks at different levels have separate parameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Achieves comparable performance to FlowNet with 96% less number of parameters.&lt;/p&gt;

&lt;h3 id=&quot;pwcnet-cvpr-2018-paper-code-video&quot;&gt;PWCNet (CVPR 2018) &lt;a href=&quot;https://arxiv.org/abs/1709.02371&quot;&gt;paper&lt;/a&gt; &lt;a href=&quot;https://github.com/NVlabs/PWC-Net&quot;&gt;code&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=vVU8XV0Ac_0&quot;&gt;video&lt;/a&gt;&lt;/h3&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/optical_flow/PWCNet.png&quot; alt=&quot;PWCNet&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Key ideas:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Learned feature pyramid instead of image pyramid&lt;/li&gt;
  &lt;li&gt;Warping of feature maps&lt;/li&gt;
  &lt;li&gt;Computing a cost volume of learned feature maps (correlation)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Computation steps:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Feature pyramid extractor: conv-net with down-sampling&lt;/li&gt;
  &lt;li&gt;Target feature map is warped by upsampled previous flow estimation&lt;/li&gt;
  &lt;li&gt;Cost volume is computed based on source feature map and warped target feature map&lt;/li&gt;
  &lt;li&gt;Optical flow estimator: a DenseNet type of network that takes (1) source feature map (2) cost volume (3) upsampled previous optical flow estimate&lt;/li&gt;
  &lt;li&gt;Context network: a dilated convolution network to post process the estimated optical flow&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Remarks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Multi-scale training loss&lt;/li&gt;
  &lt;li&gt;Network at each scale estimates the optical flow for that scale, not the residual optical flow (the addition happens implictly inside the optical flow estimator).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;irr-pwcnet-cvpr-2019-paper&quot;&gt;IRR-PWCNet (CVPR 2019) &lt;a href=&quot;https://arxiv.org/abs/1904.05290&quot;&gt;paper&lt;/a&gt;&lt;/h3&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/optical_flow/IRR_PWCNet.png&quot; alt=&quot;IRR PWCNet&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Key ideas&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Take the output from a previous pass through the network as input and iteratively refine it by only using a single network block with shared weights, which allows the network to residually refine the previous estimate.&lt;/li&gt;
  &lt;li&gt;For PWCNet, the decoder module at different pyramid level is achieved using a 1x1 convolution before feeding the source feature map to the optical flow estimator/decoder.&lt;/li&gt;
  &lt;li&gt;Joint occlusion and bidirectional optical flow estimation leads to further performance enhancement.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pwcnet-fusion-wacv-2019-paper&quot;&gt;PWCNet Fusion (WACV 2019) &lt;a href=&quot;https://arxiv.org/abs/1810.10066&quot;&gt;paper&lt;/a&gt;&lt;/h3&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/optical_flow/PWCNet_Fusion.png&quot; alt=&quot;PWCNet Fusion&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The paper focuses on three-frame optical flow estimation problem: given $I_{t-1}$, $I_{t}$, and $I_{t+1}$, estimate $f_{t\to t+1}$.&lt;/p&gt;

&lt;p&gt;Key ideas:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If we are given $f_{t-1\to t}$ and $f_{t\to t-1}$, and assume constant velocity of movement, then an estimate of $f_{t\to t+1}$ can be formed by backward warping $f_{t-1\to t}$ with $f_{t\to t-1}$.
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\widehat{f}_{t\to t+1} \triangleq \texttt{warp}(f_{t-1 \to t}, f_{t\to t-1}), \\
&amp;\widehat{f}_{t\to t+1}(x, y) \triangleq f_{t-1 \to t}\left(x+f_{t\to t-1}(x,y)(x), y+f_{t\to t-1}(x,y)(y)\right)
\end{align*} %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;With three frames available, we can plug-in any two-frame optical flow estimation solution (PWCNet in this case) to obtain $f_{t-1 \to t}$, $f_{t\to t+1}$ and $f_{t \to t-1}$.&lt;/li&gt;
  &lt;li&gt;A fusion network (similar to the one used in the last stage of FlowNet 2.0) can be used to fuse together &lt;script type=&quot;math/tex&quot;&gt;\widehat{f}_{t \to t-1}\triangleq\texttt{warp}(f_{t-1 \to t}, f_{t\to t-1})&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;f_{t \to t+1}&lt;/script&gt;.
    &lt;ul&gt;
      &lt;li&gt;Note that &lt;script type=&quot;math/tex&quot;&gt;\widehat{f}_{t\to t-1}&lt;/script&gt; would be identical to &lt;script type=&quot;math/tex&quot;&gt;f_{t\to t+1}&lt;/script&gt; if (a) velocity is constant (b) three optical flow estimations are correct, and (c) there are no occlusions. Brightness constancy errors of the two flow maps together with the source frame $I_t$ are fed into the fusion network to provide additional info.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why multi-frame may perform better than 2-frame solutions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;temporal smoothness leads to additional regularization.&lt;/li&gt;
  &lt;li&gt;longer time sequences may help in ambiguous situations such as occluded regions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;scopeflow-cvpr-2020-paper-code&quot;&gt;ScopeFlow (CVPR 2020) &lt;a href=&quot;https://arxiv.org/abs/2002.10770&quot;&gt;paper&lt;/a&gt; &lt;a href=&quot;https://github.com/avirambh/ScopeFlow&quot;&gt;code&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;ScopeFlow revisits the following two parts in the conventional end-to-end training pipeline/protocol&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Data augmentation:
    &lt;ol&gt;
      &lt;li&gt;photometric transformations: input image perturbation, such as color and gamma corrections.&lt;/li&gt;
      &lt;li&gt;geometric augmentations: global or relative affine transformation, followed by random horizontal and vertical flipping.&lt;/li&gt;
      &lt;li&gt;cropping&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Regularization
    &lt;ul&gt;
      &lt;li&gt;weighted decay&lt;/li&gt;
      &lt;li&gt;adding random Gaussian noises&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and advocates&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;use larger scopes (crops and zoom-out) when possible.&lt;/li&gt;
  &lt;li&gt;gradually reduce regularization&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;maskflownet-cvpr-2020-paper-code&quot;&gt;MaskFlownet (CVPR 2020) &lt;a href=&quot;https://arxiv.org/abs/2003.10955&quot;&gt;paper&lt;/a&gt; &lt;a href=&quot;https://github.com/microsoft/MaskFlownet&quot;&gt;code&lt;/a&gt;&lt;/h3&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/optical_flow/AsymOFMM.png&quot; alt=&quot;Asymmetric Occlusion-aware Featuring Matching Module&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/optical_flow/MaskFlowNetS.png&quot; alt=&quot;MaskFlowNetS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Key idea:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Incorporates a learnable occlusion mask that filters occluded areas immediately after feature warping without any explicit supervision.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yang Yang</name></author><category term="computer vision" /><summary type="html"></summary></entry><entry><title type="html">Notes on Abstract Algebra</title><link href="https://yyang768osu.github.io/posts/2019/08/abstract_algebra/" rel="alternate" type="text/html" title="Notes on Abstract Algebra" /><published>2019-08-01T00:00:00-07:00</published><updated>2019-08-01T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2019/08/notes-on-abstract-algebra</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2019/08/abstract_algebra/">&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=VdLhQs_y_E8&quot;&gt;lecture 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Group $(G, \bullet)$ is a set $G$ equipped with a binary operation (often called a product operation) $\bullet$ that satisfies four conditions: closure, associativity, identity and invertibility, together called group axiom. The one that further satisfies commutativity is called Abelian group. One immediate property of group is that every element’s inverse is unique.&lt;/p&gt;

&lt;p&gt;One example is generalized linear group $G=GL_n(\mathbb{R})\subset M_n(\mathbb{R})$, with $\bullet$ being matrix multiplication. Note that there is no addition defined in $GL_n(\mathbb{R})$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
GL_n(\mathbb{R}) = \{A_{n\times n}: \det(A)\not=0\} = \{A_{n\times n}:\text{ there is an inverse matrix }A^{-1}\}
\end{align*}&lt;/script&gt;

&lt;p&gt;To prove that it is closed under matrix multiplication, we need to show that $A\bullet B$ is invertible if $A$ and $B$ are invertible. Two proofs: (1) $A\bullet B\bullet (B^{-1}\bullet A^{-1}) = I$ (2) $\det(A\bullet B)=\det{A}\det{B}$. Since matrix multiplication is not commutative, $GL_n(\mathbb{R})$ is non-Abelian.&lt;/p&gt;

&lt;p&gt;Another example is integer $\mathbb{Z}$ with $\bullet$ being integer addition. It is an Abelian group.&lt;/p&gt;

&lt;p&gt;Another example is vector space $V$ with vector addition. It is also an Abelian group.&lt;/p&gt;

&lt;p&gt;For any set $T$, the set of all bijections (one-to-one and onto, aka injection and surjection) also forms a group with $\bullet$ being composition of maps. This is THE most general group. ALL groups arise by putting extra conditioning on this invertible bijection.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
G=\left\{\text{all bijections } g:T\to T\right\} = \text{Sym}\{T\}
\end{align*}&lt;/script&gt;

&lt;p&gt;In particular, as a famous group, for a finite set $T={1,2,\ldots, n}$, we note the symmetry group of $T$, $\text{Sym}{T}$ as symmetry group $S_n$. This is a finite group of order $n!$ that contains all permutations of a set with $n$ elements, and is non-Abelian once $n\geq 3$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=e-a8auViFM0&amp;amp;list=PLelIK3uylPMGzHBuR3hLMHrYfMqWWsmx5&amp;amp;index=2&quot;&gt;lecture 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Auto-morphism is same as bijection. $\text{Sym}(T)$ is also noted $\text{AA}(T)$, with $AA$ being All Auto-morphism.&lt;/p&gt;

&lt;p&gt;A subgroup $H\subset G$ is a subset that is closed under $\bullet$, contains the identity, and contains the inverses. Two trivial subgroups $G$ and ${e}$.&lt;/p&gt;

&lt;p&gt;$GL_n(\mathbb{R}) \subset \text{Sym}(\mathbb{R}^n)$ are bijections in $\mathbb{R}^n$ that preserve linear structure.&lt;/p&gt;

&lt;p&gt;$S_n=\text{Sym}\{1,2,\ldots, n\}$ is called permutation group of size $n$. $|S_n| = n!$.&lt;/p&gt;

&lt;p&gt;$S_3 = \{e, \tau, \tau’, \tau’’, \sigma, \sigma’\}$. $\tau$ is called transposition and $\sigma$ is rotation. Since $S_3$ is non-Abelian, $S_n$ for $n&amp;gt;3$ is definitely non-Abelian given $S_3$ is a subgroup of $S_n$.&lt;/p&gt;

&lt;p&gt;Proposition: The subgroup of $(\mathbb{Z}, +)$ are precisely given by $(b\mathbb{Z}, +)$ where $b$ is a fixed integer.&lt;/p&gt;

&lt;p&gt;For any group $G$, and any element $g\in G$, there is a natural subgroup $H=\langle g\rangle$, called cyclic subgroup, generated by $g$, consists of all product of $g$ with itself. It is the smallest subgroup containing the element $g$. $\langle g\rangle=\{e, g, g^{-1}, g^2, g^{-2}, \ldots\}$, it is certainly closed under product and has inverse. Do not necessary think that those powers are distinct!&lt;/p&gt;

&lt;p&gt;If $g^m=e$ and $m$ is the smallest such power, we say $m$ is the order of $g\in G$. If no power $g^m=e$, we say that $g$ has infinite order.&lt;/p&gt;

&lt;p&gt;Big theorem: In a finite group, every element has finite order, and the order divides the order of the group. In infinite group, element can either has finite or infinite order.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=mwcNETa0KFI&amp;amp;list=PLelIK3uylPMGzHBuR3hLMHrYfMqWWsmx5&amp;amp;index=3&quot;&gt;lecture 3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Examples of groups: $GL_n(\mathbb{R})$, symmetric group on $n$ letters $S_n$, group of integer under addition.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;All groups arise as a structure-preserving bijection of something.&lt;/li&gt;
  &lt;li&gt;Making new groups out of old&lt;/li&gt;
  &lt;li&gt;Subgraph structure of a group&lt;/li&gt;
  &lt;li&gt;Cyclic subgraph&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Isomorphism&lt;/p&gt;

&lt;p&gt;Consider the following groups:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$G_1=\{1, -1, i, -i\}\subset \mathbb{C}^*\triangleq(\mathbb{C}\backslash \{0\}, \times)$.&lt;/li&gt;
  &lt;li&gt;$G_2$: cyclic subgroup of $S_4$ (special group on four letters) generated by $\rho$, which cycles $(1,2,3,4)$ to $(2,3,4,1)$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$G_1$ and $G_2$ have the same multiplication table, but with different labeling.&lt;/p&gt;

&lt;p&gt;Formalization&lt;/p&gt;

&lt;p&gt;Isomorphism is a map $f$ between two groups which is bijective $G_1\to G_2$ such that it preserves the multiplication operations $f(x\bullet_{G_1}y) = f(x)\bullet_{G_2}f(y)$. $f$ is telling us how to relabel things.&lt;/p&gt;

&lt;p&gt;Fact&lt;/p&gt;

&lt;p&gt;Every two cyclic groups of order $n$ are isomorphic.&lt;/p&gt;

&lt;p&gt;Definition: a cyclic group is a group generated by some element. $G_1$ and $G_2$ are isomorphic if there is an isomorphism $f: G_1\to G_2$ between them.&lt;/p&gt;

&lt;p&gt;$(\mathbb{R}, +)$ and $(\mathbb{R}_{&amp;gt;0}, \times)$ are isomorphic with the isomorphic $f(x)=exp(x)$. They are non-finite and non-cyclic.&lt;/p&gt;

&lt;p&gt;Example for finite groups that are not cyclic. Klein-4 group. Two ways to write it.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$G_1:\{e, \tau_1=((1,2)(3,4)), \tau_2=((1,3),(2,4)), \tau_1\tau_2=((1,4),(2,3))\}$.&lt;/li&gt;
  &lt;li&gt;$G_2: \{I, (-1,0; 0, 1), (1, 0; 0, -1), (-1,0;0;-1)\}$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Is Klein-4 group isomorphic to $\{1, -1, i, -i\}$. No, the former does not have elements of order 4 whereas $i$ and $-i$ are with order $4$.&lt;/p&gt;

&lt;p&gt;Some properties to test whether groups are isomorphic:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$|G_1|=|G_2|$&lt;/li&gt;
  &lt;li&gt;$G_1$ abelian $\Leftrightarrow$ $G_2$ abelian&lt;/li&gt;
  &lt;li&gt;$G_1$ and $G_2$ have the same number of elements of every order.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Yang Yang</name></author><category term="algebra" /><summary type="html">lecture 1</summary></entry><entry><title type="html">Markov Chain Monte Carlo: Gibbs, Metropolis-Hasting, and Hamiltonian</title><link href="https://yyang768osu.github.io/posts/2019/07/mcmc/" rel="alternate" type="text/html" title="Markov Chain Monte Carlo: Gibbs, Metropolis-Hasting, and Hamiltonian" /><published>2019-07-06T00:00:00-07:00</published><updated>2019-07-06T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2019/07/markov-chain-monte-carlo</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2019/07/mcmc/">&lt;p&gt;A fundamental problem in statistical learning is to compute the expectation with respect to some target probability distribution $\pi$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbb{E}_\pi\left[f\right] \triangleq \int \pi(x) f(x) dx.
\end{align*}&lt;/script&gt;

&lt;p&gt;There are two difficulties in the evaluation of the above (1) often $\pi(\cdot)$ is available to us only as a form of unnormalized probability, i.e., it can be evaluated only up to a normalizing constant (2) even if $\pi(\cdot)$ can be evaluated exactly, it is often hard to directly generate samples from it (e.g., for high-dimensional space).&lt;/p&gt;

&lt;p&gt;One example application is Bayesian inference, where the posterior probability of the latent $\pi(x|D)$ is available only in the form of prior $\pi(x)$ times likelihood $\pi(D|x)$ up to the unknown normalizing constant of $\pi(D)$, and we would like to either sample or obtain the expectation with respect to the posterior probability.&lt;/p&gt;

&lt;p&gt;The idea of Markov Chain Monte Carlo (MCMC) is to construct a Markov chain whose stationary distribution is exactly the target distribution with easy-to-sample transition kernels. One could then start with a random initial state, and yield samples by simply running the transitions and use the generated samples after the chain reaches steady state for the Monte Carlo evaluation of the expectation.&lt;/p&gt;

&lt;p&gt;For the design of such Markov chain, all methods that I encountered utilize the following theorem&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;An irreducible and aperiodic Markov chain with transition probability $P$ has stationary distribution of $\pi$ if it satisfies \begin{align} 
\pi(x)P(x’|x) = \pi(x’)P(x|x’) \notag
\end{align}&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The game, then, is to design $P$ for which the above equality holds. In this article, we will go through three MCMC methods with different ways in the design of $P$, namely &lt;strong&gt;Gibbs sampling&lt;/strong&gt;, &lt;strong&gt;Metropolis-Hastings&lt;/strong&gt;, and &lt;strong&gt;Hamiltonian Monte Carlo&lt;/strong&gt; (HMC).&lt;/p&gt;

&lt;p&gt;As a side note, it is worth pointing out that the above equation, referred to as &lt;em&gt;detailed balance equation&lt;/em&gt;, is a sufficient but not necessary condition for a Markov chain to have stationary distribution $\pi$. It defines a special case of Markov chain called reversible Markov chain. The detailed balance equation should be contrasted with &lt;em&gt;global balance equation&lt;/em&gt; below, which all Markov chains with stationary distribution $\pi$ satisfy. Then it shouldn’t be surprising that global balance equation can be easily derived from detailed balance equation (by summing over $x’$ on both sides of Equation (1)) but not the other way around.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\pi(x) = \sum_{x'} \pi(x')P(x'|x).
\end{align*}&lt;/script&gt;

&lt;h2 id=&quot;gibbs-sampling&quot;&gt;Gibbs sampling&lt;/h2&gt;

&lt;p&gt;In Gibbs sampling, the transition probability $P$ is defined as the following&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
P\left(x'|x\right)=\left\{
\begin{array}{ll}
\frac{1}{d}\pi\left(x'_j|x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_d\right) &amp; \text{if there exits }j\text{ such that }x_i'=x_i\text{ for }i\not=j.\\
0&amp;\text{otherwise.}
\end{array}
\right.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The state $x$ is a vector of dimension $d$, and the transition probability from state $x$ to state $x’$ is non-zero when they differ by only one dimension, say dimension $j$, and the transition probability is designed to be the conditional probability of $x’_j$, given all the other dimensions fixed, scaled by $1/d$. This corresponds to a transition scheme where we uniformly pick a dimension $j$, and then randomly sample a value in dimension $j$ following the conditional distribution. Detailed balance equation holds with such design&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\pi(x)P(x'|x)\\
=&amp;\frac{1}{d}\pi(x)\pi(x'_j|x_1, \ldots, x_{j-1}, x_{j+1}, \ldots, x_d)\\
=&amp; \frac{1}{d}\pi(x)\pi(x')/\sum_{z}\pi(x_1, \ldots, x_{j-1}, z, x_{j+1}, \ldots, x_d)\\
=&amp;\frac{1}{d}\pi(x)\pi(x')/\sum_{z}\pi(x'_1, \ldots, x'_{j-1}, z, x'_{j+1}, \ldots, x'_d)\\
=&amp;\frac{1}{d}\pi(x')\pi(x_j|x'_1, \ldots, x'_{j-1}, x'_{j+1}, \ldots, x'_d)\\
=&amp;\pi(x')P(x|x').
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The premise of Gibbs sampling is that the conditional distribution of one dimension given the rest is much easier to normalize and sample from. It is quite limited though, in the sense that the transition can never go very far in each step – only one dimension can be changed at a time. As a consequence, the transition matrix is quite sparse and the Markov chain may suffer from very large mixing time (time to stationary distribution) and it may not scale well with large dimensional space.&lt;/p&gt;

&lt;h2 id=&quot;metropolis-hastings&quot;&gt;Metropolis Hastings&lt;/h2&gt;

&lt;p&gt;Metropolis Hastings algorithm is a much more general version of Gibbs; in fact it encompasses both Gibbs sampling and Hamiltonian MC as special realizations. The basic idea is to construct the transition distribution from a flexible form of proposal distribution $g(x’|x)$, corrected by a &lt;em&gt;acceptance ratio&lt;/em&gt; term $A(x’,x)$  to guarantee reversibility in time. Specifically, the acceptance ratio is chosen to enforce the detailed balance equation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\pi(x) g(x'|x) A(x', x) = \pi(x') g(x|x') A(x, x').
\end{align*}&lt;/script&gt;

&lt;p&gt;The actual transition probability is then $P(x’|x) \triangleq g(x’|x) A(x’, x)$, corresponding to a sampling scheme where we first sample from $g(x’|x)$ to have a candidate next state $x’$, and then accept this candidate with probability $A(x’, x)$. If the candidate state is rejected, the next state will remain the same as the current state. For an arbitrary proposal distribution $g$, from the above equation, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\frac{A(x', x)}{A(x, x')} = \frac{\pi(x')g(x|x')}{\pi(x)g(x'|x)}.
\end{align*}&lt;/script&gt;

&lt;p&gt;To reduce the mixing time of the Markov chain, it is desirable to maximize the acceptance ratio $A$. This means that we want to set either $A(x’,x)$ or $A(x, x’)$ to be $1$ for any pair of $x$ and $x’$, resulting in the expression below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
A(x', x) = \min\left\{1, \frac{\pi(x')g(x|x')}{\pi(x)g(x'|x)}\right\}.
\end{align}&lt;/script&gt;

&lt;p&gt;In the above equation, since $\pi$ appear in both numerator and denominator, we can easily work with unnormalized probability distribution, as long as it can be evaluated efficiently for each data point.&lt;/p&gt;

&lt;p&gt;Metropolis-Hasting algorithm itself is just a MCMC framework; it still relies on a good choice of proposal distribution to perform well. The design of $g$ can be problem specific and is the &lt;em&gt;art&lt;/em&gt;. The clear optimal choice of is $g(x’|x)=\pi(x)$, which degenerates to the direct sampling of $\pi$ with acceptance ratio of $1$.&lt;/p&gt;

&lt;h2 id=&quot;hamiltonian-monte-carlo&quot;&gt;Hamiltonian Monte Carlo&lt;/h2&gt;

&lt;p&gt;Let’s now image a high dimensional surface for which the potential energy at each point $x$ is defined as $V(x)\triangleq -\log\pi(x)$. Here we introduce an auxiliary variable $p$ with the same dimension as $x$, and interpret the pair of variable $(x, p)$ as describing the position and momentum of an object on the high dimensional space.&lt;/p&gt;

&lt;p&gt;The kinetic energy of the object with mass $m$ and momentum $p$ is known as $K(p)=\frac{p^2}{2m}$ (e.g., $\frac{1}{2}mv^2 = (mv)^2/2m$). We now construct a joint probability distribution of $(x,p)$ as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\pi(x, p) = \frac{1}{Z}e^{-V(x)-K(p)} = \frac{1}{Z} e^{\log\pi(x)}e^{p^2/2m} = \frac{1}{Z}\pi(x)\mathcal{N}\left(p|0, \sqrt{m}\right).
\end{align*}&lt;/script&gt;

&lt;p&gt;Two remarks here: (1) The joint probability defined above is a function of the total energy $V(x) + K(p)$ (potential energy plus kinetic energy) of the imaginary object. (2) Since the marginal distribution of $\pi(x, p)$ with respect to $x$ is $\pi(x)$, if we can construct an effective MCMC algorithm for $(x, p)$, we then obtain an MCMC algorithm for $x$ by discarding $p$.&lt;/p&gt;

&lt;p&gt;The key in Hamiltonian MC is to use Hamiltonian mechanism as a way to obtain new candidate state (corresponding to proposal $g$ in Metropolis-Hastings).  Hamiltonian mechanics is an alternative reformation of the classic Newtonian mechanics describing Newton’s second law of motion. It characterizes the time evolution of the system in terms of location $x$ and momentum $p$, with the conservation of the sum of potential energy $V(x)$ and Kinetic energy of $K(p)$, a.k.a. Hamiltonian $\mathcal{H}(x, p) \triangleq V(x) + K(p)$, through the following differential equations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\frac{d p}{dt} =&amp; -\frac{\partial \mathcal{H}}{\partial x} &amp;\text{force equals to negative gradient of potential energy}\\
\frac{d x}{dt} =&amp; \frac{\partial \mathcal{H}}{\partial p} &amp;\text{velocity equals to derivative of kinetic energy w.r.t. momentum}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;By solving the path of $(x, p)$ according to Hamiltonian mechanics, we are essentially traversing along the contour for which $\pi (x, p)$ is fixed. This provide a very nice way of coming up with a proposal function $g(x’, p’| x, p)$ without having to reject any candidate. In other words, if we start with the point $(x, p)$ and derive the system state $(x_\tau, p_\tau)$ after a period of time $\tau$ , we then know that $\pi(x, p) = \pi(x_\tau, p_\tau)$. If we further apply a negation in the momentum, then the proposal function is reversible.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x, p \xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau \xrightarrow[]{\substack{\text{negate}\\\text{momentum}}} x_\tau, -p_\tau  = x', p'\\
x', p' \xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x'_\tau, p'_\tau \xrightarrow[]{\substack{\text{negate}\\\text{momentum}}}x'_\tau, -p'_\tau  = x, p
\end{align*}&lt;/script&gt;

&lt;p&gt;If we have perfect solver for the differential equation, then according to Equation (2) there is no need to reject any transition proposal. However, in reality the differential equation can only be solved in approximation with error, and thus $\pi(x, p)\not=\pi(x’, p’)$, meaning that the acceptance ratio is not strictly $1$ and certain fraction of the transition proposal would be rejected. It is worth noting that the method for computing the solution to the differential equation should still be reversible to respect the detailed balance equation. One hidden condition for such transition to be feasible is that the potential energy $V(\cdot)$ has to be differentiable, implying that the target distribution $\pi(\cdot)$ should be differentiable.&lt;/p&gt;

&lt;p&gt;So now we have defined a proposal function according to Hamiltonian mechanics, which leads to large acceptance ratio. Are we done here? Not yet. If we stop here, then the Markov chain we defined is reducible, i.e., not every state is accessible from an initial state. In fact, we only have pairwise transition in the Markov chain. To ensure the sampling of the entire space, another proposal distribution $g_2$ is introduced, taking advantage of the fact that $\pi(x, p)$ has factorized form for which $p$ follows a zero-mean normal distribution – the proposal distribution $g_2$ simply samples the momentum value $p$ from the corresponding marginal distribution. For such proposal, the corresponding acceptance ratio is $1$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
A((x, p'), (x, p)) = \min\left\{1, \frac{\pi(x, p')g_2(p|p')}{\pi(x, p)g_2(p'|p)}\right\} = \min\left\{1, \frac{\pi(x)}{\pi(x)}\right\}=1 .
\end{align*}&lt;/script&gt;

&lt;p&gt;Now we concatenate the above two proposals to have the final form of Hamiltonian MC sampling&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x, p_0 \xrightarrow[]{\substack{\text{resample}\\\text{momentum}}} x, p 
\xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau 
\xrightarrow[]{\substack{\text{negate}\\\text{momentum}}} x_\tau, -p_\tau  = x', p'.
\end{align*}&lt;/script&gt;

&lt;p&gt;Since every time after applying the Hamiltonian mechanics the momentum is resampled, we can ignore the momentum negation operation, leading to the following&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x \xrightarrow[]{\substack{\text{resample}\\\text{momentum}}} x, p 
\xrightarrow[]{\substack{\text{Hamiltonian}\\\text{mechanics}}} x_\tau, p_\tau 
\xrightarrow[]{\substack{\text{discard}\\\text{momentum}}} x_\tau = x',
\end{align*}&lt;/script&gt;

&lt;p&gt;and the corresponding acceptance ratio is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
A((x_\tau, p_\tau), (x, p)) = \min\left\{1, \frac{\pi(x_\tau, p_\tau)}{\pi(x, p)}\right\} =  \min\left\{1, e^{\mathcal{H}(x, p) - \mathcal{H}(x_\tau, p_\tau)}\right\}.
\end{align*}&lt;/script&gt;</content><author><name>Yang Yang</name></author><category term="Monte Carlo" /><summary type="html">A fundamental problem in statistical learning is to compute the expectation with respect to some target probability distribution $\pi$</summary></entry><entry><title type="html">Normalizing Flow I: understanding the change of variable equation</title><link href="https://yyang768osu.github.io/posts/2019/03/normalizing-flow-1/" rel="alternate" type="text/html" title="Normalizing Flow I: understanding the change of variable equation" /><published>2019-03-15T00:00:00-07:00</published><updated>2019-03-15T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2019/03/normalizing-flow-I-change-of-variable-equation</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2019/03/normalizing-flow-1/">&lt;p&gt;Normalizing flow is a technique for constructing complex probability distributions through invertible transformations of a simple distribution. It has been studied and applied in generative models under two contexts: (1) characterizing the approximation posterior distribution of latent variables in the case of variational inference (2) directly approximating the data distribution. When used in the second context, it has demonstrated its capability in generating high-fidelity audio, image, and video data.&lt;/p&gt;

&lt;p&gt;The study of generative models is all about learning a distribution &lt;script type=&quot;math/tex&quot;&gt;\mathbb{P}_{\mathcal{X}}&lt;/script&gt; that fits the data $\mathcal{X}$ well. With such distribution $\mathbb{P}(\mathcal{X})$ we can, among other things, generate, by sampling from  $\mathbb{P}_{\mathcal{X}}$, artificial data point that resembles $\mathcal{X}$. Since the true data distribution lies in high-dimensional space and is potentially very complex, it is essential to have a parameterized distribution family that is flexible and expressive enough to approximate the true data distribution well.&lt;/p&gt;

&lt;p&gt;The idea of flow-based methods is to &lt;em&gt;explicitly&lt;/em&gt; construct a parameterized family of distributions by transforming a known distribution &lt;script type=&quot;math/tex&quot;&gt;\mathbb{P}_{\mathcal{Z}}&lt;/script&gt;, e.g., a standard multi-variant Gaussian, through a concatenation of function mappings. Let’s consider the elementary case of a single function mapping $g$. For each sampled value $z$ from $\mathbb{P}_{\mathcal{Z}}$, we map it to a new value $x=g(z)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
z \xrightarrow{g(.)}  x
\end{align*}&lt;/script&gt;

&lt;p&gt;Up until this point, we have not introduced anything new. This way of transforming a known distribution using a function mapping $g$ is also adopted by generative adversarial networks (GAN). The question that flow-based method asks is: can we get a tractable probability density function (pdf) of $x=g(z)$? If so, we can &lt;em&gt;directly&lt;/em&gt; optimize the probability density of the dataset, i.e., the log likelihood of the data, rather than resorting to the duality approach adopted by GAN, or the lower-bound approach adopted by VAE.&lt;/p&gt;

&lt;p&gt;Unfortunately, for a general function $g$ that maps $z$ to $x$, the pdf of the new random variable $x=g(z)$ is quite complicated and usually intractable due to the need to calculate a multi-dimensional integral. However, if we restrict $g$ to be a bijective (one-to-one correspondence) and differentiable function, then the general change-of-variable technique reduces to the following tractable form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbb{P}_\mathcal{X}(x) =  \mathbb{P}_{\mathcal{Z}}(z)\left|\det \frac{d g(z)}{d z}\right|^{-1}, x=g(z)
\end{align*}&lt;/script&gt;

&lt;p&gt;An important consequence with the bijective assumption is that $z$ and $x$ must have the same dimension: if $z$ is a $d-$dimensional vector $z=[z_1, z_2, \ldots, z_d]$, the corresponds $x$ must also be a $d-$dimensional vector $x=[x_1, x_2, \ldots, x_d]$. It is worth emphasizing that the bijective assumption is essential to the tractability of the change-of-variable operation, and the resulting dimension invariance is a key restriction in flow-based methods.&lt;/p&gt;

&lt;p&gt;The above equation, albeit tractable, looks by no means familiar or friendly — what is with the absolute value? the determinant? the Jacobian? the inverse? The whole equation screams for an intuitive explanation. So here we go — let’s gain some insights into the meaning of the formula.&lt;/p&gt;

&lt;p&gt;First off, since $g$ is bijective and thus invertible, we can denote the inverse of $g$ as $f=g^{-1}$, which allows us to rewrite the equation as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbb{P}_\mathcal{X}(x) =  \mathbb{P}_{\mathcal{Z}}(f(x))\left|\det \frac{d x}{d f(x)}\right|^{-1} =  \mathbb{P}_{\mathcal{Z}}(f(x))\left|\det \frac{d f(x)}{d x}\right|
\end{align*}&lt;/script&gt;

&lt;p&gt;In the last equation, we get ride of the inverse by resorting to the identity that the determinant of an inverse is the inverse of the determinant, the intuition of which will become clear later.&lt;/p&gt;

&lt;p&gt;To understand the above equation, we start with a fundamental invariance in the change of probability random variables: &lt;strong&gt;the probability mass of the random variable $z$ in any subset of $\mathcal{Z}$ must be the same as the probability mass of $x$ in the corresponding subset of $\mathcal{X}$ induced by transformation from $z$ to $x$&lt;/strong&gt;, and vice versa.&lt;/p&gt;

&lt;p&gt;Let us exemplify the above statement with an example. Consider the case when $x$ and $z$ are 2 dimensional, and focus on a small rectangular in $\mathcal{X}$ defined by two corner points $(a, b)$ and $(a+\Delta x_1, b + \Delta x_2)$. If $\Delta x_1$ and $\Delta x_2$ are small enough, we can approximate the probability mass on the rectangular as the density $\mathbb{P}_\mathcal{X}$ evaluated at point $(a,b)$ times the area of the rectangular. More precisely,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;P {\big(}  (x_1, x_2) \in [a, a+\Delta x_1]\times[b, b+\Delta x_2] {\big)}\\
\approx&amp; \mathbb{P}_\mathcal{X} ((a, b)) \times \text{area of }[a, a+\Delta x_1]\times[b, b+\Delta x_2]\\
=&amp;\mathbb{P}_\mathcal{X} ((a, b)) \Delta x_1 \Delta x_2
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This approximation is basically assuming that the probabilistic density on the rectangular stays constant and equal to $\mathbb{P}_\mathcal{X} ((a, b))$, which holds asymptotically true as we shrink the width $\Delta x_1$ and height $\Delta x_2$ of the rectangular. The left figure below provides an illustration.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/flow.png&quot; alt=&quot;change of variable equation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now resorting to the aforementioned invariance, the probability mass on the $\Delta x_1 \times \Delta x_2$ rectangular must remain unchanged after the transformation. So what does the rectangular look like after the transformation of $f$? Let us focus on the corner point $(a+\Delta x_1, b)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
f((a+\Delta x_1, b))=&amp;(f_1(a+\Delta x_1,b), f_2(a+\Delta x_1,b))  \\
=&amp; (f_1(a,b), f_2(a,b))  \\
+&amp; \left(\frac{\partial f_1}{\partial x_1}(a, b)\Delta x_1 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_1\right) \text{ }\text{ first order component} \\
+&amp; \left(o(\Delta x_1), o(\Delta x_1)\right) \text{ }\text{ second and higher order residual}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;With $\Delta x_1$ and $\Delta x_2$ small enough, we can just ignore the higher order term and keep the linearized term. As can be seen from the figure above, the rectangular area is morphed into a parallelogram defined by the two vectors&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\left(\frac{\partial f_1}{\partial x_1}(a, b)\Delta x_1 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_1\right)\\
&amp;\left(\frac{\partial f_1}{\partial x_2}(a, b)\Delta x_2 ,   \frac{\partial f_2}{\partial x_1}(a, b)\Delta x_2\right)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;We have &lt;a href=&quot;https://textbooks.math.gatech.edu/ila/determinants-volumes.html&quot;&gt;geometry&lt;/a&gt; to tell us that the area of a parallelogram is just the absolute determinant of the matrix composed of the edge vectors, which is expressed as below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
{\Bigg|} \det \underbrace{\left[
\begin{array}{ll}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_1}\\
\frac{\partial f_1}{\partial x_2} &amp; \frac{\partial f_2}{\partial x_2}
\end{array}
\right]_{(a,b)}}_{\substack{\text{Jacobian of $f$}\\\text{evaluated at $(a,b)$}}}
{\Bigg |} 
\Delta x_1 \Delta x_2
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;By plugging in the above into the invariance statement, we reached the following identity&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbb{P}_\mathcal{X} ((a, b)) \Delta x_1 \Delta x_2 = \mathbb{P}_\mathcal{Z} (f(a, b)) \left|\det {\bf J}_f(a,b)\right| \Delta x_1 \Delta x_2
\end{align*}&lt;/script&gt;

&lt;p&gt;With $\Delta x_1\Delta x_2$ canceled out, we reached our target equation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathbb{P}_\mathcal{X} (x) = \mathbb{P}_\mathcal{Z} (f(x)) \left|\det {\bf J}_f(x)\right| = \mathbb{P}_\mathcal{Z} (f(x)) \left|\det \frac{\partial f(x)}{\partial x}\right|.
\end{align*}&lt;/script&gt;

&lt;p&gt;For data with dimension larger than two, the above equation still holds, with the distinctions that the parallelogram becomes a parallelepiped, and the concept of area becomes a more general notion of volume.&lt;/p&gt;

&lt;p&gt;It should be clear now what the physical interpretation is for the absolute-determinant-of-Jacobian — it represents the &lt;strong&gt;local, linearized rate of volume change&lt;/strong&gt; (quoted from &lt;a href=&quot;https://blog.evjang.com/2018/01/nf1.html&quot;&gt;this excellent blog&lt;/a&gt;) for the function transform. Why do we care about the rate of volume change? exactly because of the invariance of probability measure — in order to make sure each volume holds the same measure of probability before and after the transformation, we need to factor in the volume change induced by the transformation.&lt;/p&gt;

&lt;p&gt;With this interpretation that the absolute-determinant-of-Jacobian is just local linearized rate of volume change, it should not be surprising that the determinant of a Jacobian of an inverse function is the inverse of the determinant Jacobian of the original function. In other words, if function $f$ expands a volume around $x$ by rate of $r$, then the inverse function $g=f^{-1}$ must shrink a volume around $f(x)$ by the same rate of $r$.&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="generative model" /><summary type="html">Normalizing flow is a technique for constructing complex probability distributions through invertible transformations of a simple distribution. It has been studied and applied in generative models under two contexts: (1) characterizing the approximation posterior distribution of latent variables in the case of variational inference (2) directly approximating the data distribution. When used in the second context, it has demonstrated its capability in generating high-fidelity audio, image, and video data.</summary></entry><entry><title type="html">Understanding conventional HMM-based ASR training</title><link href="https://yyang768osu.github.io/posts/2018/11/conventional-hmm-asr-training/" rel="alternate" type="text/html" title="Understanding conventional HMM-based ASR training" /><published>2018-11-17T00:00:00-08:00</published><updated>2018-11-17T00:00:00-08:00</updated><id>https://yyang768osu.github.io/posts/2018/11/understanding-conventional-hmm-based-asr-training</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2018/11/conventional-hmm-asr-training/">&lt;p&gt;Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here $\theta$ denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/HMM.png&quot; alt=&quot;conventional HMM based ASR probabilistic assumption&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;maximum-likelihood-training&quot;&gt;Maximum likelihood training&lt;/h2&gt;

&lt;p&gt;In maximum likelihood estimation (MLE), as stated in the equation below, the objective is to maximize the likelihood of the data being generated by the generative model. In other words, we want to find the value of the parameters $\theta$ so that the above model best explains the acoustic features (e.g., spectrogram) that we observe.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\max_\theta \prod_{n=1}^N \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right)\\
=&amp;\arg\max_\theta \sum_{n=1}^N \log \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;For ease of notation, for the rest of this section, let’s ignore the conditioning on ${\bf y}^{(n)}$ (or ${\bf p}^{(n)}$). The difficulty in evaluating the above log-likelihood lies in the need to marginalize over all potential values of ${\bf z}^{(n)}$. This formulation falls right into the discussion of the previous two posts:  &lt;a href=&quot;/posts/2018/08/variational_inference_1/&quot;&gt;variational lower bound&lt;/a&gt; and &lt;a href=&quot;/posts/2018/08/variational_inference_1/&quot;&gt;expectation maximization&lt;/a&gt;, which provide an iterative algorithm to approach the solution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\theta^{[i+1]} = \arg\max_{\theta} \sum_{n=1}^N \int \color{red}{\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)}\log \mathbb{P}\left({\bf x}^{(n)}, {\bf z}^{(n)};\theta\right)d z^{(n)}.
\end{align}&lt;/script&gt;

&lt;p&gt;Most of the computation complexity in the above equation lies in finding the posterior probability of the latent state given the observed $\color{red}{\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta\right)}$. To elaborate on how the posterior probability is computed, let’s expand the acoustic model part in the previous figure as below, which is essentially a hidden-Markov chain.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/HMM2.png&quot; alt=&quot;conventional HMM based acoustic model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The inference problem (finding the posterior of the latent given the observed) in a hidden Markov chain can be solved by a forward-backward algorithm. The algorithm manifests itself as BCJR algorithm in convolutional code bit-level MAP decoding and &lt;a href=&quot;/posts/2018/08/kalman_filter_particle_filter/&quot;&gt;Kalman filtering&lt;/a&gt; in linear dynamic system.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\text{Forward path: }&amp;\text{calculate }\mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\text{ from }\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right) \notag \\
&amp;\mathbb{P}\left(z_t, {\bf x}_{1\to t};\theta\right) = \sum_{z_{t-1}} \mathbb{P}(z_{t}|z_{t-1};\theta)\mathbb{P}(x_{t}|z_{t};\theta)\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right)  \notag \\
\text{Backward path: }&amp;\text{calculate }\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right)\text{ from }\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
&amp;\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) = \sum_{z_{t+1}} \mathbb{P}(z_{t+1}|z_{t};\theta)\mathbb{P}(x_{t+1}|z_{t+1};\theta)\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
\text{Combined: }&amp;\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) \notag \\
&amp;\mathbb{P}\left(z_t| {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) / \sum_{z_t}\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;circular-dependency-between-segmentation-and-recognition&quot;&gt;Circular dependency between segmentation and recognition&lt;/h2&gt;
&lt;p&gt;The expectation-maximization formulation for likelihood maximization reveals a fundamental circular dependency between segmentation and recognition.&lt;/p&gt;

&lt;p&gt;Here &lt;strong&gt;segmentation&lt;/strong&gt; refers to the alignment of sub-phoneme states of ${\bf y}$ and the acoustic feature observations ${\bf x}$, encoded in the hidden state sequence ${\bf z}$, and &lt;strong&gt;recognition&lt;/strong&gt; refers to the classification of sub-phoneme hidden state sequence ${\bf z}$ for the corresponds acoustic feature observations ${\bf x}$.&lt;/p&gt;

&lt;p&gt;The two equations below make the circular dependency precise:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
\theta^{[i]} &amp;
\underset{\text{ }}{\xrightarrow{\substack{\text{update soft-alignment}\\\text{based on recognition}}}} \mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)\text{ using Equation (2)}\\
\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right) &amp;
\underset{\text{ }}{\xrightarrow{\substack{\text{update recognition}\\\text{based on soft-alignment}}}}
\theta^{[i+1]}\text{ using Equation (1)}
\end{align*} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;It is easy to argue that to have an accurate alignment, we need accurate recognition, and to train an accurate recognition, we have to rely on accurate alignment/segmentation.&lt;/p&gt;

&lt;p&gt;In a convention ASR system, to bootstrap the training procedure, we have to start with a dataset that has human curated phoneme boundary/segmentation. Once the system is capacitated with reasonable recognition/inference, it is no longer confined with human aligned dataset and a much larger dataset can be used with just waveform and the corresponding phoneme transcription. Eventually, after the system is able to deliver robust segmentation, we can make hard decision on the alignment, and only focus on improving the recognition performance with potentially a different system that has a much larger capacity, e.g., a DNN replacing the GMM model.&lt;/p&gt;

&lt;h2 id=&quot;decoding&quot;&gt;Decoding&lt;/h2&gt;
&lt;p&gt;In the decoding stage, we try to find the word/sentence with the maximum a posterior (MAP) probability given the observed data&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{=}&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) \\
=&amp;\arg\max_{\bf y} \underbrace{\mathbb{P}({\bf x}|{\bf p};\theta)}_{\text{acoustic model}}\times\underbrace{\mathbb{P}({\bf p}|{\bf y})}_{\text{lexion}}\times\underbrace{\mathbb{P}({\bf y})}_{\text{language model}}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The lexicon and language model together construct a state transition diagram, which we unrolled in time to form a decoding trellis. For each transcription hypothesis, a proper MAP decoder would sum across all the paths in the trellis that corresponds to the transcription, which is computationally prohibitive.&lt;/p&gt;

&lt;p&gt;One simplification one can make is to find the most probable path by running the Viterbi algorithm. However, even for Viterbi algorithm, the complexity is still too high for practical deployment due to the large state space and potentially large number of time steps.&lt;/p&gt;

&lt;p&gt;To further reduce the computation complexity, the conventional system resorts to the beam-search algorithm – basically a breath-first-search algorithm on the trellis that maintain only a limited number of candidates. The beam-search algorithm is often run on a weighted finite state transducer that captures the concatenation of language model and lexicon.&lt;/p&gt;

&lt;h2 id=&quot;discrepancy-between-mle-training-and-map-decoding&quot;&gt;Discrepancy between MLE training and MAP decoding&lt;/h2&gt;

&lt;p&gt;At first glance into the MAP decoding equation, it may appear that the MLE based training is well-aligned with the decoding process: maximizing the posterior probability of the transcription ${\bf y}$ given the acoustic feature ${\bf x}$ is equivalent to maximizing the likelihood of the observation ${\bf x}$. The argument being that the probability of the observation $\mathbb{P}(x;\theta)$ is anyway a constant dictated by the natural of people’s speech, not something we can control. But is it true?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\text{inference time:}\\
&amp;\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;It turns out there is a subtle difference between inference time (MAP decoding) and training time (MLE parameter update) that render the above statement wrong.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\text{training time:}\\
&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{\not=}&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;As is evident by comparing the above two equations, when we try to update parameter $\theta$ to maximize directly the posterior probability of the transcription ${\bf y}$ given the acoustic feature ${\bf x}$, we can no longer ignore the term $\mathbb{P}({\bf x};\theta)$. The key is to realize that we model the speech as a generative model, where &lt;strong&gt;the probability of observing a certain acoustic features ${\bf x}$ is not dictated by the nature, but rather the generative model that we assume&lt;/strong&gt;. By updating the parameter $\theta$ that best increase the likelihood, we inevitably change $\mathbb{P}({\bf x};\theta)$ too, and thus there is no guarantee that the posterior probability is increased. $\mathbb{P}({\bf x};\theta)$ is calculated by marginalizing over all potential transcriptions: $\mathbb{P}({\bf x};\theta)=\sum_{\bf y}\mathbb{P}({\bf x}|{\bf y};\theta)$.&lt;/p&gt;

&lt;p&gt;To elaborate, in MLE, we try to maximize $\color{red}{\mathbb{P}({\bf y}|{\bf x};\theta)}$ with respect to $\theta$, we may very well also increased the likelihood for competing transcription sequences $\color{blue}{\mathbb{P}({\bf x}|{\bf \tilde{y}};\theta)}$, potentially resulting in decreased posterior probability $\mathbb{P}({\bf y}|{\bf x};\theta)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp; \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&amp; \frac{\color{red}{\mathbb{P}({\bf x}|{\bf y};\theta)}\mathbb{P}({\bf y})}{\sum_{\bf \tilde{y}}\color{blue}{\mathbb{P}({\bf x}|{\bf \tilde{y}};\theta)}\mathbb{P}({\bf \tilde{y}})}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Fundamentally, &lt;strong&gt;the misalignment is rooted from the fact that we are using a generative model for discriminative tasks&lt;/strong&gt;. In the next section, we discuss several ways to train the generative model for better discrimination.&lt;/p&gt;

&lt;h2 id=&quot;sequence-discriminative-training&quot;&gt;Sequence discriminative training&lt;/h2&gt;
&lt;p&gt;To bridge the aforementioned discrepancy, several other training targets are proposed.&lt;/p&gt;

&lt;p&gt;to be continued…&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="ASR" /><category term="HMM" /><summary type="html">Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here $\theta$ denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.</summary></entry></feed>