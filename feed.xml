<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="https://yyang768osu.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://yyang768osu.github.io/" rel="alternate" type="text/html" /><updated>2019-02-11T22:45:28-08:00</updated><id>https://yyang768osu.github.io/</id><title type="html">Yang Yang</title><subtitle>Engineer at Qualcomm</subtitle><author><name>Yang Yang</name></author><entry><title type="html">Understanding conventional HMM-based ASR training</title><link href="https://yyang768osu.github.io/posts/2012/11/conventional-hmm-asr-training/" rel="alternate" type="text/html" title="Understanding conventional HMM-based ASR training" /><published>2018-11-17T00:00:00-08:00</published><updated>2018-11-17T00:00:00-08:00</updated><id>https://yyang768osu.github.io/posts/2012/11/understanding-conventional-hmm-based-asr-training</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/11/conventional-hmm-asr-training/">&lt;p&gt;Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here $\theta$ denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/HMM.png&quot; alt=&quot;conventional HMM based ASR probabilistic assumption&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;maximum-likelihood-training&quot;&gt;Maximum likelihood training&lt;/h2&gt;

&lt;p&gt;In maximum likelihood estimation (MLE), as stated in the equation below, the objective is to maximize the likelihood of the data being generated by the generative model. In other words, we want to find the value of the parameters $\theta$ so that the above model best explains the acoustic feature values that we observe.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\max_\theta \prod_{n=1}^N \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right)\\
=&amp;\arg\max_\theta \sum_{n=1}^N \log \mathbb{P}\left({\bf x}^{(n)}|{\bf y}^{(n)};\theta\right).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;For ease of notation, for the rest of this section, let’s ignore the conditioning on ${\bf y}^{(n)}$ (or ${\bf p}^{(n)}$). The above formulation falls right into the discussion of the previous two posts:  &lt;a href=&quot;/posts/2012/08/variational_inference_1/&quot;&gt;variational lower bound&lt;/a&gt; and &lt;a href=&quot;/posts/2012/08/variational_inference_1/&quot;&gt;expectation maximization&lt;/a&gt;, which provide an iterative algorithm to approach the solution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\theta^{[i+1]} = \arg\max_{\theta} \sum_{n=1}^N \int \color{red}{\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)}\log \mathbb{P}\left({\bf x}^{(n)}, {\bf z}^{(n)};\theta\right)d z^{(n)}.
\end{align}&lt;/script&gt;

&lt;p&gt;Most of the computation complexity in the above equation lies in finding the posterior probability of the latent state given the observed $\color{red}{\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta\right)}$. To elaborate on how the posterior probability is computed, let’s expand the acoustic model part in the previous figure as below, which is essentially a hidden-Markov chain.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/HMM2.png&quot; alt=&quot;conventional HMM based acoustic model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The inference problem (finding the posterior of the latent given the observed) in a hidden Markov chain can be solved by a forward-backward algorithm. The algorithm manifests itself as BCJR algorithm in convolutional code bit-level MAP decoding and &lt;a href=&quot;/posts/2012/08/kalman_filter_particle_filter/&quot;&gt;Kalman filtering&lt;/a&gt; in linear dynamic system.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\text{Forward path: }&amp;\text{calculate }\mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\text{ from }\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right) \notag \\
&amp;\mathbb{P}\left(z_t, {\bf x}_{1\to t};\theta\right) = \sum_{z_{t-1}} \mathbb{P}(z_{t}|z_{t-1};\theta)\mathbb{P}(x_{t}|z_{t};\theta)\mathbb{P}\left(z_{t-1},{\bf x}_{1\to t-1};\theta\right)  \notag \\
\text{Backward path: }&amp;\text{calculate }\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right)\text{ from }\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
&amp;\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) = \sum_{z_{t+1}} \mathbb{P}(z_{t+1}|z_{t};\theta)\mathbb{P}(x_{t+1}|z_{t+1};\theta)\mathbb{P}\left({\bf x}_{t+2\to T}{\big |}z_{t+1};\theta\right) \notag \\
\text{Combined: }&amp;\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t,{\bf x}_{1\to t};\theta\right)\mathbb{P}\left({\bf x}_{t+1\to T}{\big |}z_t;\theta\right) \notag \\
&amp;\mathbb{P}\left(z_t| {\bf x}_{1\to T};\theta\right) = \mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right) / \sum_{z_t}\mathbb{P}\left(z_t, {\bf x}_{1\to T};\theta\right)
\end{align} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;circular-dependency-between-segmentation-and-recognition&quot;&gt;Circular dependency between segmentation and recognition&lt;/h2&gt;
&lt;p&gt;The expectation-maximization formulation for likelihood maximization reveals a fundamental circular dependency between segmentation and recognition.&lt;/p&gt;

&lt;p&gt;Here &lt;strong&gt;segmentation&lt;/strong&gt; refers to the alignment of sub-phoneme states of ${\bf y}$ and the acoustic feature observations ${\bf x}$, encoded in the hidden state sequence ${\bf z}$, and &lt;strong&gt;recognition&lt;/strong&gt; refers to the classification of sub-phoneme hidden state sequence ${\bf z}$ for the corresponds acoustic feature observations ${\bf x}$.&lt;/p&gt;

&lt;p&gt;The two equations below make the circular dependency precise:
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align*}
\theta^{[i]} &amp;
\underset{\text{ }}{\xrightarrow{\substack{\text{update soft-alignment}\\\text{based on recognition}}}} \mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right)\text{ using Equation (2)}\\
\mathbb{P}\left({\bf z}^{(n)}| {\bf x}^{(n)};\theta^{[i]} \right) &amp;
\underset{\text{ }}{\xrightarrow{\substack{\text{update recognition}\\\text{based on soft-alignment}}}}
\theta^{[i+1]}\text{ using Equation (1)}
\end{align*} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;It is easy to argue that to have a accurate alignment, one would need accurate recognition, and to train an accurate recognition, one would have to rely on accurate alignment/segmentation.&lt;/p&gt;

&lt;p&gt;In convention ASR system, to bootstrap the training procedure, one has to start with a dataset that has human curated phoneme boundary/segmentation. Once the system is capacitated with reasonable recognition/inference, it is no longer confined with human aligned dataset and a much larger dataset can be used with just waveform and transcription pair. Eventually, after the system is able to deliver robust segmentation, one can make hard decision on the alignment, and only focus on improving the recognition performance with potentially a different system that has a much larger capacity, e.g., a DNN replacing the GMM model.&lt;/p&gt;

&lt;h2 id=&quot;decoding&quot;&gt;Decoding&lt;/h2&gt;
&lt;p&gt;In the decoding stage, one tries to find the word/sentence with the maximum a posterior (MAP) probability given the observed data&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{=}&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) \\
=&amp;\arg\max_{\bf y} \underbrace{\mathbb{P}({\bf x}|{\bf p};\theta)}_{\text{acoustic model}}\times\underbrace{\mathbb{P}({\bf p}|{\bf y})}_{\text{lexion}}\times\underbrace{\mathbb{P}({\bf y})}_{\text{language model}}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The lexicon and language model together construct a state transition diagram, which we unrolled in time to form a decoding trellis. For each transcription hypothesis, a proper MAP decoder would sum across all the paths in the trellis that corresponds to the transcription, which is computationally prohibitive.&lt;/p&gt;

&lt;p&gt;One simplification one can make is to find the most probable path by running the Viterbi algorithm. However, even for Viterbi algorithm, the complexity is still too high for practical deployment due to the large state space and potentially large number of time steps.&lt;/p&gt;

&lt;p&gt;To further reduce the computation complexity, the conventional system resorts to the beam-search algorithm – basically a breath-first-search algorithm on the trellis that maintain only a limited number of candidates. The beam-search algorithm is often run on a weighted finite state transducer that captures the concatenation of language model and lexicon.&lt;/p&gt;

&lt;h2 id=&quot;discrepancy-between-mle-training-and-map-decoding&quot;&gt;Discrepancy between MLE training and MAP decoding&lt;/h2&gt;

&lt;p&gt;At first glance into the MAP decoding equation, it may appear that the MLE based training is well-aligned with the decoding process: maximizing the posterior probability of the transcription ${\bf y}$ given the acoustic feature ${\bf x}$ is equivalent to maximizing the likelihood of the observation ${\bf x}$. The argument being that the probability of the observation $\mathbb{P}(x;\theta)$ is anyway a constant dictated by the natural of people’s speech, not something we can control. But is it true?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\text{inference time:}\\
&amp;\arg\max_{\bf y} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&amp;\arg\max_{\bf y} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;It turns out there is a subtle difference between inference time (MAP decoding) and training time (MLE parameter update) that render the above statement wrong.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\text{training time:}\\
&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
\color{red}{\not=}&amp;\arg\max_{\color{red}{\theta}} \mathbb{P}({\bf x}|{\bf y};\theta)\mathbb{P}({\bf y})
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;As is evident by comparing the above two equations, when we try to update parameter $\theta$ to maximize directly the posterior probability of the transcription ${\bf y}$ given the acoustic feature ${\bf x}$, we can no longer ignore the term $\mathbb{P}({\bf x};\theta)$. The key is to realize that we model the speech as a generative model, where &lt;strong&gt;the probability of observing a certain acoustic features ${\bf x}$ is not dictated by the nature, but rather the generative model that we assume&lt;/strong&gt;. By updating the parameter $\theta$ that best increase the likelihood, we inevitably change $\mathbb{P}({\bf x};\theta)$ too, and thus there is no guarantee that the posterior probability is increased. $\mathbb{P}({\bf x};\theta)$ is calculated by marginalizing over all potential transcriptions: $\mathbb{P}({\bf x};\theta)=\sum_{\bf y}\mathbb{P}({\bf x}|{\bf y};\theta)$.&lt;/p&gt;

&lt;p&gt;To elaborate, in MLE, we try to maximize $\color{red}{\mathbb{P}({\bf y}|{\bf x};\theta)}$ with respect to $\theta$, we may very well also increased the likelihood for competing transcription sequences $\color{blue}{\mathbb{P}({\bf x}|{\bf \tilde{y}};\theta)}$, potentially resulting in decreased posterior probability $\mathbb{P}({\bf y}|{\bf x};\theta)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\mathbb{P}({\bf y}|{\bf x};\theta) \\
=&amp; \mathbb{P}({\bf x},{\bf y};\theta) / \mathbb{P}({\bf x};\theta)\\
=&amp; \frac{\color{red}{\mathbb{P}({\bf x}|{\bf y};\theta)}\mathbb{P}({\bf y})}{\sum_{\bf \tilde{y}}\color{blue}{\mathbb{P}({\bf x}|{\bf \tilde{y}};\theta)}\mathbb{P}({\bf \tilde{y}})}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Fundamentally, &lt;strong&gt;the misalignment is rooted from the fact that we are using a generative model for discriminative tasks&lt;/strong&gt;. In the next section, we discuss several ways to train the generative model for better discrimination.&lt;/p&gt;

&lt;h2 id=&quot;sequence-discriminative-training&quot;&gt;Sequence discriminative training&lt;/h2&gt;
&lt;p&gt;To bridge the aforementioned discrepancy, several other training targets are proposed.&lt;/p&gt;

&lt;p&gt;to be continued…&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="ASR" /><category term="HMM" /><summary type="html">Conventional HMM-based ASR system assumes a generative model comprised of a language model, a lexicon (e.g., pronunciation dictionary), and an acoustic model, as illustrated below. Here $\theta$ denotes the parameters to be learned and it comprises of the HMM state transition probabilities and GMM/DNN parameters.</summary></entry><entry><title type="html">Comparison of end-to-end ASR models</title><link href="https://yyang768osu.github.io/posts/2012/11/end_to_end_asr_models/" rel="alternate" type="text/html" title="Comparison of end-to-end ASR models" /><published>2018-11-13T00:00:00-08:00</published><updated>2018-11-13T00:00:00-08:00</updated><id>https://yyang768osu.github.io/posts/2012/11/comparison-of-end-2-end-asr-models</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/11/end_to_end_asr_models/">&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/end2end.png&quot; alt=&quot;End-to-end ASR model derivation&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/transcription_model.png&quot; alt=&quot;Sentence model for CTC, RNN-T, and Attention&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;language-model-integration&quot;&gt;Language model integration&lt;/h2&gt;</content><author><name>Yang Yang</name></author><category term="ASR" /><summary type="html"></summary></entry><entry><title type="html">Gumbel max and Gumbel softmax</title><link href="https://yyang768osu.github.io/posts/2012/11/gumbel_max_and_gumbel_softmax/" rel="alternate" type="text/html" title="Gumbel max and Gumbel softmax" /><published>2018-11-05T00:00:00-08:00</published><updated>2018-11-05T00:00:00-08:00</updated><id>https://yyang768osu.github.io/posts/2012/11/gumbel-max-and-gumbel-softmax</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/11/gumbel_max_and_gumbel_softmax/">&lt;p&gt;Before we talk about Gumbel distribution, let’s refresh our knowledge on exponential distribution. It is well-known that the exponential distribution is min-stable: the min of $n$ I.I.D. exponential random variables  $X_i\sim \text{Exp}(\lambda_i), i=1,2,\ldots, n$ is also exponentially distributed with decay rate $\sum_{1\leq i\leq n} \lambda_i$, as can be seen from the equation below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
P(\min_{1\leq i \leq n} X_i &gt; x) &amp;= \prod_{1\leq i\leq n} P(X_i&gt;x) = e^{-\sum_{1\leq i\leq n}\lambda_i x}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;A lesser known property is that the arg-min of exponential variables is a multinomial distribution with event probabilities $\left(\frac{\lambda_1}{\sum_{i}\lambda_i}, \ldots, \frac{\lambda_n}{\sum_{i}\lambda_i}\right)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;P(\arg\min_{1\leq i\leq n}X_i =k)\\
=&amp;\int_0^\infty P(\arg\min_{1\leq i\leq n} X_i = x | X_k=x)f(X_k = x)dx\\
=&amp;\int_0^\infty \prod_{i=1\to n, i\not=k} e^{-\lambda_i x} \lambda_k e^{-\lambda_k x} dx\\
=&amp;\frac{\lambda_k}{\sum_{i=1\to n} \lambda_i}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;As we will see shortly, this property directly leads to the Gumbel max trick.&lt;/p&gt;

&lt;h2 id=&quot;gumbel-max-trick&quot;&gt;Gumbel max trick&lt;/h2&gt;

&lt;p&gt;Assume that we are given a multinomial (a.k.a. categorical) distribution with unnormalized event probabilities $\lambda_i, i=1\to n$ (i.e., $\sum_{i=1}^n\lambda_i\not=1$), the above property provides us a way to sample from the distribution without the need for normalization:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;draw $n$ samples from an exponential distributions with decay rate of $1$&lt;/li&gt;
  &lt;li&gt;scale the value of these $n$ samples with $1/\lambda_i$ for $i=1\to n$.&lt;/li&gt;
  &lt;li&gt;take the index of the minimum of the scaled samples&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To be more precise, we are utilizing the fact that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\min_{1\leq i\leq n} \left(\frac{1}{\lambda_i}s_i\right) \sim \text{Multinomial}\left(\frac{\lambda_1}{\sum_{i}\lambda_i}, \ldots, \frac{\lambda_n}{\sum_{i}\lambda_i}\right)\\
&amp;\text{where } s_i\sim \text{Exp}(1), \forall i.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;For the case of soft-max operation, we have direct access to the log of the unnormalized probabilities $\alpha_i=\log \lambda_i$ (multinomial logit), instead of the unnormalized probabilities itself. In this case, we can modify the above equation as below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\min_{1\leq i\leq n} \left(\frac{1}{e^{\alpha_i}}s_i\right) \sim \text{Multinomial}\left(\frac{e^{\alpha_1}}{\sum_{i}e^{\alpha_i}}, \ldots, \frac{e^{\alpha_n}}{\sum_{i}e^{\alpha_i}}\right)\\
&amp;\text{where } s_i\sim \text{Exp}(1), \forall i.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;One observation is that the left hand side of the above equation is invariant to any linear transform. The Gumbel-max trick is obtained by taking $-\log(\cdot)$ operation to the right-hand-side, in which case $-\log(\text{Exp}(1))$ is a standard Gumbel distribution, leading to the equation below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\arg\max_{1\leq i\leq n} \left(\alpha_i+g_i\right) \sim \text{Multinomial}\left(\frac{e^{\alpha_1}}{\sum_{i}e^{\alpha_i}}, \ldots, \frac{e^{\alpha_n}}{\sum_{i}e^{\alpha_i}}\right)\\
&amp;\text{where } g_i\sim \text{Gumbel}(\text{location}=0, \text{scale}=1), \forall i.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This provides us a way to obtain samples directly from the logits without going through the exponentiate-and-normalization step&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;draw $n$ samples from a standard Gumbel distributions with location of $0$ and scale of $1$.&lt;/li&gt;
  &lt;li&gt;add the values of the $n$ samples to the logits.&lt;/li&gt;
  &lt;li&gt;take the index of the minimum of the $n$ summations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Essentially, &lt;em&gt;the Gumbel max trick converts the sampling operation from a categorical/multinomial distribution into an argmax operation&lt;/em&gt;. The sampling process can be expedited if we pre-calculate and store a stream of Gumbel samples.&lt;/p&gt;

&lt;h2 id=&quot;gumbel-softmax&quot;&gt;Gumbel softmax&lt;/h2&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/gumbel.png&quot; alt=&quot;Gumbel softmax&quot; /&gt;&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="gumble" /><summary type="html">Before we talk about Gumbel distribution, let’s refresh our knowledge on exponential distribution. It is well-known that the exponential distribution is min-stable: the min of $n$ I.I.D. exponential random variables $X_i\sim \text{Exp}(\lambda_i), i=1,2,\ldots, n$ is also exponentially distributed with decay rate $\sum_{1\leq i\leq n} \lambda_i$, as can be seen from the equation below.</summary></entry><entry><title type="html">An introduction to Kalman filter and particle filter</title><link href="https://yyang768osu.github.io/posts/2012/08/kalman_filter_particle_filter/" rel="alternate" type="text/html" title="An introduction to Kalman filter and particle filter" /><published>2018-08-20T00:00:00-07:00</published><updated>2018-08-20T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/kalman-filter-and-particle-filter</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/kalman_filter_particle_filter/">&lt;p&gt;Kalman filter and particle filter are concepts that are intimidating for new learners due to its involved mathmatical discription, and are straightforward once you grasp the main idea and get used to Gaussian distributions. The goal of this post is to take a journey to Kalman filter by dissecting its idea and operation into pieces that are easy to absorb, and then assemble them together to give the whole picture. As a last step, we will see that particle filter achieves the same goal for non-Gaussian system resorting to Monte Carlo sampling.&lt;/p&gt;

&lt;p&gt;Below let’s walk through three simple problems and their solutions stemming from Gaussian distributions, and then stitching them together to form the problem that Kalman filter tries to solve and present its solution.&lt;/p&gt;

&lt;h2 id=&quot;a-conditional-gaussian-distribution&quot;&gt;A. Conditional Gaussian distribution&lt;/h2&gt;

&lt;p&gt;Here I assume you have a basic knowledge regarding multi-variant Gaussian distribution. A multi-variant Gaussian distribution is captured by its mean vector and covariance matrix, often denoted as $\mu$ and $\Sigma$. Below let us consider the bi-variant Gaussian vector $[z,x]^T$, with the following general notation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[
\begin{array}{c}
z\\
x
\end{array}
\right]
\sim
\mathcal{N}
\left(
\left[
\begin{array}{c}
\mu_z\\
\mu_x
\end{array}
\right],
\left[\begin{array}{cc}
\Sigma_z &amp; \Sigma_{zx} \\
\Sigma_{xz} &amp; \Sigma_{x}
\end{array}\right]
\right)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The covariance matrix is formally defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[\begin{array}{cc}
\Sigma_z &amp; \Sigma_{zx} \\
\Sigma_{xz} &amp; \Sigma_{x}
\end{array}\right]
=&amp;
\mathbb{E}\left[
\left[
\begin{array}{c}
z-\mu_z\\
x-\mu_x
\end{array}
\right]
\left[
\begin{array}{c}
z-\mu_z\\
x-\mu_x
\end{array}
\right]^T
\right]\notag\\
=&amp;
\left[
\begin{array}{cc}
\mathbb{E}[(z-\mu_z)(z-\mu_z)^T] &amp; \mathbb{E}[(z-\mu_z)(x-\mu_x)^T]\\
\mathbb{E}[(x-\mu_x)(z-\mu_z)^T] &amp; \mathbb{E}[(x-\mu_x)(x-\mu_x)^T]
\end{array}
\right]\notag.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The off-diagonal term represents the cross covariance between the two random variables $x$ and $z$, which, by checking the definition above, satisfies $\Sigma_{xz}=\Sigma^T_{zx}$. The larger the cross covariance, the more correlated the two random variables are. For correlated random variables, knowing the value of one would help us in guessing the value of the other. Let us take a look at the figure below for a concrete example.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/bivariant_normal.png&quot; alt=&quot;Bivariant Gaussian&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The figure above illustrates the joint distribution of a bivariant Gaussian distribution with $\mu_z=\mu_x=0$, $\Sigma_z=\Sigma_x=1$, and $\Sigma_{zx}=\Sigma_{xz}=0.8$. The marginal distributions of both $x$ and $z$ are $\mathcal{N}(0,1)$. The contour of the distribution forms a thin ellipse, reflecting the strong covariance $\Sigma_{zx}=\Sigma_{xz}=0.8$ between the two random variables $x$ and $z$. To testify the claim that knowing one variable would help us estimating the other,  let us take a look at the the conditional distribution of $z$ given $x=1$, and compare it with the marginal distribution of $z$. As can be seen from the figure above, the distribution of $z|x=1$ has much narrow span than $z$ with a shift in the mean. The reduction of variance of $z$ after observing $x=1$ is an evident that the observation of $x$ narrows down the potential values of $z$.&lt;/p&gt;

&lt;p&gt;An important fact here is that the conditional distribution of a joint-Gaussian distribution is also Gaussian&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
p(z|x) = p(x,z)/p(x)\sim\mathcal{N}\left(\mu_{z|x}, \Sigma_{z|x}\right)\notag.
\end{align*}&lt;/script&gt;

&lt;p&gt;Below are two identities on the general expressions of the conditional distribution, here let us accept them as they are without bothering with any proof.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z|x} &amp;= \mu_z + \Sigma_{zx}\Sigma_{xx}^{-1}(x-\mu_x)\notag\\ 
\Sigma_{z|x} &amp;= \Sigma_{z} - \Sigma_{zx}\Sigma_x^{-1}\Sigma_{xz} \text{ } \left(\preccurlyeq \Sigma_{z}\right)  \notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The above two equations are very important, and lies in the core of many concepts such as MMSE estimator, Wiener filter and of course, Kalman filter. To see that knowing $x$ would reduce the uncertainty in $z$, here let’s just point out that the entropy (measure of uncertainty) of a Gaussian random vector is an increasing function of the determinant of the covariance-matrix, and that $\Sigma_{z|x}$ always &lt;a href=&quot;https://math.stackexchange.com/questions/466158/on-the-difference-of-two-positive-semi-definite-matrices&quot;&gt;has a smaller determinant&lt;/a&gt; than $\Sigma_z$ for any non-zero cross covariance $\Sigma_{zx}$, followed from the second equation with the fact that $\Sigma_{z}-\Sigma_{z|x}$ $=\Sigma_{zx}\Sigma_x^{-1}\Sigma_{xz}\succcurlyeq 0$ is a semi-positive-definite matrix.&lt;/p&gt;

&lt;p&gt;These two equations will become useful when we visit part C, and we will come back to them.&lt;/p&gt;

&lt;h2 id=&quot;b-gaussian-distribution-with-linear-transformation&quot;&gt;B. Gaussian distribution with linear transformation&lt;/h2&gt;

&lt;p&gt;In the first section we looked at the case of obtaining the conditional distribution from a joint Gaussian distribution, here let’s look at the distribution of a Gaussian vector going through a linear transform. More precisely, let us define a random variable $z_n$ as obtained from the following transform&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z_{n-1}&amp;\sim\mathbb{N}(\mu_{n-1}, V_{n-1})\notag\\
z_n |z_{n-1}&amp;\sim A z_{n-1}+a+\mathcal{N}(0,\Gamma)= \mathbb{N}(Az_{n-1}+a, \Gamma)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;One typical example for the above problem setup is the following: consider the problem of tracking the location and velocity of an object traveling in a strict line. Let us label $z_n=[\text{loc}_n, \text{vel}_n]^T$ as the location-velocity state of the object in time-step $n$. To model the estimation inaccuracy, assume that $z_n$ is a random variable with mean $\mu_{n-1}$ and variance $V_{n-1}$. Here the mean value reflect the estimated value and the variance can be viewed as capturing the amount of and the structure of the uncertainty in the estimate. The location-velocity estimate in the time-step $n$ can be modeled by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[
\begin{array}{c}
\text{loc}_{n}\\
\text{vel}_{n}
\end{array}
\right]=
\underbrace{
\left[
\begin{array}{cc}
1 &amp; 1\notag\\
0 &amp; 1\notag
\end{array}
\right]}_{
\text{loc}_{n} = \text{loc}_{n-1}+\text{vel}_{n-1}
}
\times
\left[
\begin{array}{c}
\text{loc}_{n-1}\\
\text{vel}_{n-1}
\end{array}
\right]
+
\underbrace{
\left[
\begin{array}{c}
a_\text{loc}\notag\\
a_\text{vel}\notag
\end{array}
\right]}_{\substack{
\text{external known}\\\text{change}
}
}
+
\underbrace{
\left[
\begin{array}{c}
\text{noise}_\text{loc}\notag\\
\text{noise}_\text{vel}\notag
\end{array}
\right]}_{\substack{
\text{additional noise}\\\text{in the system}
}
}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;with a one-to-one correspondence to the conditional probability $z_n|z_{n-1}$ restated above.&lt;/p&gt;

&lt;p&gt;The problem of interest here is to characterize the distribution of $z_n$, given that it is obtained from a linear transformation of a previous estimate with additional Gaussian noise. Precisely, what we want to solve is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(z_n) =&amp; \int p(z_{n}|z_{n-1})p(z_{n-1})dz_{n-1}\notag.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here is another important property of Gaussian distribution: any linear transformation of Gaussian variable is still Gaussian. With this property given, we can calculate the mean and variance of the updated state $z_n$, as shown below, which fully captures its distribution.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z_n} =&amp;\mathbb{E}[Az_{n-1}+a]= A\mu_{n-1}+a\notag\\
\Sigma_{z_n} =&amp;\mathbb{E}[(Az_{n-1}-A\mu_{n-1})(Az_{n-1}-A\mu_{n-1})^T]+\Gamma\notag\\
=&amp; AV_{n-1}A^T + \Gamma\notag,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;which leads to the following solution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
z_n \sim \mathcal{N}\left(A\mu_{n-1}+a, AV_{n-1}A^T + \Gamma\right).
\end{align*}&lt;/script&gt;

&lt;h2 id=&quot;c-bayes-theorem-for-gaussian&quot;&gt;C. Bayes’ theorem for Gaussian&lt;/h2&gt;

&lt;p&gt;In part A, we provide the equation for calculating the conditional distribution from a joint Gaussian distribution, i.e., for a given joint-Gaussian probability $p(x,z)$, the conditional distribution of $p(z|x)$ is also Gaussian and it can be expressed in closed-form.&lt;/p&gt;

&lt;p&gt;In this part, we consider a slightly more complex problem, whereby we are given Gaussian distributions $p(x|z)$ and $p(z)$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
z&amp;\sim \mathcal{N}(\nu, P),\notag\\
x|z&amp;\sim \mathcal{N}(Cz+c,\Pi) = Cz+c+\mathcal{N}(0, \Pi),\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the problem is find the posterior distribution $p(z|x)$, which can be expressed using $p(x|z)$ and $p(z)$ by Bayes’ rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
p(z|x) = \frac{p(z)p(x|z)}{p(x)}  \propto p(z)p(x|z).\notag
\end{align*}&lt;/script&gt;

&lt;p&gt;Here’s a typical application for this problem: let us consider the task of estimating the temperature and humidity of a room (denoted as vector $z$). We are given two sources of information: (1) prior knowledge on the distribution from history data and, e.g., $p(z)$ (2) the reading from a thermometer with some known accuracy $p(x|z)$. Intuitively, a good estimate should be obtained by fusing these two informations. Indeed, this is evident from the Bayes’ rule, where the posterior probability of $z$ given $x$ is proportional to the product of the two distributions $p(z)p(x|z)$.&lt;/p&gt;

&lt;p&gt;Since part A taught us how to obtain a conditional distribution from a joint distribution. We can solve this problem by obtaining the joint distribution $p(z,x)=p(z)p(x|z)$ first and then plugin the solution presented in part A.&lt;/p&gt;

&lt;p&gt;For Gaussian, the multi-variant joint distribution is fully captured by the marginalized mean/variance together with the cross-variance among all factors, which, in our case, can be expressed as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{x} &amp;= \mathbb{E}[Cz+c+\mathcal{N}(0, \Sigma)] = C\nu+c\notag\\
\Sigma_{x} &amp;= \mathbb{E}[xx^T] = \mathbb{E}[(Cz-C\nu)(Cz-C\nu)^T]+\Sigma=C P C^T + \Pi\notag\\
\Sigma_{zx} &amp;= \mathbb{E}[z(Cz-C\mu)^T]=P C^T  \notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Accordingly, the joint distribution of $z$ and $x$ can be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\left[
\begin{array}{c}
z\\
x
\end{array}
\right]
\sim
\mathcal{N}
\left(
\left[
\begin{array}{c}
\nu\\
C\nu+c
\end{array}
\right],
\left[\begin{array}{cc}
P &amp;P C^T \\
CP &amp; C P C^T + \Pi
\end{array}\right]
\right)\notag.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now, by plugging in the solution in part A, we can obtain below the expression of the mean and variance of the posterior probability $p(z|x)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z|x} &amp;= \nu + PC^T (CPC^T+\Pi)^{-1}(x-C\nu-c)\notag\\
\Sigma_{z|x} &amp;= P - PC^T (CPC^T+\Pi)^{-1}CP\notag 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;To simplify the expression as well as to gain some insights into the expression, it is necessary to group some of the terms in $K$ below and substitute the corresponding terms.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
K\triangleq PC^T(CPC^T+\Pi)^{-1},\notag
\end{align*}&lt;/script&gt;

&lt;p&gt;resulting in the rewritten form below:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mu_{z|x} &amp;= {\color{red}\nu} + K(x-C\nu-c)\notag\\
\Sigma_{z|x} &amp;= {\color{red}P}-KCP =(I-KC)P.\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;It is interesting to observe that the highlighted term in the expression above is the mean and the variance of the prior distribution $p(z)$ without taking $p(x|z)$ into account. The effect of the $p(x|z)$ can be thought of as a correction to the prior distribution: the mean is shifted by $K(x-Cv-c)$ and the covariance matrix is reduced by $KCP$ (or shrunk by $(I-KC)$), leading to a refined posterior distribution $p(z|x)$. Here $K$ can be considered as a &lt;em&gt;gain&lt;/em&gt; factor, as it shifts the mean towards that dictated by $x$ and it shrinks the covariance matrix, leading to a more concentrated distributed with less amount of uncertainty.&lt;/p&gt;

&lt;p&gt;Next we will see that Kalman filter is just a repeated (or sequential) application of this Bayes’ rule on Gaussian distribution.&lt;/p&gt;

&lt;h2 id=&quot;kalman-filter&quot;&gt;Kalman filter&lt;/h2&gt;

&lt;p&gt;It’s time to assemble what we learnt from the previous parts. Let’s consider following an evolving system, where the system state $z_n$ follows linear evolving over time, whose true value is hidden from us. Every time instance, we obtain a noisy observation $x_n$ of the system state. The noisy observation $x_n$ may not be directly the state itself, but is in general an linear function of the state of interest, with added Gaussian noise. The task is to keep updating the belief on the system state, based on all the noisy observations, and the knowledge on the system evolution itself. In the degenerated case where the system does not evolve, then the problem amount to the sequential application of Bayes’ rule on the same hidden variable to fuse all the instances of noisy observations.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/linear_dynamic_system.png&quot; alt=&quot;Bivariant Gaussian&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To devise a sequence update rule on the brief of the system state based on all observations $p(z_n| x_1^n)$, let us look at the atomic case when $p(z_{n-1}|x_1^{n-1})$ — the prior belief of the previous state, and $p(x_n|z_n)$ — the noisy observation based on the current state, are given, and the task is to find $p(z_n|x_1^{n})$ — the posterior belief of the current state.&lt;/p&gt;

&lt;p&gt;In other words, we want to find an iterative procedure that update the belief on the system state, based on linear system evolution and noisy state observation. Precisely, we need to solve the following problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\underset{\substack{\\\mathcal{N}(\mu_{n-1}, V_{n-1})}}{p(z_{n-1}|x_1^{n-1})}
\underset{\text{ }}{\xrightarrow{\substack{\text{system evolution }\\p(z_n|z_{n-1})}}}  
\underset{\substack{\\\mathcal{N}(\nu_{n-1}, P_{n-1})}}{p(z_n|x_1^{n-1}) }
\underset{\text{ }}{\xrightarrow{\substack{\text{noisy observation }\\p(x_n|z_{n})}}}  
\underset{\substack{\\\mathcal{N}(\mu_{n}, V_{n})}}{p(z_{n}|x_1^{n})}
\end{align*}&lt;/script&gt;

&lt;!---
Here we assume that the noisy observations $x$ are independent given the underlying system state $z$ (this assumption is actually encoded in the graphic model above), then $p(z_{n-1}\|x_1^{n-1})$  captures all the information regarding $x_1^{n-1}$, and we can simply drop them from the expression. 
--&gt;

&lt;p&gt;The decomposed two sub-problem above correspond to part B and part C respectively, for which we can get the following solution&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$p(z_n|x_1^{n-1}) = \int p(z_n|z_{n-1}) p(z_{n-1}|x_1^{n-1})dz_{n-1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{Input: }&amp;\notag\\
z_{n-1}|x_1^{n-1}\sim&amp;\mathcal{N}(\mu_{n-1}, V_{n-1})\notag\\
z_n|z_{n-1} \sim&amp; \mathcal{N}(Az_{n-1}+a, \Gamma)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{Solution: }&amp;\notag\\
z_n|x_1^{n-1}\sim&amp; \mathcal{N}(\nu_{n-1}, P_{n-1})\notag\\
\nu_{n-1}=&amp;  A\mu_{n-1}+a\notag\\
P_{n-1}=&amp; AV_{n-1}A^T+\Gamma\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$p(z_n|x_1^n)\propto p(x_n|z_n)p(z_n|x_1^{n-1})$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{Input: }&amp;\notag\\
z_n|x_1^{n-1}\sim&amp; \mathcal{N}(\nu_{n-1},P_{n-1})\notag\\
x_n|z_n\sim&amp; \mathcal{N}(Cz_n+c,\Pi)\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{Solution: }&amp;\notag\\
z_n|x_1^n \sim&amp;\mathcal{N}(\mu_n, V_n)\notag\\ 
\mu_n =&amp; \nu_{n-1} + K_n(x_n-C\nu_{n-1}-c)\notag\\
V_n =&amp; (I-K_nC)P_{n-1}\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
K_n\triangleq P_{n-1}C^T(CP_{n-1}C^T+\Pi)^{-1}\notag
\end{align*}&lt;/script&gt;

&lt;p&gt;The final solution above is the Kalman filter equation, and $K_n$ is referred to as the Kalman gain.&lt;/p&gt;

&lt;h2 id=&quot;particle-filter&quot;&gt;Particle filter&lt;/h2&gt;

&lt;p&gt;One biggest constraint for the application of Kalman filter is that it assumes a linear dynamic system where the state transition and noisy observations are linear processes, which is evident from the probabilistic-graphic-model diagram shown before. The linear assumption is necessary to make sure that all the distributions involved in the system are Gaussian, which is easy to characterize and analytically tractable.&lt;/p&gt;

&lt;p&gt;In general cases, seldom do we have a system being linear. Even with a linear system, the distribution may not be Gaussian. The most cited example for the explanation of particle filter is localization. We can fit in the problem of localization as a dynamic system with hidden variables and heterogeneous system evolutions. Here the location of the system at time-step $n$ is modeled by the hidden variable $z_n$, and any observations made by accelerometer, GPS, and various other type of sensors are captured in $x_n$. Since $z_n$ represent the belief of the system’s location in a map, it can hardly be described by a Gaussian distribution and may not even have a tractable form. On top of that, the observations made by the sensors may not be a linear function of the system location.&lt;/p&gt;

&lt;p&gt;In this type of systems, instead of trying to deriving the exact distributions of hidden variables, it is more practical to characterize them using sets of samples. The sample update are achieved by a scheme often referred to as sequential Monte Carlo sampling, which we will introduce next.&lt;/p&gt;

&lt;p&gt;Again we emphasis that the problem at hand is the inference of hidden variables in a non-linear dynamic system. The goal is to characterize the posterior probability of the system state $z_n$ given all previous observations $x_1^n\triangleq x_1,x_2,\ldots x_n$. Specifically, we want to have an iterative procedure that update the system belief at time-slot $n$: $p(z_n|x_1^n)$ from the belief in the previous time instance $n-1$ ($p(z_{n-1}|x_1^{n-1}$) by considering both the system evolution $p(z_n|z_{n-1)}$ and the updated noisy observations $p(x_n|z_n)$.&lt;/p&gt;

&lt;p&gt;Drawing similarity to Kalman filter, we can represent the problem as the following:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\underset{\substack{\\\text{weighted samples}}}{p(z_{n-1}|x_1^{n-1})}
\underset{\text{sampling}}{\xrightarrow{\substack{\text{system evolution }\\p(z_n|z_{n-1})}}}  
\underset{\substack{\\\text{samples}}}{p(z_n|x_1^{n-1}) }
\underset{\text{importance weighting}}{\xrightarrow{\substack{\text{noisy observation }\\p(x_n|z_{n})}}}  
\underset{\substack{\\\text{weighted samples}}}{p(z_{n}|x_1^{n})}
\end{align*}&lt;/script&gt;

&lt;p&gt;The plan of attack, as suggested by the annotations in the above equation, is to get samples from the potentially intractable probabilities. Let’s start by assuming that we have a set of samples $\{\color{red}{z_n^{(s)}}, s=1,\ldots, S\}$ that represent the distribution of $p(z_n|x_1^{n-1})$, and the task is to generate a new set of sample $\{\color{blue}{z_{n+1}^{(s)}}, s=1,\ldots, S\}$ representing the distribution of $p(z_{n+1}|x_1^n)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(z_n| x_1^n) \propto&amp;\text{ }  p(x_n | z_n) \color{red}{p(z_n | x_1^{n-1})}\notag\\
\color{blue}{p(z_{n+1}|x_1^n)} =&amp; \int p(z_n| x_1^n) p(z_{n+1}|z_n) dz_n\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since the evaluation of $\color{blue}{p(z_{n+1}|x_1^n)}$ involves taking the expectation, a corresponding sampling approach would be to approximate it using sampling from  $\color{red}{p(z_n | x_1^{n-1})}$ together with the technique of importance sampling to bridge the gap between the $p(z_n| x_1^n)$ and $\color{red}{p(z_n | x_1^{n-1})}$, resulting in the derivation below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\color{blue}{p(z_{n+1}|x_1^n)} =&amp; \int p(z_n| x_1^n) p(z_{n+1}|z_n) dz_n\notag\\
=&amp;\int  \color{red}{p(z_n| x_1^{n-1})} \frac{p(z_n| x_1^n)}{\color{red}{p(z_n| x_1^{n-1})}}  p(z_{n+1}|z_n) dz_n\notag\\
=&amp; \frac{ \int \color{red}{p(z_n| x_1^{n-1})}   p(x_n|z_n)  p(z_{n+1}|z_n) dz_n}{
\int \color{red}{p(z_n| x_1^{n-1})}   p(x_n|z_n)   dz_n
},
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;which leads to the following Monte Carlo approximation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\color{blue}{p(z_{n+1}|x_1^n)} \approx&amp;\sum_{s=1}^S \frac{p(x_n|z_n^{(s)})}{\sum_{l=1}^S p(x_n|z_n^{(l)})}p(z_{n+1}|z_n^{(s)})\notag\\
&amp;\sum_{s=1}^S w_n^{(s)}p(z_{n+1}|z_n^{(s)})\notag
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Comparing the above equation with the one before, one can realize that the probability $p(z_n|x_1^n)$ is basically represented by a set of weighted samples, where the samples $z_n^{(s)}$ are drawn from $\color{red}{p(z_n|x_1^{n-1})}$ and the weights are defined as $w_n^{(s)}\triangleq \frac{p(x_n|z_n^{(s)})}{\sum_{l=1}^S p(x_n|z_n^{(l)})}$.&lt;/p&gt;

&lt;p&gt;Eventually, according to the above equation, to obtain samples $\color{blue}{\{x_{n+1}^{(s)}, s=1,\ldots, S\}}$ from $\color{blue}{p(z_{n+1}|x_1^n)}$, one can equivalently draw samples from $\sum_{s=1}^S w_n^{(s)}p(z_{n+1}|z_n^{(s)})$, which is itself a weighted sum of $p(z_{n+1}|z_n^{(s)})$ for each sample in $\color{red}{\{x_{n}^{(s)}, s=1,\ldots, S\}}$.&lt;/p&gt;

&lt;p&gt;With this we complete the derivation of the particle filter. In essence, it can be viewed as the sampling counterpart of Kalman filter, that generalizes to non-linear systems. The sequential Monte Carlo method is also referred to as sampling-importance-resampling in the literature.&lt;/p&gt;

&lt;p&gt;To link the math with a specific example, i recommend &lt;a href=&quot;https://www.youtube.com/watch?v=aUkBa1zMKv4&quot;&gt;this video&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;bigger-picture&quot;&gt;Bigger picture&lt;/h2&gt;

&lt;p&gt;Looking from a wider angle, both Kalman filter and Particle filter are inference algorithms in hidden Markov models with continuous random variables. Specifically, the formulation we went through corresponds to the forward procedure in HMM inference, and one can generalize it for the backward procedure as well.&lt;/p&gt;

&lt;p&gt;The forward-backward algorithm itself is a special realization of belief-propagation or message-passing-algorithm applied in a HMM system, whose probabilistic graphic model is a tree.&lt;/p&gt;

&lt;p&gt;The HMM model and the forward-backward procedure has manifestation in different areas:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;BCJR algorithm in the decoding of convolutional/Turbo code in communication systems&lt;/li&gt;
  &lt;li&gt;Baum-Welch algorithm, commonly used in speech recognition systems with discrete HMM models&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yang Yang</name></author><category term="kalman filter" /><summary type="html">Kalman filter and particle filter are concepts that are intimidating for new learners due to its involved mathmatical discription, and are straightforward once you grasp the main idea and get used to Gaussian distributions. The goal of this post is to take a journey to Kalman filter by dissecting its idea and operation into pieces that are easy to absorb, and then assemble them together to give the whole picture. As a last step, we will see that particle filter achieves the same goal for non-Gaussian system resorting to Monte Carlo sampling.</summary></entry><entry><title type="html">Griffin-Lim algorithm for waveform reconstruction</title><link href="https://yyang768osu.github.io/posts/2012/08/griffin-lim/" rel="alternate" type="text/html" title="Griffin-Lim algorithm for waveform reconstruction" /><published>2018-08-12T00:00:00-07:00</published><updated>2018-08-12T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/griffin-lim-algorithm-for-waveform-reconstruction</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/griffin-lim/">&lt;p&gt;Here’s the problem: we are given some &lt;em&gt;modified&lt;/em&gt; short-time-Fourier-transform (STFT) or &lt;em&gt;modified&lt;/em&gt; spectrogram (magnitude of STFT). The STFT/spectrogram is called &lt;em&gt;modified&lt;/em&gt; to stress the fact that it in general does not corresponds to any valid time-domain signal: it may be obtained by modifying a STFT/spectrogram of some real signal, or it may simply be generated by a neural network. The task here is to synthesize a time-domain signal from this target STFT/spectrogram, such that the STFT/spectrogram of the synthesized signal is as close to the target as possible.&lt;/p&gt;

&lt;p&gt;Let us denote the target STFT as $Y_w[mS,k]$, where $m$ is the index of the time-window, and $k$ represents the frequency axis. Here $S$ denote the step size of the time-domain window, and $mS$ can be viewed as the center of the sliding time windows. Correspondingly, if we are talking about spectrogram, the target can be denoted as  $|Y_w(mS,\omega)|$, to emphasis that there is no phase information.&lt;/p&gt;

&lt;p&gt;First, let’s align the definition and notation of the STFT/spectrogram. For a given time-domain signal $x[n]$, it is masked by a finite length window function $w$, yielding $x_w[mS,n]\triangleq w[n-mS]x[n]$, with $mS$ denoting the offset of the window from the original, and $m$ being the index of time-domain slice. The STFT $X_w[mS,k]$ is obtained by taking the DFT of each windowed signal $x_w[mS, n]$ with respect to time-index $n$: $X_w[mS,k]=\text{DFT}(x_w[mS, n])$.&lt;/p&gt;

&lt;p&gt;The objective is then to find a time domain signal $x[n]$ such that its STFT $X_w[mS,k]$ is as close to the target $Y_w[mS,k]$ as possible. Note, again, that the target $Y_w[mS,k]$ in general does not correspond to any valid time-domain signal. In other words, there is no time-domain signal that produces $Y_w[mS,k]$ as its STFT. Going back to the objective, a natural way to define the closeness is simply the sum squared distance, expressed as below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
D\left(X_w, Y_w\right) = \frac{1}{N}\sum_{m=-\infty}^{\infty}\sum_{k=0}^{N-1}|X_w[mS,k]-Y_w[mS,k]|^2\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;where $N$ denote the window-length (and correspondingly the DFT size). By Parseval’s theorem we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
D\left(X_w, Y_w\right) =&amp;\sum_{m=-\infty}^{\infty}\sum_{l=-\infty}^{\infty}|x_w[mS,l]-y_w[mS,l]|^2\notag\\
=&amp;\sum_{m=-\infty}^{\infty}\sum_{l=-\infty}^{\infty}|w[n-mS]x[n]-y_w[mS,l]|^2\notag,
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $y_w[mS,l]$ is the IDFT of $Y_w[mS,k]$.&lt;/p&gt;

&lt;p&gt;This is just the least-square problem, where the solution of $x[n]$ can be obtained by taking the derivative with respect to $D\left(X_w, Y_w\right)$ and enforce it to be zero, yielding the following solution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x[n]=\frac{  \sum_{m=-\infty}^\infty w[n-Sm]y_w[mS,n]  }{  \sum_{m=-\infty}^\infty w^2[n-Sm]  }\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;With carefully designed window function, we can make the denominator to be $1$, resulting in a form where the reconstructed signal $x$ is simply the sum of the IDFT of the target STFT, weighted by the window function $w$. Two such window functions are: (1) Rectangular window $w_r$ with length $L$ being an integer multiple of the sliding step-size $S$ (2) Sinusoidal window&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
w_s[n] = \frac{2w_r[n]}{\sqrt{4a^2+2b^2}}\left[a+b\cos\left(\frac{2\pi n}{L}+\phi\right)\right]\notag.
\end{align*}&lt;/script&gt;

&lt;p&gt;with length $L$ being a multiple of four times the window shift $S$.&lt;/p&gt;

&lt;p&gt;Now that we have introduced the reconstruction of modified STFT with a least-square error estimate (LSEE) criterion, let us look at the corresponding reconstruction of the modified spectrogram. Without phase information, the least-square optimization problem is modified to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
D\left(X_w, Y_w\right) = \frac{1}{N}\sum_{m=-\infty}^{\infty}\sum_{k=0}^{N-1}\left(|X_w[mS,k]|-|Y_w[mS,k]|\right)^2\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;It can be showed that the following iterative algorithm reduces the error function above after each iteration:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Start with initial estimate of $x^{(t)}[n]$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Obtain its STFT $X_w^{(t)}[mS, k]$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Replace the magnitude of $X_w^{(t)}[mS, k]$ with the target spectrogram $|Y_w[mS, k]|$ and preserve the phase. More precisely, obtain $\widehat{X}_w^{(t)}[mS, k]$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\widehat{X}_w^{(t)}[mS, k]=|Y_w[mS, k]|\frac{X_w^{(t)}[mS, k]}{|X_w^{(t)}[mS, k]|}\notag.
\end{align*}&lt;/script&gt;

&lt;ol&gt;
  &lt;li&gt;Get an updated estimate of $x^{(t+1)}[n]$ by running the LSEE algorithm with target STFT of $\widehat{X}_w^{(t)}[mS, k]$&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
x^{(t+1)}[n]=\frac{  \sum_{m=-\infty}^\infty w[n-Sm]\text{IDFT}\left(\widehat{X}_w^{(t)}[mS, k]\right) }{  \sum_{m=-\infty}^\infty w^2[n-Sm]  }\notag,
\end{align*}&lt;/script&gt;

&lt;p&gt;and go back to step 1.&lt;/p&gt;

&lt;p&gt;https://pdfs.semanticscholar.org/ade8/d1511a61d78948bb0d43e207593389935421.pdf&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="text to speech" /><summary type="html">Here’s the problem: we are given some modified short-time-Fourier-transform (STFT) or modified spectrogram (magnitude of STFT). The STFT/spectrogram is called modified to stress the fact that it in general does not corresponds to any valid time-domain signal: it may be obtained by modifying a STFT/spectrogram of some real signal, or it may simply be generated by a neural network. The task here is to synthesize a time-domain signal from this target STFT/spectrogram, such that the STFT/spectrogram of the synthesized signal is as close to the target as possible.</summary></entry><entry><title type="html">A step-by-step guide to variational inference (4): variational auto encoder</title><link href="https://yyang768osu.github.io/posts/2012/08/variational_inference_4/" rel="alternate" type="text/html" title="A step-by-step guide to variational inference (4): variational auto encoder" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/variational-inference-IV-variational-auto-encoder</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/variational_inference_4/">&lt;p&gt;The variational lower bound $\mathcal{L}$ sits in the core of variational inference. It connects the density estimation problem with the Bayesian inference problem through a variational (free to vary) distribution $q$, and it converts both problems into an optimization problem. Here let’s briefly revisit the identity associated with variational lower bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\ln p(X;\theta) &amp;= \text{KL}{\big(}q|| p(Z|X;\theta){\big)} + \mathcal{L}(q,\theta)\\
\text{where }\mathcal{L}(q, \theta) &amp;=\int q(Z) \ln \frac{p(X,Z;\theta)}{q(Z)} dZ
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The identity holds for any arbitrary probability function $q$. $\mathcal{L}$ is a lower bound for the data log-likelihood $\ln p(X;\theta)$ given the non-negativity of the KL divergence. From the identify we can obtain the following two equations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\ln p(X;\theta) &amp;= \max_{q} \mathcal{L}(q,\theta)\\
p(Z|X;\theta) &amp;= \underset{q}{\arg\max} \mathcal{L}(q,\theta),
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;which testified the claim that both density estimation (LHS of the first equation) and Bayesian inference (LHS of the second equation) are linked with the same optimization function. There are two implications if we increases the value of $\mathcal{L}$ by tweaking the distribution $q$: (1) $\mathcal{L}$ becomes a tighter lower bound of $\ln p(X;\theta)$, meaning that it is closer to the true data log-likelihood in value (2) the distribution function $q$ itself is closer to the true posterior distribution measured in KL divergence.&lt;/p&gt;

&lt;p&gt;Often, we are also given the task of finding the ML estimate of the parameter $\theta$ (or MAP estimate of the parameter $\theta$), which requires taking the maximum of $\ln p(X;\theta)$ (or $\ln p(X|\theta) + \ln p_\text{prior}(\theta)$ for the MAP case) with respect to $\theta$, yielding the following problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\max_{\theta}\max_{q}\mathcal{L}(q, \theta).
\end{align*}&lt;/script&gt;

&lt;p&gt;By increasing the variation lower bound $\mathcal{L}$ with respect to $\theta$, by which the model is parameterized, we are essentially searching for model that can better fit to the data.&lt;/p&gt;

&lt;p&gt;It should be clear that is it desirable to maximize $\mathcal{L}$ with respect to both the variational distribution $q$ and the generative parameter $\theta$: maximize it with respect to $q$ would yield a better inference function; maximize it with respect to $\theta$ would give us a better model.&lt;/p&gt;

&lt;p&gt;Instead of allowing $q$ to be any function within the probability function space, for analytical tractability, we assume that it is parameterized by $\phi$ and is a function of the observed data $x$, denoted as $q_\phi(x)$. For the generative model, let us modified the notation slightly by assuming that the prior distribution $p(z)$ is unparameterized, and denote the conditional generative probability as $p_\theta(x|z)$, leading to the following expression of the variational lower bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(\phi, \theta, x^{(i)}) = \int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln \frac{p_\theta\left(x^{(i)}|z^{(i)}\right)p\left(z^{(i)}\right)}{q_\phi\left(z^{(i)}|x^{(i)}\right)}dz^{(i)}
\end{align*}&lt;/script&gt;

&lt;p&gt;Note that we used to express the variational lower bound in terms of the complete observed dataset $X={x^{(1)},\ldots, x^{(N)}}$ as well as the corresponding latent variables $Z={z^{(1)},\ldots, z^{(N)}}$. Since each data point and the corresponding latent variables are generated independently, it can be decomposed into the summation of $N$ terms, one for each data point $x^{(i)}$ as shown above. Those $N$ identity equations are linked through global parameter $\phi$ and $\theta$.&lt;/p&gt;

&lt;p&gt;As discussed before, to obtain a better model and to obtain a closer approximation to the true posterior inference function, one needs to differentiate and optimize $\sum_{i=1}^N\mathcal{L}(\phi, \theta, x^{(i)})$ with respect to both $\phi$, the parameter of the inference function, and $\theta$, the parameter of the model. Here’s a plan: let us calculate the gradient for the lower bound with respect to both parameters, and be done with the problem by applying our favorite stochastic gradient descent algorithm to find a solution. Actually we will show later that such a stochastic training framework is analogous to using an auto-encoder architecture with a specific regularization function.&lt;/p&gt;

&lt;p&gt;Soon enough you will realize a major challenge: it is not clear how to differentiate against $\phi$. There is very little chance for us to expect a close-form expression if we directly differentiate what is inside the integral, as the integral itself is hard even without the differentiation. We will  spend some time here to dig into the issue, which is the key to the understanding of the variational auto-encoding algorithm.&lt;/p&gt;

&lt;p&gt;Since the lower-bound exists in the form of the expectation with respect to the variational distribution $q_\phi$, the work-around here is to seek for Monte-Carlo estimation for the integral with the sampling from distribution $q_\phi\left(z^{(i)}|x^{(i)}\right)$. Let us focus on the general problem of $\nabla_\phi \mathbb{E}_{q_\phi(z|x)}\left[f(z)\right]$, for which there are two approaches that use sampling to approximate the expectation:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approach 1&lt;/strong&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\nabla_\phi \mathbb{E}_{q_\phi(z|x)}\left[f(z)\right] \\
=&amp;\int \nabla_\phi q_\phi(z|x)  f(z) dz\\
=&amp;\int q_\phi(z|x) \frac{\nabla_\phi q_\phi(z|x)}{q_\phi(z|x)}  f(z) dz\\
=&amp; \int q_\phi(z|x)  \nabla_\phi \ln q_\phi(z|x) f(z) dz \\
=&amp; \mathbb{E}_{q_\phi(z|x)}\left[ \nabla_\phi \ln q_\phi(z|x) f(z)\right] \\
\text{(Monte Carlo)} \approx &amp;\frac{1}{S}\sum_{s=1}^S \nabla_\phi \ln q_\phi(z^{[s]}|x) f(z^{[s]}) 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Approach 2&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;This approach makes an additional assumption on $q_\phi(z|x)$: assume that we can obtain samples of $z$ by first sampling through a distribution $p(\epsilon)$ that is independent of $\phi$, and then apply a $(\phi,x)$-dependent transformation of  $g_\phi(\epsilon, x)$. Effectively we are assuming that the random variable $\mathcal{Z}$ is a $\phi-$dependent function of a $\phi$-independent random variable $\mathcal{E}$: $\mathcal{Z} = g_\phi(\mathcal{E},x)$. Reflecting this assumption in the differential of expectation, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\nabla_\phi \mathbb{E}_{q_\phi(z|x)}\left[f(z)\right]\\
=&amp;\nabla_\phi \int q_\phi(z|x)f(z) dz\\
\text{(parameter substitution)}=&amp;\nabla_\phi \int p(\epsilon)f(g_\phi(\epsilon, x))d\epsilon\\
=&amp; \int p(\epsilon) \nabla_\phi f(g_\phi(\epsilon, x))d\epsilon\\
\text{(Monte Carlo)}\approx&amp; \frac{1}{S}\sum_{s=1}^S  \nabla_\phi f(g_\phi(\epsilon^{[s]},x))
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This seems like a good solution: the Monte Carlo sampling itself is not a function of $\phi$, and $\phi$ just appear as the parameter of the transformation function  $g_\phi$ that maps the samples from $\mathcal{E}$ to the samples in $\mathcal{Z}$. In this case $q_\phi$ is just the induced distribution as a function of the prior distribution of $\mathcal{E}$ as well as the transformation function $g_\phi$. This parameter substitution technique is branded as &lt;em&gt;the reparameterization trick&lt;/em&gt; in the original paper of variational auto encoder.&lt;/p&gt;

&lt;p&gt;To understand the implication of such assumption, let’s ask this question: is it feasible to design the prior distribution of $\mathcal{E}$ and the transformation function $g_\phi$ in any arbitrary form? You may wonder why do we even care. Well there is a hidden factor that we need to take care of before claiming victory. Looking at the variational lower bound expression, not only do we need to integrate with respect to the distribution $q_\phi$, which can be achieved using Monte Carlo by the help of this reparameterization trick, we also need to ensure a closed-form expression of the density function $q_\phi(z|x)$ itself, as it lives inside the expectation/integral. This limits the way we can choose the random variable $\mathcal{E}$ and the function $g_\phi$.&lt;/p&gt;

&lt;p&gt;To investigate on the requirement of $\mathcal{E}$ and $g_\phi$ such that the induced random variable $\mathcal{Z} = g_\phi(\mathcal{E},x)$ has a tractable density/distribution function (easy to evaluate), let’s try to express distribution $q_\phi$ as a function of $p_\epsilon$ and $g_\phi(z,x)$. For any monotonic function $g_\phi$, the induced distribution $q_\phi$ &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_variable#Functions_of_random_variables&quot;&gt;can be derived&lt;/a&gt; as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_\phi(z) = p_\epsilon\left(g_\phi^{-1}(z)\right)\left|\frac{\partial g_\phi^{-1}(z)}{\partial z}\right|.
\end{align*}&lt;/script&gt;

&lt;p&gt;To enforce a closed form expression for $q_\phi$, we have two general design choices on $p_\epsilon$ and $g_\phi$, as is evident from the expression above: (1) let $p_\epsilon$ be a uniform distribution on $[0,1]$ and $g_\phi=\text{CDF}^{-1}$ be the inverse of any distribution with closed-form cumulative distribution function. (2) let $p_\epsilon$ be any distribution with closed form density and $g_\phi$ be an easy form of monotonic function, e.g., a linear function.&lt;/p&gt;

&lt;p&gt;In the context of variational auto encoder in the original paper, the second design choice is picked: $p_\epsilon$ is chosen as the standard normal distribution and  $g_\phi$ is a linear function of $\epsilon$, whose slope and intercept is an arbitrary function of $x$ and $\phi$ characterized using a neural network. In this case the induced distribution $q_\phi$ is a normal distribution whose mean and variances is determined by a neural network with the input $x$ and parameter $\phi$.&lt;/p&gt;

&lt;p&gt;Now that we went through what &lt;em&gt;the reparameterization trick&lt;/em&gt; is, let us return back to the problem of finding the gradient of $\mathcal{L}(\phi, \theta, x^{(i)})$ with respect to $\phi$ and $\theta$. Applying the reparameterization trick, we obtain the following gradient-friendly Monte Carlo estimate of the variational lower bound&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}(\phi, \theta, x^{(i)}) =&amp; \int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln \frac{p_\theta\left(x^{(i)}|z^{(i)}\right)p\left(z^{(i)}\right)}{q_\phi\left(z^{(i)}|x^{(i)}\right)}dz^{(i)}\\
\text{(Monte Carlo)}\approx&amp;\frac{1}{S}\sum_{s=1}^S \ln  \frac{p_\theta\left(x^{(i)}|z^{(i)[s]}\right)p\left(z^{(i)[s]}\right)}{q_\phi\left(z^{(i)[s]}|x^{(i)}\right)}\\
\text{(Reparameterization)}=&amp;\frac{1}{S}\sum_{s=1}^S \ln  \frac{p_\theta\left(x^{(i)}|g_\phi (\epsilon^{[s]}, x^{(i)})\right)p\left(g_\phi (\epsilon^{[s]}, x^{(i)})\right)}{q_\phi\left(g_\phi (\epsilon^{[s]}, x^{(i)})|x^{(i)}\right)}\\
\text{where } \epsilon^{[s]}&amp;\text{ is drawn i.i.d. from }p_\epsilon. 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Here’s an alternative way to decompose $\mathcal{L}$ and apply Monte Carlo and reparameterization, for which there is a close form expression for the second term (KL divergence) and only the first part is approximated.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}(\phi, \theta, x^{(i)}) =&amp; \int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln \frac{p_\theta\left(x^{(i)}|z^{(i)}\right)p\left(z^{(i)}\right)}{q_\phi\left(z^{(i)}|x^{(i)}\right)}dz^{(i)}\\
=&amp;\int q_\phi\left(z^{(i)}|x^{(i)}\right)\ln p_\theta\left(x^{(i)}|z^{(i)}\right) dz^{(i)}-\text{KL}\left( q_\phi\left(z^{(i)}|x^{(i)}\right){\Big|\Big|} p\left(z^{(i)}\right)\right)\\
\text{(Monte Carlo)}\approx&amp; \frac{1}{S}\sum_{s=1}^S \ln p_\theta\left(x^{(i)}|z^{(i)[s]}\right)  -\text{KL}\left( q_\phi\left(z^{(i)}|x^{(i)}\right){\Big|\Big|} p\left(z^{(i)}\right)\right)\\
\text{(Reparameterization)}\approx&amp; \frac{1}{S}\sum_{s=1}^S \ln p_\theta\left(x^{(i)}|g_\phi (\epsilon^{[s]}, x^{(i)})\right)  -\text{KL}\left( q_\phi\left(z^{(i)}|x^{(i)}\right){\Big|\Big|} p\left(z^{(i)}\right)\right)
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This decomposition leads to the interpretation of probabilistic auto-encoder, which is named variational auto-encoder as it rooted from the maximization of the variational lower bound.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/vae.png&quot; alt=&quot;Variational Auto Encoder&quot; /&gt;&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="variational inference" /><summary type="html">The variational lower bound $\mathcal{L}$ sits in the core of variational inference. It connects the density estimation problem with the Bayesian inference problem through a variational (free to vary) distribution $q$, and it converts both problems into an optimization problem. Here let’s briefly revisit the identity associated with variational lower bound</summary></entry><entry><title type="html">A step-by-step guide to variational inference (3): mean field approximation</title><link href="https://yyang768osu.github.io/posts/2012/08/variational_inference_3/" rel="alternate" type="text/html" title="A step-by-step guide to variational inference (3): mean field approximation" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/variational-inference-III-mean-field-approximation</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/variational_inference_3/">&lt;p&gt;We have learned in the previous post that E-M algorithm tries to find a ML or MAP solution to the parameters of a generative model. It is build on top of two major premises:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\ln p(X,Z;\theta)$ is in a much simpler form than $\ln P(X;\theta)$,&lt;/li&gt;
  &lt;li&gt;$\ln p(Z|X;\theta)$ is easy to obtain.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;While the first one is often true in that both $p(X|Z;\theta)$ and $p(Z;\theta)$ given as part of the model and are usually designed to be simple, the second one is a very strong assumption and does not hold in most cases. In this post, we remove the second premise, and introduce a way to obtain an approximation of $p(Z|X;\theta)$.&lt;/p&gt;

&lt;p&gt;In what follows, we modify notation slightly by assuming that there is prior distribution on any parameters of interest, and conceptually merge $\theta$ as part of the latent variable $Z$ (which is common across different data samples) and remove $\theta$ from the notation.&lt;/p&gt;

&lt;p&gt;As we discussed in the previous posts, the Bayesian inference problem is to find the posterior probability $p(Z|X)$, which is in general very hard due to the integral/summation (in most cases multi-dimensional integral/summation) in the denominator below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
p(Z|X) = \frac{p(X|Z)p(Z)}{\int p(X|Z)p(Z) dZ}.
\end{align*}&lt;/script&gt;

&lt;p&gt;We also showed that with the introduction of a variational distribution $q(Z)$, we can convert the problem of finding $p(X|Z)$ as an optimization problem below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
p(Z|X) &amp;= \arg\max_{q}\mathcal{L}(q).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;However, this optimization problem above is still very hard and it does not lend itself to any easy solution. Here, the objective $\mathcal{L}(q)$, called the variational lower-bound, is a functional as it maps a function into a scalar value, whose domain is the space of all functions.&lt;/p&gt;

&lt;p&gt;Since it is hard to optimize the variational lower bound as is, one may wonder, how about constraining the search space of $q$ from all potential functions to within a limited function space? Could that make the problem simpler? Even though we may not find the optimal solution after restricting the set of functions we could search from, the hope is that by doing so we can device practical algorithms with solutions that are reasonably close to the true posterior. This is exactly the idea behind mean field approximation.&lt;/p&gt;

&lt;p&gt;In the mean field method, we add a constraint to the domain of the optimization: instead of allowing $q(Z)$ to be in arbitrary form, we only look at cases when it can be factorized into a product form with disjoint latent variables in each multiplicative factor. More specifically, we divide the dimension of latent variables into $K$ groups $Z=[Z_1, Z_2, \ldots, Z_K]$ and enforce $q$ to have the form of $q(Z)=q_1(Z_1)q_2(Z_2)\ldots q_K(Z_K)$. Put it in precise math, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q^* = \underset{q=q_1 q_2 \ldots q_K}{\arg\max}\mathcal{L}(q)
\end{align*}&lt;/script&gt;

&lt;p&gt;Referring back to the equation on the decomposition of observed data log-likelihood&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\ln p(X) = \text{KL}(q || p(Z|X)) + \mathcal{L}(q),
\end{align*}&lt;/script&gt;

&lt;p&gt;we know that by maximizing $\mathcal{L}(q)$ with respect to functions with form $q(Z)=\prod_{k=1}^K q_k(Z_k)$, we are trying to find one function in the confined function space (defined as the set of functions that can be factorized as such) that is closest to the true posterior $ p(Z|X)$ measured in KL divergence. It is worth emphasizing that we are merely constraining $q(Z)$ to have this factorization form, and do not make any assumption on what each individual factor would look like.&lt;/p&gt;

&lt;p&gt;Let us plug in the factorized form of $q(Z) = \prod_{k=1}^K q_k(Z_k)$ in the expression of the variational lower bound, yielding&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\mathcal{L}(q) =&amp; \int q(Z)\ln\frac{p(X, Z)}{q(Z)}dZ\\
=&amp; \int q(Z)\ln p(X, Z)dZ +  \int q(Z)\ln\frac{1}{q(Z)}dZ\\
=&amp; \int \prod_{k=1}^K q_k(Z_k) \ln p(X, Z)dZ +  \sum_{k=1}^K\int q_k(Z_k)\ln\frac{1}{q_k(Z_k)}dZ_k.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The second term is just the entropy of $q$, which, given the assumption that it can be decomposed into independent factors, becomes the summation of the entropy for each individual $q_k$.&lt;/p&gt;

&lt;p&gt;It may not be immediately apparent why this modified formulation is any easier to solve. Nevertheless, let us proceed by making the temporary assumption that among the $K$ factors, all are known except for one factor $q_j$. Then, we just need to maximize $\mathcal{L}$ with respect to $q_j$ with all the other factors $q_{k}, k\not=j$ as given. The variational lower bound can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\int q_j(Z_j) \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]dZ_j + \int q_j(Z_j)\ln \frac{1}{q_j(Z_j)}dZ_j + \text{constant}\\
=&amp;\int q_j(Z_j)\ln\frac{\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)}{q_j(Z_j)}dZ_j + \text{constant}
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Since any term that does not involve $q_j$ would not affect the solution to $\arg\max_{q_j}\mathcal{L}(q)$, we just mark those as constant. Here it comes a key observation: notice how the non-constant term resembles the definition of a negative KL divergence between $q_j(Z_j)$ and $\mathbb{E}_{q_k, k\not=j}$ $\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)$. The only issue is that $\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)$ may not be a proper probability measure that sum/integrate to $1$. Luckily, since scaling $\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)$ only amounts to adding/subtracting a constant term, we know that $\mathcal{L}(q)$ is maximized when&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_j(Z_j)  \propto \exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right),
\end{align*}&lt;/script&gt;

&lt;p&gt;or more accurately,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
q_j(Z_j)  = \frac{\exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right)}{\int \exp\left( \mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]\right) dZ_j}.
\end{align*}&lt;/script&gt;

&lt;p&gt;This result tells us that, among the $K$ factors, if we have all but one factor fixed, then the optimal solution of that left out function that maximize the variation lower bound (or equivalently, minimizes the KL divergence to the true posterior distribution) can be written in the above form as a function of all the other factors.&lt;/p&gt;

&lt;p&gt;This leads to a nice iterative solution that iteratively visits each factor, and maximize the variational lower bound with respect to the target factor treating all the other factors as known. In special cases, the normalization constant term in the dominator of the above equation could be directly inferred if the numerator term already suggests certain type of known distribution.&lt;/p&gt;

&lt;p&gt;It is interesting to note that, to apply this mean-field-approximation method, one only need to make the assumption on how to partition the latent variable dimensions into disjoint groups, one for each factor, without making any assumption on the detailed function form of any factor. The detail form of the factorized distribution would be obtained as a result of the iterative procedure.&lt;/p&gt;

&lt;p&gt;There is one caveat that we should mention. Looking at the equation above, to find the optimal factor $q_j$, assuming all the other are know, we still need to make sure that the expectation $\mathbb{E}_{q_k, k\not=j}[\ln p(X,Z)]$ results in tractable form. Given that the expectation itself is a multi-dimensional integral/summation, in general it is hard to guarantee a closed form expression. The expectation may be tractable with specific models and specific ways on which the latent variables are partitioned, which limits the domain where mean-field-approximation could be applied.&lt;/p&gt;

&lt;p&gt;Here we introduced mean field approximation from the perspective of Bayesian inference. As a final remark, it is straightforward to show that it also provide a way to evaluate observed data likelihood and thus can be useful with model-selection as well. According to the identity below, we know that as we maximize $\mathcal{L}$, not only do we obtain a variational distribution that is close to the true posterior in the KL divergence sense, we also obtained a surrogate for the log-likelihood, as the lower bound $\mathcal{L}$ is a lower bound which gets tighter as it becomes larger.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\ln p(X) = \text{KL}(q || p(Z|X)) + \mathcal{L}(q),
\end{align*}&lt;/script&gt;

&lt;p&gt;If we are given $M$ models, then one can conduct mean field method on each of them, obtain the corresponding optimized variational lower-bound, and use it as the surrogate to rate the likelihood of each model. We can even combine the prior distribution of the $M$ models, if there is any, to obtain a maximum a posterior (MAP) selection of the model.&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="variational inference" /><summary type="html">We have learned in the previous post that E-M algorithm tries to find a ML or MAP solution to the parameters of a generative model. It is build on top of two major premises:</summary></entry><entry><title type="html">A step-by-step guide to variational inference (2): expectation maximization</title><link href="https://yyang768osu.github.io/posts/2012/08/variational_inference_2/" rel="alternate" type="text/html" title="A step-by-step guide to variational inference (2): expectation maximization" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/variational-inference-II-expectation-maximization</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/variational_inference_2/">&lt;p&gt;In the previous post we went through the derivation of variational lower-bound, and showed how it helps convert the Bayesian inference and density estimation problem to an optimization problem. Let’s briefly recap the problem setup and restate some key points.&lt;/p&gt;

&lt;p&gt;Consider a very general generative graphic model where each data point $x^{(n)}$ is generated from a latent variable $z^{(n)}$ conforming to a given distribution $p(X|Z;\theta)$, with $z^{(n)}$ itself drawn from a given prior distribution $p(Z; \theta)$. $\theta$ captures the set of variables that the two probabilities are parameterized with. Two fundamental problems are to (1) estimate the density of existing dataset $X$, i.e. $p(X;\theta)$ and (2) derive the posterior probability of the latent variable $Z$ given the observed data $X$, i.e., $p(Z|X;\theta)$. The exact solution of both problems requires the evaluation of the often intractable integral $\int P(X| Z;\theta)P(Z;\theta)dZ$.&lt;/p&gt;

&lt;p&gt;With the introduction of a variational/free distribution function $q(Z)$, we have the following identity:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\ln p(X;\theta) = \text{KL}{\big(}q||p(Z|X;\theta){\big)} + \mathcal{L}(q,\theta),
\end{align*}&lt;/script&gt;

&lt;p&gt;which says that the marginalized probability of dataset $X$ can be decomposed into a sum of two terms with the first one being the KL divergence between $q(Z)$ and the true posterior distribution $p(Z|X;\theta)$ and the second one expressed below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(q,\theta)=\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ,
\end{align*}&lt;/script&gt;

&lt;p&gt;which is referred to as the variational lower bound: it is called a lower-bound as it is always less than $\ln p(X;\theta)$ due to the non-negativity of KL divergence, and it is called variational as it is itself a functional that maps a variational/free distribution function $q$ to a scalar value. This identity is quite exquisite in that it turns both the density estimation problem and the latent variable inference problem into an optimization problem, evident from the two equations below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\ln p(X;\theta) &amp;= \max_{q}\mathcal{L}(q,\theta)\\
p(Z|X;\theta) &amp;= \arg\max_{q}\mathcal{L}(q,\theta).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The problem that Expectation Maximization algorithm designs to solve is the maximum-likelihood (ML) estimate of the parameter $\theta$. Mind you that $\theta$ is the parameter of the graphic model, and the task is to find a $\theta$ such that the model best explain the existing data. In precise term, the problem is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\max_{\theta}\ln p(X;\theta).
\end{align*}&lt;/script&gt;

&lt;p&gt;Now, resorting to the variational lower bound, equivalently we can also focus on the following maximization-maximization problem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\max_{\theta}\ln p(X;\theta) = \max_{\theta}\max_{q}\mathcal{L}(q,\theta),
\end{align*}&lt;/script&gt;

&lt;p&gt;A natural question is: why would this be any easier to evaluate compared with maximizing $\ln p(X;\theta)$ head on? did we increase our burden by considering a nested-maximization optimization problem rather than a single-maximization one?&lt;/p&gt;

&lt;p&gt;To answer we need to have the objective function under scrutiny. Looking at the detailed expression of $\mathcal{L}(q,\theta)$, the main hurdle is the evaluation the log-likelihood of the joint observed-latent variable $p(Z,X;\theta)$. We want to emphasis that the two probability distributions $p(Z;\theta)$ and $p(X|Z;\theta)$ are given as part of the model assumption, and they usually come in the form of well-known distributions, e.g., Gaussian, multinomial, exponential, etc. Thus, the joint likelihood of observed and hidden variable $p(Z,X;\theta)=p(Z;\theta)p(X|Z;\theta)$ is in an amenable form. Also, quite often, taking logarithm on it would break up all the multiplicative terms as summation, resulting in quite tractable from. Better yet, the parameters $\theta$ that we need to compute gradient with, may naturally be decomposed into different terms in the summation, making the calculation of derivative easy with respect to individual parameters.&lt;/p&gt;

&lt;p&gt;On the other hand, to compute the marginalized likelihood of the observed data only, i.e., $P(X;\theta)$, one need to sum or integrate out the effect of $Z$ from $p(Z,X;\theta)$, which may lead to complicated expression. While the evaluation of $P(X;\theta)$ may still be fine when, e.g., the marginalization only requires the summation of a finite number of terms (which is the case for the Gaussian mixture model), the real deal breaker is the difficulty in taking derivative of the log-likelihood with respective to the parameters: taking logarithm on $P(X;\theta)$ almost surely won’t result in nice decomposition, as the logarithm is gated by the integral or summation, and the log-sum expression is a lot harder to break when we compute the derivative with respect to the parameters $\theta$.&lt;/p&gt;

&lt;p&gt;Coming back to the maximization-maximization problem, it is natural to devise an iterative algorithm that maximize the objective function $\mathcal{L}(q,\theta)$ with alternating axis:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{E step: }&amp;q^{(t)} = \arg\max_{q}\mathcal{L}(q,\theta^{(t)})\\
\text{M step: }&amp;\theta^{(t+1)} = \arg\max_{\theta}\mathcal{L}(q^{(t)}, \theta) 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;It is worth mentioning that the first optimization problem is in general a very difficult one, as it requires searching through the whole function space. According to the derivation of the variational lower bound derivation we know that the optimal solution is the posterior distribution $p(Z|X;\theta^{(t)})$, which is hard to obtain. Actually finding an approximated posterior by maximizing the variational lower bound is the main theme in variational inference. Techniques of mean-field-approximation, and variational auto-encoder, which we cover in subsequent posts, targets at this problem.&lt;/p&gt;

&lt;p&gt;To proceed, we make a very strong assumption that $p(Z|X;\theta^{(t)})$ can be easily obtained. As we will see later that with certain simple model (e.g., Gaussian mixture model), it is indeed a valid assumption, nevertheless it is the key assumption that significantly limits the application of the expectation maximization algorithm.&lt;/p&gt;

&lt;p&gt;Anyway, for now let us live with this strong assumption, then the E-step results in the following  expression&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\text{E step: }q^{(t)} = p(Z|X;\theta^{(t)}).
\end{align*}&lt;/script&gt;

&lt;p&gt;Coming back to the second maximization problem (M-step), with $q^{(t)}$ fixed, we can decompose the variational lower bound as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(q^{(t)}, \theta) = \int q^{(t)}(Z)\ln p(X,Z;\theta)dZ + \int q^{(t)}(Z) \ln\frac{1}{q^{(t)}(Z)}dZ.
\end{align*}&lt;/script&gt;

&lt;p&gt;The second term above is just a constant term reflecting the entropy of $q^{(t)}$, so let us ignore it, and then the second maximization problem reduces to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{M step: }\theta^{(t+1)} =&amp;\max_{\theta} \int p(Z|X;\theta^{(t)}) \ln P(Z,X;\theta)dZ. 
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;The maximization target above can be viewed as finding the expectation of complete data (combining observed variable and latent variable) log likelihood, where the expectation is with respect to a fixed distribution on the latent variable $Z$.&lt;/p&gt;

&lt;p&gt;Let’s put the two steps together and review the whole iterative process. We are given a model with a set of parameters captured in $\theta$. The task is find the values of the parameters $\theta$ such that the model best explain the existing observed data at hand. At the beginning, we take a random guess on the value of the parameters. With that initial parameters, we find the posterior probability of the latent variable for each data point $x$ in the training data set $X$. Then, using that posterior probability, we calculate the expected complete-data log-likelihood, and try to find parameters $\theta$ so that the complete-data log-likelihood is maximized. With $\theta$ updated, we refresh our calculation of the posterior probability and iterative the process.&lt;/p&gt;

&lt;p&gt;In fact, K-means clustering algorithm is one instance of expectation-maximization procedure with certain model assumption. It is helpful to think of the E-M iterative process from the perspective of K-means clustering: for K-means clustering, the latent variable is one-dimensional with value from $1$ to $K$, implying the registration to one of the $K$ clusters. The parameter of the model is the center of the clusters, denoted as $\theta={c_1, \ldots, c_K}$. In the initial setup, we randomly set these $K$ cluster centers. For each data, we assign it to the nearest cluster, which is effectively assigning its latent variable. This step corresponds to finding the posterior distribution (E-step), with one of the clustering having probability $1$. After each data is assigned to its cluster with the initial values of the cluster centers, which gives us complete data in the form of (observed data, latent variable) pair, the next step is to adjust the center based on its constituent. This step corresponds to the maximizing of the expected complete-data log-likelihood (M-step), although this expectation is taken in a degenerated form as the posterior probability for the latent variable is in the form of $0/1$.&lt;/p&gt;

&lt;p&gt;We finish the treatment of E-M algorithm with the following closing remarks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The E-M iterative algorithm is guaranteed to reach a local maximum on the log-likelihood of the observed data $p(X;\theta)$, as both steps increases it.&lt;/li&gt;
  &lt;li&gt;It is not necessary to find the maximum in the M-step. So long as the updated $\theta$ increase the complete-data log-likelihood, we are still in the right direction.&lt;/li&gt;
  &lt;li&gt;So far we focused on finding the maximum-likelihood (ML) solution to $\theta$ (local maximum). In the case when there is prior distribution $p_\text{prior}(\theta)$ on $\theta$, we can use the same process to find a maximum-a-posterior (MAP) solution (local maximum), utilizing the fact that $p(\theta|X) \propto p(X|\theta)p_\text{prior}(\theta)$. The problem is modified as&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\max_{\theta}\ln p(\theta|X) = \max_{\theta}\left(\max_{q}\mathcal{L}(q,\theta) {\color{red} + \ln p_\text{prior}(\theta)}\right),
\end{align*}&lt;/script&gt;

&lt;p&gt;with slightly modified procedure below&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\text{E step: }&amp;q^{(t)} = p(Z|X,\theta^{(t)})\\
\text{M step: }&amp;\theta^{(t+1)} =\max_{\theta} \int p(Z|X,\theta^{(t)}) \ln P(Z,X|\theta)dZ {\color{red} + \ln p_\text{prior}(\theta)}. 
\end{align*} %]]&gt;&lt;/script&gt;</content><author><name>Yang Yang</name></author><category term="variational inference" /><summary type="html">In the previous post we went through the derivation of variational lower-bound, and showed how it helps convert the Bayesian inference and density estimation problem to an optimization problem. Let’s briefly recap the problem setup and restate some key points.</summary></entry><entry><title type="html">A step-by-step guide to variational inference (1): variational lower bound</title><link href="https://yyang768osu.github.io/posts/2012/08/variational_inference_1/" rel="alternate" type="text/html" title="A step-by-step guide to variational inference (1): variational lower bound" /><published>2018-08-05T00:00:00-07:00</published><updated>2018-08-05T00:00:00-07:00</updated><id>https://yyang768osu.github.io/posts/2012/08/variational-inference-I-varitional-lower-bound</id><content type="html" xml:base="https://yyang768osu.github.io/posts/2012/08/variational_inference_1/">&lt;p&gt;In machine learning, graphic models are often used to describe the factorization of probability distributions. The detailed form of the graphic model encodes one’s belief/hypothesis regarding the underlying structure of the data. In this article, we confine the discussion to a general form of directed graphic model as illustrate below.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/images/dgm.png&quot; alt=&quot;generative model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let $X=\{x^{(1)}, x^{(2)}, \ldots, x^{(n)}\}$ denote the dataset of interest, be it a set of images, a set of sound clips, or a set of document, depending on the problem at hand. The model describes a way for which the data is generated: we first sample a hidden/latent variable $z$ from the distribution $P(Z;\theta)$, and then sampled a data point $x$ from the distribution $P(X|z;\theta)$ given the latent variable. The two probabilities are defined as we like and are given as part of the graphic models. Here we assume that the two probabilities are parameterized by a set of variables $\theta$, although in general we could merge it as part of latent variable $Z$ as well (as a global latent variable, the value of which is shared among all data samples), if there is a prior distribution for $\theta$. It is worth noting that the $P(Z;\theta)$ and $P(X|Z;\theta)$ could be further factorized, whichever way we design them to be, however, in this article we will just focus on the general setting.&lt;/p&gt;

&lt;p&gt;There are two intertwined problems associated with this form of generative models: density estimation, and Bayesian inference. For the problem of density estimation, we want to estimate the probability that the model assigns to all the data $X$ we are given in training, or more precisely $p(X; \theta)$. The value of $p(X; \theta)$ explains how likely it is for the model to generate the given training data. The larger the $p(X; \theta)$, the better our model explains the existing data. For models that are parameterized by $\theta$, fitting the model to best match the training data amounts to finding a value of $\theta$ that maximize the density $p(X; \theta)$.&lt;/p&gt;

&lt;p&gt;For the problem of Bayesian inference, we want to infer the posterior probability of the unobserved hidden/latent variable given the observed data, or more precisely $p(Z|X;\theta)$. It is easy to see that these two problems are naturally intertwined from Bayes rule: $p(Z|X) = \frac{p(X|Z)p(Z)}{P(X)}$: since $p(X|Z)$ and $p(Z)$ are already given as part of the model assumption, if we solve one of the two problems, then the other one can be solved as well. Conceptually, the solution to the problems can be viewed as trying to find a reverse graph in the generative model.&lt;/p&gt;

&lt;p&gt;Let’s now take a step back and ask the question: why do we bother with the introduction of latent/hidden variable? Can we just propose a model that captures $p(X;\theta)$ directly, and save the trouble of Bayesian inference for the latent variable all together?  Anyway, even with the direct characterization of  $p(X;\theta)$, the discussion above should still holds: the larger the  $p(X;\theta)$, the better our model explains the given data, and with a good model we can apply sampling to generate artificial data.&lt;/p&gt;

&lt;p&gt;The benefits of the hidden/latent variables are two-fold:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The adoption of hidden/latent variables allows one to construct complex marginal data distributions $p(X)$ from simple and easy to evaluate distribution functions. For example, with $p(Z;\theta)$ being the multinomial distribution and $p(X|Z;\theta)$ being the normal distribution, we arrive at the Gaussian mixture model $p(X;\theta)=\int P(X|Z;\theta)P(Z;\theta)dZ$, which can characterize a wide range of complex distribution and has significantly more expressiveness power compared with just Gaussian or multinomial distribution alone. It is evident that a model that characterizes more complex distributions can better fit the data, especially with the high-dimensional complicated data we are usually focusing on.&lt;/li&gt;
  &lt;li&gt;The hidden/latent variables can be viewed as general features extracted from the data, which can be utilized for any downstream tasks. The hidden/latent variables normally has much lower dimension compared with the data itself, and they represent low-dimensional message that conveys condensed information regarding the corresponding data. If a model can fit the data well, meaning that the likelihood is high for the model to generate the training data by sampling $X$ conditioned on a sampling of $Z$, then one can argue that $Z$ should capture the essence of the data. It is interesting to note how the above two points echo the previous discussion regarding the inter-connection between density estimation and Bayesian inference.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the Bayesian inference problem, as stated before, the task is to find the posterior distribution of the unobserved variable $Z$ given then observed variable $X$. Instead of tackling this problem head on by deriving $p(Z|X;\theta)=\frac{p(X|Z;\theta)p(Z;\theta)}{\int p(X|Z;\theta)p(Z;\theta)dZ}$, which is often intractable, let us introduce another distribution $q(Z)$ with the goal of mimicking $p(Z|X;\theta)$, and look at what the KL divergence between the two could decompose into:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\text{KL}{\big(}q||p(Z|X;\theta){\big)}\\
=&amp;\int q(Z) \ln \frac{q(Z)}{p(Z|X;\theta)}dZ\\
=&amp;\int q(Z) \ln \frac{q(Z)}{P(Z|X;\theta)}\frac{p(X;\theta)}{p(X;\theta)}dZ\\
=&amp;\int q(Z) \ln \frac{q(Z)p(X;\theta)}{P(Z,X;\theta)}dZ\\
=&amp;\int q(Z) \ln p(X;\theta)dZ-\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ\\
=&amp;\ln p(X;\theta)-\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Making the short-hand notation of&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\mathcal{L}(q,\theta)=\int q(Z) \ln \frac{P(Z,X;\theta)}{q(Z)}dZ,
\end{align*}&lt;/script&gt;

&lt;p&gt;we can simplify the above equation as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align*}
\ln p(X;\theta) = \text{KL}{\big(}q||p(Z|X;\theta){\big)} + \mathcal{L}(q,\theta).
\end{align*}&lt;/script&gt;

&lt;p&gt;The above equation is the cornerstone for a broad range of variational methods, which we will keep coming back to for later posts. Let’s stare at it for a while, observe it from different angles, and learn to appreciate its elegancy.&lt;/p&gt;

&lt;p&gt;We should first observe that the three terms have fixed polarity: KL divergence is always nonnegative, whereas the log-likelihood term on the LHS of the equation, as well as the expression $\mathcal{L}(q,\theta)$ is always non-positive. At first glance into the definition of $\mathcal{L}$, it may look like it can be written in the form of negative KL divergence. However, one should note that $P(Z,X;\theta)$ is not a proper probability on $Z$ as $\int p(X,Z;\theta)dZ = p(X;\theta)&amp;lt;1$.&lt;/p&gt;

&lt;p&gt;Given that the KL divergence term is always nonnegative, $\mathcal{L}(q,\theta)$ yield a lower bound on the log-likelihood of the data. In precise term, we have $\ln p(X;\theta) \geq \mathcal{L}(q,\theta)$.&lt;/p&gt;

&lt;p&gt;The term $\mathcal{L}(q,\theta)$ can be viewed as a functional that maps a probability distribution function into a value. 
Since the analysis and optimization of functional falls into the realm of calculus of variations, the distribution function $q$ itself is often called the variational distribution, and the lower bound $\mathcal{L}(q,\theta)$ is referred to as the variational lower-bound.&lt;/p&gt;

&lt;p&gt;It is important to realize that the above equation is another manifestation of the inter-connection between the data likelihood $p(X;\theta)$ and the posterior distribution of latent variable $p(Z|X;\theta)$, this time linked through the variational distribution function $q$. For a fixed parameter $\theta$, if we increase the variational lower bound $\mathcal{L}(q,\theta)$ by adjusting $q$, then the updated lower-bound is closer to the log-likelihood of the data. At the same time, since an increment in $\mathcal{L}(q,\theta)$ would infer a decrement of $\text{KL}(q||p(Z|X;\theta))$, we know that the updated variational distribution $q$ is closer to the true posterior distribution measured in KL divergence. To precisely capture these observations, we arrive at the following two equations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
\ln p(X;\theta) &amp;= \max_{q}\mathcal{L}(q,\theta)\\
p(Z|X;\theta) &amp;= \arg\max_{q}\mathcal{L}(q,\theta).
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;This is the core of variational inference: with an introduction of a variational distribution $q$, we can turn both the log-likelihood calculation (i.e., density estimation) problem and the Bayesian inference problem into an optimization problem, and attack it with different optimization algorithms. This inference-optimization duality provides a very powerful tool. It is the backbone of many of the variational inference related methods such as expectation-maximization, mean-field approximation, and variational auto-encoder, which we will discuss in details in the subsequent posts.&lt;/p&gt;

&lt;p&gt;As a closing note below we list two alternative proofs for the variational lower-bound.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\ln p(X;\theta)\\
=&amp;\ln\frac{p(X,Z;\theta)}{p(Z|X;\theta)}\\
=&amp;\int q(Z)\ln\frac{p(X,Z;\theta)}{p(Z|X;\theta)}dZ\\
=&amp;\int q(Z)\ln\frac{p(X,Z;\theta) q(Z)}{p(Z|X;\theta) q(Z)}dZ\\
=&amp;\int q(Z)\ln\frac{p(X,Z;\theta)}{q(Z)}dZ+\int q(Z)\ln\frac{q(Z)}{p(Z|X;\theta)}dZ.\\
=&amp;\mathcal{L}(q,\theta) + \text{KL}{\big(}q||p(Z|X;\theta){\big)}.
\end{align*} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
&amp;\ln p(X;\theta)\\
=&amp;\ln \int p(X,Z;\theta)dZ\\
=&amp;\ln \int q(Z)\frac{p(X,Z;\theta)}{q(Z)}dZ \\
\geq &amp; \int q(Z)\ln\frac{p(X,Z;\theta)}{q(Z)}dZ.
\end{align*} %]]&gt;&lt;/script&gt;</content><author><name>Yang Yang</name></author><category term="variational inference" /><summary type="html">In machine learning, graphic models are often used to describe the factorization of probability distributions. The detailed form of the graphic model encodes one’s belief/hypothesis regarding the underlying structure of the data. In this article, we confine the discussion to a general form of directed graphic model as illustrate below.</summary></entry></feed>